# RAFT ê¸°ë°˜ RAG ì „ìš© SLLM íŒŒì¸íŠœë‹

![RAFT Overview](https://img.shields.io/badge/RAFT-Retrieval%20Augmented%20Fine%20Tuning-blue)
![vLLM](https://img.shields.io/badge/vLLM-High%20Performance%20Serving-green)
![LoRA](https://img.shields.io/badge/LoRA-Parameter%20Efficient%20Fine%20Tuning-orange)

## ğŸ“‹ ëª©ì°¨

1. [í”„ë¡œì íŠ¸ ê°œìš”](#-í”„ë¡œì íŠ¸-ê°œìš”)
2. [í•µì‹¬ ê¸°ìˆ  ì†Œê°œ](#-í•µì‹¬-ê¸°ìˆ -ì†Œê°œ)
3. [í”„ë¡œì íŠ¸ êµ¬ì¡°](#-í”„ë¡œì íŠ¸-êµ¬ì¡°)
4. [í™˜ê²½ ì„¤ì •](#-í™˜ê²½-ì„¤ì •)
5. [ë°ì´í„° ì¤€ë¹„ ê³¼ì •](#-ë°ì´í„°-ì¤€ë¹„-ê³¼ì •)
6. [íŒŒì¸íŠœë‹ ê³¼ì •](#-íŒŒì¸íŠœë‹-ê³¼ì •)
7. [vLLMì„ í™œìš©í•œ RAG ì‹œìŠ¤í…œ](#-vllmì„-í™œìš©í•œ-rag-ì‹œìŠ¤í…œ)
8. [ì‹¤í–‰ ê°€ì´ë“œ](#-ì‹¤í–‰-ê°€ì´ë“œ)
9. [ì„±ëŠ¥ ìµœì í™” íŒ](#-ì„±ëŠ¥-ìµœì í™”-íŒ)
10. [ì°¸ê³  ìë£Œ](#-ì°¸ê³ -ìë£Œ)

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

ë³¸ í”„ë¡œì íŠ¸ëŠ” **RAFT(Retrieval Augmented Fine Tuning)** ê¸°ë²•ì„ í™œìš©í•˜ì—¬ í•œêµ­ì–´ ë„ë©”ì¸ì— íŠ¹í™”ëœ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•

- ğŸ” **RAFT ê¸°ë²•**: ë¬¸ì„œ ê²€ìƒ‰ê³¼ ìƒì„±ì„ í†µí•©í•œ íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹
- ğŸš€ **vLLM**: ê³ ì„±ëŠ¥ ì¶”ë¡  ë° ì„œë¹™ì„ ìœ„í•œ ìµœì í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬
- ğŸ›ï¸ **LoRA**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  íŒŒì¸íŠœë‹
- ğŸ‡°ğŸ‡· **í•œêµ­ì–´ íŠ¹í™”**: KLUE-MRC ë°ì´í„°ì…‹ ê¸°ë°˜ í•œêµ­ì–´ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ

### í”„ë¡œì íŠ¸ ëª©ì 

1. **ë„ë©”ì¸ íŠ¹í™” RAG ì‹œìŠ¤í…œ êµ¬ì¶•**: íŠ¹ì • ë„ë©”ì¸ì˜ ë¬¸ì„œì—ì„œ ì •í™•í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±
2. **íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹**: LoRAë¥¼ í™œìš©í•œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ëª¨ë¸ í•™ìŠµ
3. **ê³ ì„±ëŠ¥ ì¶”ë¡ **: vLLMì„ í†µí•œ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ëª¨ë¸ ì„œë¹™
4. **ì‹¤ë¬´ ì ìš© ê°€ëŠ¥í•œ ì‹œìŠ¤í…œ**: ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ RAG íŒŒì´í”„ë¼ì¸

## ğŸ”¬ í•µì‹¬ ê¸°ìˆ  ì†Œê°œ

### RAFT (Retrieval Augmented Fine Tuning)

RAFTëŠ” 2024ë…„ì— ë°œí‘œëœ í˜ì‹ ì ì¸ íŒŒì¸íŠœë‹ ê¸°ë²•ìœ¼ë¡œ, RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

#### RAFTì˜ í•µì‹¬ ì•„ì´ë””ì–´

```mermaid
graph TD
    A[ì§ˆë¬¸] --> B[ë¬¸ì„œ ê²€ìƒ‰]
    B --> C[ê´€ë ¨ ë¬¸ì„œ + ë¬´ê´€í•œ ë¬¸ì„œ]
    C --> D[RAFT íŒŒì¸íŠœë‹]
    D --> E[ì •ë‹µ ë¬¸ì„œë§Œ ì°¸ì¡°í•˜ì—¬ ë‹µë³€ ìƒì„±]
    D --> F[ë¬´ê´€í•œ ë¬¸ì„œëŠ” ë¬´ì‹œ]
    E --> G[ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€]
```

#### RAFTì˜ í•™ìŠµ ì „ëµ

1. **Positive Documents**: ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µì´ í¬í•¨ëœ ë¬¸ì„œ
2. **Negative Documents**: ì§ˆë¬¸ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ì •ë‹µì´ ì—†ëŠ” í˜¼ë€ìŠ¤ëŸ¬ìš´ ë¬¸ì„œ  
3. **Citation Learning**: ë‹µë³€ì—ì„œ ì°¸ì¡°í•œ ë¬¸ì„œ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ë„ë¡ í•™ìŠµ

```python
# RAFT í•™ìŠµ ë°ì´í„° ì˜ˆì‹œ
{
    "question": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
    "docs": [
        "doc1: ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸íŠ¹ë³„ì‹œì…ë‹ˆë‹¤.",  # Positive
        "doc2: ì¼ë³¸ì˜ ìˆ˜ë„ëŠ” ë„ì¿„ì…ë‹ˆë‹¤.",              # Negative  
        "doc3: ì¤‘êµ­ì˜ ìˆ˜ë„ëŠ” ë² ì´ì§•ì…ë‹ˆë‹¤.",            # Negative
        "doc4: ì„œìš¸ì€ í•œê°•ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë°œì „í–ˆìŠµë‹ˆë‹¤.",    # Negative
        "doc5: ë¶€ì‚°ì€ ëŒ€í•œë¯¼êµ­ì˜ ì œ2ì˜ ë„ì‹œì…ë‹ˆë‹¤."      # Negative
    ],
    "answer": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸íŠ¹ë³„ì‹œì…ë‹ˆë‹¤[[doc1]]."
}
```

#### RAFT ë°ì´í„°ì…‹ êµ¬ì„± ì •ë¦¬

##### 1. ë…¼ë¬¸ ê·¼ê±° ê¸°ë°˜ ìš”ì†Œ

- **ê³¨ë“  ë¬¸ì„œ + ë„¤ê°€í‹°ë¸Œ ìƒ˜í”Œ(ë””ìŠ¤íŠ¸ë™í„°) ì¡°í•©**
  - ê¸°ë³¸ì ìœ¼ë¡œ ì§ˆë¬¸(Q) + ê³¨ë“  ë¬¸ì„œ(D*) + kê°œì˜ ë””ìŠ¤íŠ¸ë™í„°(Di)ë¡œ êµ¬ì„±
  - ì‹¤í—˜ì—ì„œëŠ” ë³´í†µ 1ê°œì˜ ê³¨ë“  ë¬¸ì„œ + 4ê°œì˜ ë””ìŠ¤íŠ¸ë™í„° ì‚¬ìš©

- **ë””ìŠ¤íŠ¸ë™í„° ê°œìˆ˜ ë‹¤ì–‘í™”**
  - kê°’ì„ ë‹¬ë¦¬í•˜ì—¬(0~n) ëª¨ë¸ì´ ë‹¤ì–‘í•œ retrieval ìƒí™©ì— ê²¬ê³ í•´ì§€ë„ë¡ í›ˆë ¨

- **ì—¬ëŸ¬ ë¬¸ì„œ ì¡°í•© (ë©€í‹°ê³¨ë“ )**
  - HotpotQA ê°™ì€ ê²½ìš° ì •ë‹µì´ ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ìœ ë„ë  ìˆ˜ ìˆì–´, ê³¨ë“  ë¬¸ì„œê°€ ë³µìˆ˜ì¼ ìˆ˜ ìˆìŒ

- **ì²´ì¸ì˜¤ë¸Œì˜íŠ¸(CoT) + ì¸ìš© í¬í•¨**
  - ë‹µë³€ì€ reasoning ê³¼ì •ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ê³ , ë¬¸ì„œ ì¸ìš©(##begin_quote## â€¦ ##end_quote##)ì„ ëª…ì‹œí•˜ë„ë¡ êµ¬ì„±

##### 2. ë³€ê²½ëœ ë‚´ì—­ (ë³¸ í”„ë¡œì íŠ¸ ì ìš© ë²„ì „)

- **ì§ˆë¬¸ íŒ¨ëŸ¬í”„ë ˆì´ì§• (ëª…ì‚¬êµ¬ ë³€í™˜ ë“±)**
  - ë…¼ë¬¸ì—ëŠ” ì—†ëŠ” ì•„ì´ë””ì–´. ë°ì´í„° ë‹¤ì–‘í™”ì™€ ì¼ë°˜í™” ì„±ëŠ¥ ê°•í™”ë¥¼ ìœ„í•´ ì¶”ê°€

- **ì§ˆë¬¸ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ë°ì´í„° ì œê±°**
  - ë…¼ë¬¸ì—ì„œëŠ” (1âˆ’P)% ìƒ˜í”Œì—ì„œ ê³¨ë“  ë¬¸ì„œë¥¼ ì œê±°í•˜ê³  ë””ìŠ¤íŠ¸ë™í„°ë§Œ ë„£ëŠ” ë°©ì‹ì„ ì‚¬ìš©
  - í•˜ì§€ë§Œ ì‹¤ì œ ì‹¤í—˜ì—ì„œëŠ” over-refusal ë¬¸ì œ(ëª¨ë¸ì´ ë¬´ì‘ë‹µ/íšŒí”¼ â†’ recall í•˜ë½)ë¥¼ ì•¼ê¸°í•´ ì œì™¸í•¨

### vLLM (Very Large Language Model Serving)

vLLMì€ UC Berkeleyì—ì„œ ê°œë°œí•œ ê³ ì„±ëŠ¥ LLM ì¶”ë¡  ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

#### ì£¼ìš” ê¸°ìˆ 

1. **PagedAttention**: 
   - ìš´ì˜ì²´ì œì˜ ê°€ìƒ ë©”ëª¨ë¦¬ ê¸°ë²•ì„ LLMì— ì ìš©
   - KV ìºì‹œë¥¼ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ê´€ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
   - ê¸°ì¡´ ëŒ€ë¹„ ìµœëŒ€ 24ë°° ë¹ ë¥¸ ì²˜ë¦¬ëŸ‰ ë‹¬ì„±

2. **Continuous Batching**:
   - ë™ì  ë°°ì¹˜ ì²˜ë¦¬ë¡œ GPU í™œìš©ë„ ê·¹ëŒ€í™”
   - ìš”ì²­ë³„ ìƒì„± ê¸¸ì´ê°€ ë‹¬ë¼ë„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬

3. **Optimized CUDA Kernels**:
   - NVIDIA GPUì— ìµœì í™”ëœ ì»¤ë„ë¡œ ì—°ì‚° ê°€ì†í™”

### LoRA (Low-Rank Adaptation)

LoRAëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

#### LoRAì˜ ì›ë¦¬

```python
# ê¸°ì¡´ Linear Layer: W âˆˆ R^(dÃ—k)
# LoRA: W + Î”W, where Î”W = BA
# B âˆˆ R^(dÃ—r), A âˆˆ R^(rÃ—k), r << min(d,k)

class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=8):
        super().__init__()
        self.lora_A = nn.Linear(in_features, rank, bias=False)
        self.lora_B = nn.Linear(rank, out_features, bias=False)
        self.scaling = 1.0 / rank
        
    def forward(self, x):
        # ì›ë³¸ ê°€ì¤‘ì¹˜ëŠ” ê³ ì •, LoRA ë¶€ë¶„ë§Œ í•™ìŠµ
        return self.lora_B(self.lora_A(x)) * self.scaling
```

#### LoRAì˜ ì¥ì 

- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ 0.1-1%ë§Œ í•™ìŠµ
- **ë¹ ë¥¸ í•™ìŠµ**: í•™ìŠµ ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ê°ì†Œ
- **ëª¨ë“ˆí™”**: ì—¬ëŸ¬ íƒœìŠ¤í¬ë³„ LoRA ì–´ëŒ‘í„°ë¥¼ ì‰½ê²Œ êµì²´ ê°€ëŠ¥

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
raft/
â”œâ”€â”€ README.md                          # í”„ë¡œì íŠ¸ ì„¤ëª…ì„œ (ì´ íŒŒì¼)
â”œâ”€â”€ 2403.10131v2.pdf                   # RAFT ë…¼ë¬¸ ì›ë³¸
â”œâ”€â”€ 01_ë°ì´í„°ì¤€ë¹„/                      # ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¤€ë¹„
â”‚   â”œâ”€â”€ 01_klue-mrc_negative_samples.ipynb          # Negative sampling
â”‚   â”œâ”€â”€ 02_klue_mrc_prompt_docs_5_answer.ipynb      # 5ê°œ ë¬¸ì„œ ì¡°í•© ë°ì´í„°
â”‚   â”œâ”€â”€ 03_klue_mrc_prompt_docs_1_4_answer.ipynb    # 1+4 ë¬¸ì„œ ì¡°í•© ë°ì´í„°  
â”‚   â”œâ”€â”€ 04_klue_mrc_nominal_question_docs_1_5_answer.ipynb # ëª…ì‚¬í˜• ì§ˆë¬¸ ë°ì´í„°
â”‚   â”œâ”€â”€ 05_klue_mrc_prompt_multidocs_answer.ipynb   # ë‹¤ì¤‘ ë¬¸ì„œ ë³µí•© ì§ˆë¬¸
â”‚   â”œâ”€â”€ klue-mrc-v1.1_train.json                   # KLUE-MRC ì›ë³¸ ë°ì´í„°
â”‚   â””â”€â”€ *.csv                                       # ì „ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ë“¤
â”œâ”€â”€ 02_finetuning/                     # ëª¨ë¸ íŒŒì¸íŠœë‹ ë° ì¶”ë¡ 
â”‚   â”œâ”€â”€ 06_fine_tuning.ipynb                       # RAFT íŒŒì¸íŠœë‹ ì‹¤í–‰
â”‚   â”œâ”€â”€ 07_vllm_rag.ipynb                          # vLLM RAG ì‹œìŠ¤í…œ êµ¬í˜„
â”‚   â””â”€â”€ [GIP] 2025 êµ­ê°€ë³„ ICT ì‹œì¥ë™í–¥_ë³´ê³ ì„œ_ë¯¸êµ­.pdf  # RAG í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ
```

## ğŸ”§ í™˜ê²½ ì„¤ì •

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

- **GPU**: NVIDIA GPU (CUDA ì§€ì›, 16GB+ VRAM ê¶Œì¥)
- **ë©”ëª¨ë¦¬**: 32GB+ RAM ê¶Œì¥
- **ì €ì¥ê³µê°„**: 100GB+ ì—¬ìœ  ê³µê°„
- **Python**: 3.8+

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Hugging Face ìƒíƒœê³„
pip install transformers datasets accelerate tokenizers

# íŒŒì¸íŠœë‹ ê´€ë ¨
pip install peft trl bitsandbytes

# vLLM ì„¤ì¹˜
pip install vllm

# RAG ê´€ë ¨
pip install langchain langchain-community langchain-huggingface
pip install chromadb pypdf sentence-transformers

# ì„ë² ë”© ë° ìœ í‹¸ë¦¬í‹°
pip install FlagEmbedding scikit-learn pandas numpy tqdm

# ì‹œê°í™” ë° ê¸°íƒ€
pip install matplotlib seaborn jupyter
```

### GPU í™˜ê²½ í™•ì¸

```python
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU count: {torch.cuda.device_count()}")
print(f"Current GPU: {torch.cuda.get_device_name(0)}")
```

## ğŸ“Š ë°ì´í„° ì¤€ë¹„ ê³¼ì •

### 1. KLUE-MRC ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ

```python
# KLUE-MRC v1.1 ë°ì´í„°ì…‹ (46MB)
!wget https://github.com/KLUE-benchmark/KLUE/raw/refs/heads/main/klue_benchmark/klue-mrc-v1.1/klue-mrc-v1.1_train.json
```

### 2. Negative Sampling ì „ëµ

RAFTì˜ í•µì‹¬ì€ **Hard Negative Sampling**ì…ë‹ˆë‹¤. ëª¨ë¸ì´ í˜¼ë™í•  ìˆ˜ ìˆëŠ” ìœ ì‚¬í•˜ì§€ë§Œ ë¶€ì •í™•í•œ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```python
# BGE-M3 ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
from FlagEmbedding import BGEM3FlagModel

model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ìƒìœ„ 4ê°œ negative samples ì¶”ì¶œ
similarity_matrix = cosine_similarity(question_embeds, context_embeds)
np.fill_diagonal(similarity_matrix, -np.inf)  # ìê¸° ìì‹  ì œì™¸

topk_idx = np.argpartition(similarity_matrix, -4, axis=1)[:, -4:]
negative_samples = [[context_list[i] for i in row] for row in topk_idx]
```

### 3. ë‹¤ì–‘í•œ ë°ì´í„° ì¡°í•© ìƒì„±

#### A. 5ê°œ ë¬¸ì„œ ì¡°í•© (1 Positive + 4 Negative)
```python
docs = [positive_context] + negative_samples[:4]
```

#### B. ë³µí•© ì§ˆë¬¸ ìƒì„± (Multi-Document QA)
```python
# ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì¢…í•©í•´ì•¼ í•˜ëŠ” ë³µì¡í•œ ì§ˆë¬¸ ìƒì„±
question = "Aì™€ Bì˜ ì°¨ì´ì ê³¼ Cì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
```

### 4. ë°ì´í„° í¬ë§· ë³€í™˜

OpenAI Chat Completion í˜•ì‹ìœ¼ë¡œ ë³€í™˜:

```python
def format_data(row):
    system_prompt = """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì—¬ëŸ¬ ë¬¸ì„œ(docs)ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìµœëŒ€í•œ ì •í™•í•˜ê²Œ, ê·¸ë¦¬ê³  ë¬¸ì„œ ë‚´ì—ì„œë§Œ ì •ë³´ë¥¼ ê·¼ê±°ë¡œ í•˜ì—¬ ë‹µë³€í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤.
    
ì•„ë˜ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”:
- ë‹µë³€ì€ ë°˜ë“œì‹œ docsì—ì„œ ì°¾ì€ ë‚´ìš©ì— í•œí•´ì„œë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
- ë‹µë³€ì—ì„œ ì¸ìš©í•˜ëŠ” ë¶€ë¶„ì´ ìˆë‹¤ë©´, ë°˜ë“œì‹œ í•´ë‹¹ ë¬¸ì„œì˜ ë²ˆí˜¸(ì˜ˆ: [[doc1]], [[doc2]])ë¡œ ê·¼ê±°ë¥¼ í‘œì‹œí•´ ì£¼ì„¸ìš”.
- ëª¨ë“  ë‹µë³€ì€ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”."""

    docs_str = '\n'.join([f"doc{i+1}: {doc}" for i, doc in enumerate(row['docs'])])
    
    user_prompt = f"""ì§ˆë¬¸: {row['question']}

docs:
{docs_str}

ìœ„ì˜ docs ì¤‘ì—ì„œë§Œ ì •ë³´ë¥¼ ê·¼ê±°ë¡œ í•˜ì—¬, ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”."""

    return {
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
            {"role": "assistant", "content": row['answer']}
        ]
    }
```

## ğŸ¯ íŒŒì¸íŠœë‹ ê³¼ì •

### 1. ë² ì´ìŠ¤ ëª¨ë¸ ì„ íƒ

```python
# NCSOFTì˜ í•œêµ­ì–´ íŠ¹í™” Llama ëª¨ë¸ ì‚¬ìš©
pretrained_model_name = "NCSOFT/Llama-VARCO-8B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    pretrained_model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ 16ë¹„íŠ¸ ì •ë°€ë„
)
```

### 2. LoRA ì„¤ì •

```python
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=32,           # LoRA ìŠ¤ì¼€ì¼ë§ íŒ©í„°
    lora_dropout=0.1,        # ê³¼ì í•© ë°©ì§€
    r=8,                     # LoRA ë­í¬ (ì •ë³´ëŸ‰/ë³µì¡ë„)
    bias="none",             # bias íŒŒë¼ë¯¸í„° í•™ìŠµ ì•ˆí•¨
    target_modules=["q_proj", "v_proj"],  # Attentionì˜ Q,V projectionì—ë§Œ ì ìš©
    task_type="CAUSAL_LM",   # ì–¸ì–´ ìƒì„± íƒœìŠ¤í¬
)
```

### 3. í•™ìŠµ ì„¤ì •

```python
from trl import SFTConfig, SFTTrainer

args = SFTConfig(
    output_dir="llama3-8b-rag-ko",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=2,    # ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸°: 2*2=4
    gradient_checkpointing=True,      # ë©”ëª¨ë¦¬ ì ˆì•½
    optim="adamw_torch_fused",       # ìµœì í™”ëœ AdamW
    learning_rate=1e-4,              # í•™ìŠµë¥ 
    bf16=True,                       # bfloat16 ì—°ì‚°
    max_seq_length=8192,             # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
    logging_steps=10,
    save_steps=50,
)
```

### 4. ë°ì´í„° ì½œë ˆì´í„° (í•µì‹¬!)

RAFTì˜ í•µì‹¬ì€ **ì •ë‹µ ë¶€ë¶„ë§Œ í•™ìŠµ**í•˜ë„ë¡ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:

```python
def data_collator(batch):
    new_batch = {"input_ids": [], "attention_mask": [], "labels": []}
    
    for example in batch:
        # LLaMA-3 ì±„íŒ… í…œí”Œë¦¿ ì ìš©
        prompt = "<|begin_of_text|>"
        for msg in example["messages"]:
            role = msg["role"]
            content = msg["content"].strip()
            prompt += f"<|start_header_id|>{role}<|end_header_id|>\n{content}<|eot_id|>"
        
        # í† í¬ë‚˜ì´ì§•
        tokenized = tokenizer(prompt, truncation=True, max_length=max_seq_length)
        input_ids = tokenized["input_ids"]
        labels = [-100] * len(input_ids)  # ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  í† í° ë¬´ì‹œ
        
        # assistant ë‹µë³€ ë¶€ë¶„ë§Œ í•™ìŠµ ëŒ€ìƒìœ¼ë¡œ ì„¤ì •
        assistant_start = "<|start_header_id|>assistant<|end_header_id|>\n"
        assistant_tokens = tokenizer.encode(assistant_start, add_special_tokens=False)
        
        # assistant êµ¬ê°„ ì°¾ì•„ì„œ labelsì— ì‹¤ì œ í† í° ê°’ ë³µì‚¬
        for i in range(len(input_ids) - len(assistant_tokens)):
            if input_ids[i:i+len(assistant_tokens)] == assistant_tokens:
                start_idx = i + len(assistant_tokens)
                # <|eot_id|>ê¹Œì§€ ì°¾ì•„ì„œ í•´ë‹¹ êµ¬ê°„ì„ í•™ìŠµ ëŒ€ìƒìœ¼ë¡œ ì„¤ì •
                # ... (ìƒì„¸ êµ¬í˜„ì€ ì½”ë“œ ì°¸ì¡°)
        
        new_batch["input_ids"].append(input_ids)
        new_batch["labels"].append(labels)
    
    return new_batch
```

### 5. í•™ìŠµ ì‹¤í–‰

```python
trainer = SFTTrainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    data_collator=data_collator,
    peft_config=peft_config
)

trainer.train()  # í•™ìŠµ ì‹œì‘!
```

## ğŸš€ vLLMì„ í™œìš©í•œ RAG ì‹œìŠ¤í…œ

### 1. ëª¨ë¸ ë³‘í•© ë° ì¤€ë¹„

```python
from peft import AutoPeftModelForCausalLM

# LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©
peft_model = AutoPeftModelForCausalLM.from_pretrained("checkpoint-846")
merged_model = peft_model.merge_and_unload()

# vLLMìš©ìœ¼ë¡œ ì €ì¥
merged_model.save_pretrained("./llama3-8b-rag-ko-merged")
```

### 2. vLLM ì„œë²„ ì´ˆê¸°í™”

```python
from vllm import LLM, SamplingParams

# vLLM ëª¨ë¸ ë¡œë“œ
vllm_model = LLM(
    model="./llama3-8b-rag-ko-merged",
    dtype="bfloat16",
    gpu_memory_utilization=0.5  # GPU ë©”ëª¨ë¦¬ì˜ 50% ì‚¬ìš©
)

# ìƒì„± íŒŒë¼ë¯¸í„°
sampling_params = SamplingParams(
    temperature=0,      # ê²°ì •ì  ìƒì„±
    max_tokens=1024,    # ìµœëŒ€ ìƒì„± í† í°
)
```

### 3. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•

```python
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹
loader = PyPDFLoader("document.pdf")
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
)
chunks = text_splitter.split_documents(documents)

# ì„ë² ë”© ëª¨ë¸
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': 'cuda'}
)

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
vectorstore = Chroma.from_documents(chunks, embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
```

### 4. RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„

```python
def generate_rag_response(question, retriever, tokenizer, vllm_model, sampling_params):
    # 1. ë¬¸ì„œ ê²€ìƒ‰
    retrieved_docs = retriever.invoke(question)
    
    # 2. ë¬¸ì„œ í¬ë§·íŒ…
    docs_str = '\n'.join([f"doc{i+1}: {doc.page_content}" 
                         for i, doc in enumerate(retrieved_docs)])
    
    # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"""ì§ˆë¬¸: {question}

docs:
{docs_str}

ìœ„ì˜ docs ì¤‘ì—ì„œë§Œ ì •ë³´ë¥¼ ê·¼ê±°ë¡œ í•˜ì—¬, ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”."""}
    ]
    
    # 4. ì±„íŒ… í…œí”Œë¦¿ ì ìš©
    prompt = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    
    # 5. vLLMìœ¼ë¡œ ìƒì„±
    outputs = vllm_model.generate([prompt], sampling_params)
    response = outputs[0].outputs[0].text
    
    return response
```

## ğŸ® ì‹¤í–‰ ê°€ì´ë“œ

### ë‹¨ê³„ë³„ ì‹¤í–‰ ìˆœì„œ

1. **í™˜ê²½ ì„¤ì •**
   ```bash
   git clone <repository>
   cd raft
   pip install -r requirements.txt
   ```

2. **ë°ì´í„° ì¤€ë¹„**
   ```bash
   # 01_ë°ì´í„°ì¤€ë¹„ í´ë”ì˜ ë…¸íŠ¸ë¶ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰
   jupyter notebook 01_ë°ì´í„°ì¤€ë¹„/01_klue-mrc_negative_samples.ipynb
   ```

3. **ëª¨ë¸ íŒŒì¸íŠœë‹**
   ```bash
   jupyter notebook 02_finetuning/06_fine_tuning.ipynb
   ```

4. **RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸**
   ```bash
   jupyter notebook 02_finetuning/07_vllm_rag.ipynb
   ```

### ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ

#### LoRA ì„¤ì •
- `r=8`: ì¼ë°˜ì ì¸ ì„¤ì •, ì„±ëŠ¥ í–¥ìƒì´ í•„ìš”í•˜ë©´ 16ìœ¼ë¡œ ì¦ê°€
- `lora_alpha=32`: rì˜ 4ë°° ì •ë„ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì 
- `lora_dropout=0.1`: ê³¼ì í•© ë°©ì§€, 0.05-0.2 ë²”ìœ„ì—ì„œ ì¡°ì •

#### í•™ìŠµ ì„¤ì •
- `learning_rate=1e-4`: LoRAì˜ ì¼ë°˜ì ì¸ í•™ìŠµë¥ 
- `batch_size`: GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • (2-8)
- `gradient_accumulation_steps`: ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸° ì¡°ì •

#### vLLM ì„¤ì •
- `gpu_memory_utilization=0.5`: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (0.3-0.9)
- `max_tokens=1024`: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”

```python
# ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì„¤ì •
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    load_in_8bit=True,  # 8ë¹„íŠ¸ ì–‘ìí™”
    # load_in_4bit=True,  # ë” ê·¹ë‹¨ì ì¸ ë©”ëª¨ë¦¬ ì ˆì•½
)
```

## âš¡ ì„±ëŠ¥ ìµœì í™” íŒ

### 1. ë°ì´í„° ìµœì í™”

- **ì²­í‚¹ ì „ëµ**: ë¬¸ì„œ ê¸¸ì´ì— ë”°ë¥¸ ìµœì  ì²­í¬ í¬ê¸° ì„¤ì •
- **Negative Sampling**: ë„ˆë¬´ ì‰½ê±°ë‚˜ ì–´ë ¤ìš´ negative ì œê±°
- **ë°ì´í„° í’ˆì§ˆ**: ì¤‘ë³µ ì œê±°, ë…¸ì´ì¦ˆ ë°ì´í„° ì •ì œ

### 2. ëª¨ë¸ ìµœì í™”

- **Mixed Precision**: bf16 ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ë° ì†ë„ ê°œì„ 
- **Gradient Checkpointing**: ë©”ëª¨ë¦¬ ì ˆì•½
- **Flash Attention**: ê°€ëŠ¥í•œ ê²½ìš° Flash Attention 2.0 ì‚¬ìš©

### 3. ì¶”ë¡  ìµœì í™”

- **vLLM ì„¤ì •**: ì ì ˆí•œ `gpu_memory_utilization` ê°’ ì„¤ì •
- **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ ìš”ì²­ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬
- **KV ìºì‹œ**: PagedAttentionì˜ ì´ì  ìµœëŒ€ í™œìš©

### 4. ì‹œìŠ¤í…œ ìµœì í™”

```python
# CUDA ìµœì í™” ì„¤ì •
import torch
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True
```

## ğŸ“ˆ ì„±ëŠ¥ í‰ê°€ ë° ëª¨ë‹ˆí„°ë§

### í‰ê°€ ë©”íŠ¸ë¦­

1. **ì •í™•ì„± (Accuracy)**: ì •ë‹µ ë¬¸ì„œì—ì„œ ì˜¬ë°”ë¥¸ ì •ë³´ ì¶”ì¶œ ì—¬ë¶€
2. **ì¸ìš© ì •í™•ë„ (Citation Accuracy)**: ì˜¬ë°”ë¥¸ ë¬¸ì„œ ë²ˆí˜¸ ì¸ìš© ì—¬ë¶€
3. **ë¬´ê´€ ë¬¸ì„œ ë¬´ì‹œìœ¨**: Negative ë¬¸ì„œë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë¬´ì‹œí•˜ëŠ”ê°€
4. **ì‘ë‹µ í’ˆì§ˆ**: ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ìƒì„± ì—¬ë¶€

### í‰ê°€ ì½”ë“œ ì˜ˆì‹œ

```python
def evaluate_rag_system(test_dataset, generate_func):
    correct_answers = 0
    correct_citations = 0
    
    for example in test_dataset:
        question = example['question']
        expected_answer = example['answer']
        
        generated_answer = generate_func(question)
        
        # ì •í™•ì„± í‰ê°€
        if check_answer_correctness(generated_answer, expected_answer):
            correct_answers += 1
            
        # ì¸ìš© í‰ê°€
        if check_citation_correctness(generated_answer, example['doc_citations']):
            correct_citations += 1
    
    accuracy = correct_answers / len(test_dataset)
    citation_accuracy = correct_citations / len(test_dataset)
    
    return accuracy, citation_accuracy
```

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

#### 1. CUDA Out of Memory
```python
# í•´ê²°ì±… 1: ë°°ì¹˜ í¬ê¸° ê°ì†Œ
per_device_train_batch_size=1
gradient_accumulation_steps=4

# í•´ê²°ì±… 2: ëª¨ë¸ ì–‘ìí™”
load_in_8bit=True

# í•´ê²°ì±… 3: Gradient Checkpointing
gradient_checkpointing=True
```

#### 2. vLLM ì´ˆê¸°í™” ì‹¤íŒ¨
```python
# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¡°ì •
gpu_memory_utilization=0.3  # ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •

# ë˜ëŠ” CPU ì‚¬ìš©
device_map="cpu"
```

#### 3. í† í¬ë‚˜ì´ì € ì˜¤ë¥˜
```python
# íŒ¨ë”© í† í° ì„¤ì •
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
```

#### 4. ìƒì„± í’ˆì§ˆ ì €í•˜
- í•™ìŠµë¥  ì¡°ì •: `1e-4 â†’ 5e-5`
- LoRA ë­í¬ ì¦ê°€: `r=8 â†’ r=16`
- ë” ë§ì€ ì—í¬í¬: `epochs=3 â†’ epochs=5`

## ğŸ”¬ ì‹¤í—˜ ë° ì—°êµ¬ ë°©í–¥

### ì¶”ê°€ ì‹¤í—˜ ì•„ì´ë””ì–´

1. **ë‹¤ì–‘í•œ Negative Sampling ì „ëµ**
   - BM25 ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
   - ì˜ë¯¸ì  ìœ ì‚¬ë„ + í‚¤ì›Œë“œ ë§¤ì¹­ ì¡°í•©

2. **ëª¨ë¸ í¬ê¸°ë³„ ì„±ëŠ¥ ë¹„êµ**
   - 7B vs 13B vs 70B ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„
   - íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„± vs ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„

3. **ë„ë©”ì¸ ì ì‘ ì—°êµ¬**
   - ì˜ë£Œ, ë²•ë¥ , ê¸°ìˆ  ë¬¸ì„œ ë“± íŠ¹í™” ë„ë©”ì¸
   - ë„ë©”ì¸ë³„ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰

4. **ë‹¤êµ­ì–´ í™•ì¥**
   - ì˜ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´ ë“±ìœ¼ë¡œ í™•ì¥
   - ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ í™œìš©

## ğŸ“š ì°¸ê³  ìë£Œ

### í•µì‹¬ ë…¼ë¬¸

1. **RAFT**: [Adapting Language Model to Domain Specific RAG](https://arxiv.org/abs/2403.10131)
2. **vLLM**: [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
3. **LoRA**: [Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

### ìœ ìš©í•œ ë¦¬ì†ŒìŠ¤

- [Hugging Face Transformers ë¬¸ì„œ](https://huggingface.co/docs/transformers/)
- [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/)
- [LangChain RAG ê°€ì´ë“œ](https://python.langchain.com/docs/tutorials/rag/)
- [KLUE ë²¤ì¹˜ë§ˆí¬](https://github.com/KLUE-benchmark/KLUE)

### ì¶”ê°€ í•™ìŠµ ìë£Œ

- **ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸**
  - [RAFT ê¸°ë²• ìƒì„¸ ë¶„ì„](https://blog.example.com/raft-analysis)
  - [vLLM ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ](https://blog.example.com/vllm-optimization)

- **YouTube ê°•ì˜**
  - [RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì‹¤ì „ ê°€ì´ë“œ](https://youtube.com/watch?v=example)
  - [LoRA íŒŒì¸íŠœë‹ ë§ˆìŠ¤í„°í´ë˜ìŠ¤](https://youtube.com/watch?v=example)

## ğŸ¤ ê¸°ì—¬ ë° í”¼ë“œë°±

### ê¸°ì—¬ ë°©ë²•

1. **ì´ìŠˆ ë¦¬í¬íŒ…**: ë²„ê·¸ë‚˜ ê°œì„ ì‚¬í•­ ì œì•ˆ
2. **ì½”ë“œ ê¸°ì—¬**: Pull Requestë¥¼ í†µí•œ ì½”ë“œ ê°œì„ 
3. **ë¬¸ì„œ ê°œì„ **: READMEë‚˜ ì£¼ì„ ê°œì„ 
4. **ì‹¤í—˜ ê²°ê³¼ ê³µìœ **: ìƒˆë¡œìš´ ì‹¤í—˜ ê²°ê³¼ë‚˜ ìµœì í™” ë°©ë²• ê³µìœ 

---

> **ğŸ’¡ ì‹ ì… ì—”ì§€ë‹ˆì–´ë¥¼ ìœ„í•œ ì¡°ì–¸**
> 
> ì´ í”„ë¡œì íŠ¸ëŠ” ìµœì‹  LLM ê¸°ìˆ ë“¤ì´ ì§‘ì•½ëœ ì¢…í•© í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ë³µì¡í•´ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ, ê° ë‹¨ê³„ë¥¼ ì°¨ê·¼ì°¨ê·¼ ë”°ë¼ê°€ë©° ì‹¤ìŠµí•´ë³´ì„¸ìš”. íŠ¹íˆ ë‹¤ìŒ ìˆœì„œë¡œ í•™ìŠµí•˜ì‹œê¸¸ ê¶Œí•©ë‹ˆë‹¤:
> 
> 1. **ê¸°ì´ˆ ê°œë… ì´í•´**: RAFT, vLLM, LoRAì˜ í•µì‹¬ ì•„ì´ë””ì–´ íŒŒì•…
> 2. **ì‘ì€ ë°ì´í„°ë¡œ ì‹¤í—˜**: ì „ì²´ ë°ì´í„°ì…‹ ëŒ€ì‹  ìƒ˜í”Œ ë°ì´í„°ë¡œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
> 3. **ë‹¨ê³„ì  í™•ì¥**: ì„±ê³µì ìœ¼ë¡œ ì‘ë™í•˜ë©´ ì ì§„ì ìœ¼ë¡œ ë°ì´í„°ì™€ ëª¨ë¸ í¬ê¸° í™•ì¥
> 4. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ê° ë‹¨ê³„ì—ì„œ ë©”íŠ¸ë¦­ì„ ì¸¡ì •í•˜ê³  ê°œì„ ì  íŒŒì•…
> 5. **ì‹¤ë¬´ ì ìš©**: ìì‹ ì˜ ë„ë©”ì¸ì— ë§ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì§•
> 
> ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ì´ìŠˆë¥¼ í†µí•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

---

**ğŸš€ Happy Coding! ë©‹ì§„ RAG ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ë³´ì„¸ìš”!**
