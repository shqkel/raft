{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "-8AUkrkVYcrO",
   "metadata": {
    "id": "-8AUkrkVYcrO"
   },
   "source": [
    "# vLLM \n",
    "\n",
    "vLLM은 대규모 언어 모델의 추론 성능을 최적화하는 라이브러리입니다.\n",
    "\n",
    "**주요 특징:**\n",
    "\n",
    "- **PagedAttention**: 메모리 효율적인 attention 메커니즘\n",
    "- **Continuous Batching**: 동적 배치 처리로 처리량 향상\n",
    "- **Fast Sampling**: 효율적인 토큰 생성\n",
    "- **OpenAI Compatible API**: OpenAI API와 호환되는 서빙 인터페이스\n",
    "\n",
    "**성능 향상:**\n",
    "\n",
    "- HuggingFace Transformers 대비 최대 24배 빠른 처리량\n",
    "- GPU 메모리 사용량 최적화\n",
    "- 높은 동시 요청 처리 능력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998eb56a-0b31-49a5-b5a2-4cf37b433024",
   "metadata": {},
   "source": [
    "## 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa04369-0c54-4e0f-b81a-127e858dffd8",
   "metadata": {
    "id": "aaa04369-0c54-4e0f-b81a-127e858dffd8",
    "outputId": "8804a290-8d71-473c-d1c2-15cd0ea7c437",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain_huggingface\n",
      "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting vllm\n",
      "  Downloading vllm-0.10.1.1-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.4.21-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.43-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain-community)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.1.2)\n",
      "Collecting tokenizers>=0.19.1 (from langchain_huggingface)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.8.0.dev20250319+cu128)\n",
      "Collecting cachetools (from vllm)\n",
      "  Downloading cachetools-6.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting sentencepiece (from vllm)\n",
      "  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting blake3 (from vllm)\n",
      "  Downloading blake3-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting py-cpuinfo (from vllm)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting protobuf (from vllm)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting openai>=1.99.1 (from vllm)\n",
      "  Downloading openai-1.102.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.0.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm)\n",
      "  Downloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
      "  Downloading lm_format_enforcer-0.10.12-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<0.8.0,>=0.7.11 (from vllm)\n",
      "  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting outlines_core==0.2.10 (from vllm)\n",
      "  Downloading outlines_core-0.2.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting diskcache==5.6.3 (from vllm)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting lark==1.2.2 (from vllm)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.21 (from vllm)\n",
      "  Downloading xgrammar-0.1.21-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.12.2)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (26.3.0)\n",
      "Collecting msgspec (from vllm)\n",
      "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf>=0.13.0 (from vllm)\n",
      "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mistral_common>=1.8.2 (from mistral_common[audio,image]>=1.8.2->vllm)\n",
      "  Downloading mistral_common-1.8.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm)\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting einops (from vllm)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.10.2 (from vllm)\n",
      "  Downloading compressed_tensors-0.10.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.19.0 (from vllm)\n",
      "  Downloading depyf-0.19.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting cloudpickle (from vllm)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (3.3.0)\n",
      "Collecting scipy (from vllm)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting ninja (from vllm)\n",
      "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting pybase64 (from vllm)\n",
      "  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting cbor2 (from vllm)\n",
      "  Downloading cbor2-5.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting setproctitle (from vllm)\n",
      "  Downloading setproctitle-1.3.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting openai-harmony>=0.0.3 (from vllm)\n",
      "  Downloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Collecting numba==0.61.2 (from vllm)\n",
      "  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading ray-2.49.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting torch>=1.13.0 (from peft)\n",
      "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchaudio==2.7.1 (from vllm)\n",
      "  Downloading torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchvision==0.22.1 (from vllm)\n",
      "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting xformers==0.0.31 (from vllm)\n",
      "  Downloading xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting astor (from depyf==0.19.0->vllm)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dill (from depyf==0.19.0->vllm)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
      "  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.3)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch>=1.13.0->peft)\n",
      "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch>=1.13.0->peft) (77.0.1)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.74.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.17.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Downloading orjson-3.11.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting starlette<0.48.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading starlette-0.47.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi_cli-0.0.10-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.13.0->peft)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.23.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.24.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.1 kB)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer<0.11,>=0.10.11->vllm)\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm)\n",
      "  Downloading pydantic_extra_types-2.10.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.1->vllm) (1.7.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.99.1->vllm)\n",
      "  Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.99.1->vllm) (1.3.1)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting click>=7.0 (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading cupy_cuda12x-13.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading rich_toolkit-0.15.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi_cloud_cli-0.1.5-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting cachetools (from vllm)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading fastrlock-0.8.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting soundfile>=0.12.1 (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting soxr>=0.5.0 (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm)\n",
      "  Downloading soxr-0.5.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading rignore-0.6.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sentry-sdk>=2.20.0 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading sentry_sdk-2.35.2-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.22)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
      "Downloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading vllm-0.10.1.1-cp38-abi3-manylinux1_x86_64.whl (414.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.4/414.4 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading compressed_tensors-0.10.2-py3-none-any.whl (169 kB)\n",
      "Downloading depyf-0.19.0-py3-none-any.whl (39 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m239.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outlines_core-0.2.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl (117.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m180.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xgrammar-0.1.21-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m161.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m142.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m164.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m301.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m138.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m124.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m167.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m153.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m187.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m124.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m169.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m142.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m237.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "Downloading build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Downloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
      "Downloading grpcio-1.74.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m227.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m351.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.75-py3-none-any.whl (443 kB)\n",
      "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Downloading langsmith-0.4.21-py3-none-any.whl (378 kB)\n",
      "Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lm_format_enforcer-0.10.12-py3-none-any.whl (44 kB)\n",
      "Downloading mistral_common-1.8.4-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m181.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
      "Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m209.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.102.0-py3-none-any.whl (812 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.0/812.0 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m174.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m229.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "Downloading orjson-3.11.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m207.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Downloading ray-2.49.0-cp311-cp311-manylinux2014_x86_64.whl (70.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m148.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m176.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading sqlalchemy-2.0.43-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m163.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m239.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typer-0.17.3-py3-none-any.whl (46 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
      "Downloading blake3-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
      "Downloading cbor2-5.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (254 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
      "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m238.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m210.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m141.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
      "Downloading fastapi_cli-0.0.10-py3-none-any.whl (10 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (587 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m139.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (429 kB)\n",
      "Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading pydantic_extra_types-2.10.5-py3-none-any.whl (38 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.47.3-py3-none-any.whl (72 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Downloading zstandard-0.24.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading cupy_cuda12x-13.6.0-cp311-cp311-manylinux2014_x86_64.whl (113.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.0/113.0 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Downloading fastapi_cloud_cli-0.1.5-py3-none-any.whl (18 kB)\n",
      "Downloading fastrlock-0.8.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (54 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m255.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich_toolkit-0.15.0-py3-none-any.whl (29 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-0.5.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading rignore-0.6.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (950 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.6/950.6 kB\u001b[0m \u001b[31m136.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.35.2-py2.py3-none-any.whl (363 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=ea3647698d5ef8fb2bf72db24b6622e386c1535d0a85f7675b3e72ce7dee8793\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, py-cpuinfo, flatbuffers, fastrlock, durationpy, blake3, zstandard, zipp, websockets, uvloop, typing-inspection, triton, tqdm, threadpoolctl, tenacity, soxr, shellingham, setproctitle, sentry-sdk, sentencepiece, scipy, safetensors, rignore, requests, regex, python-multipart, python-dotenv, pyproject_hooks, pypdf, pydantic-core, pycountry, pybase64, pyasn1, protobuf, propcache, partial-json-parser, outlines_core, orjson, opencv-python-headless, oauthlib, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, mypy-extensions, multidict, msgspec, msgpack, mmh3, mdurl, marshmallow, llvmlite, llguidance, lark, jsonpatch, joblib, jiter, jinja2, interegular, importlib-resources, humanfriendly, httpx-sse, httptools, hf-xet, grpcio, greenlet, frozenlist, einops, dnspython, diskcache, dill, cupy-cuda12x, cloudpickle, click, cbor2, cachetools, bcrypt, backoff, astor, annotated-types, aiohappyeyeballs, yarl, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, SQLAlchemy, soundfile, scikit-learn, rsa, requests-toolbelt, requests-oauthlib, pydantic, pyasn1-modules, posthog, opentelemetry-proto, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, numba, markdown-it-py, importlib-metadata, huggingface_hub, googleapis-common-protos, gguf, email-validator, depyf, coloredlogs, build, aiosignal, tokenizers, rich, pydantic-settings, pydantic-extra-types, prometheus-fastapi-instrumentator, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, openai-harmony, openai, onnxruntime, nvidia-cusolver-cu12, lm-format-enforcer, langsmith, google-auth, fastapi, dataclasses-json, aiohttp, typer, transformers, torch, rich-toolkit, ray, opentelemetry-semantic-conventions, langchain-core, kubernetes, xgrammar, xformers, torchvision, torchaudio, sentence-transformers, opentelemetry-sdk, mistral_common, langchain-text-splitters, langchain_huggingface, fastapi-cloud-cli, fastapi-cli, compressed-tensors, accelerate, peft, opentelemetry-exporter-otlp-proto-grpc, langchain, langchain-community, chromadb, vllm\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 1.0.0\n",
      "    Uninstalling zipp-1.0.0:\n",
      "      Successfully uninstalled zipp-1.0.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: oauthlib\n",
      "    Found existing installation: oauthlib 3.2.0\n",
      "    Uninstalling oauthlib-3.2.0:\n",
      "      Successfully uninstalled oauthlib-3.2.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.8.55\n",
      "    Uninstalling nvidia-nvtx-cu12-12.8.55:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.8.55\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.61\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.61:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.61\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.25.1\n",
      "    Uninstalling nvidia-nccl-cu12-2.25.1:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.25.1\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.55\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.55:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.55\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.13.0.11\n",
      "    Uninstalling nvidia-cufile-cu12-1.13.0.11:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.13.0.11\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.57\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.8.57:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.57\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.61\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.61:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.61\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.57\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.8.57:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.57\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.3.14\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.3.14:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.3.14\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.7.53\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.7.53:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.7.53\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.41\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.41:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.41\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.8.0.87\n",
      "    Uninstalling nvidia-cudnn-cu12-9.8.0.87:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.8.0.87\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.6.4\n",
      "    Uninstalling importlib-metadata-4.6.4:\n",
      "      Successfully uninstalled importlib-metadata-4.6.4\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.2.55\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.2.55:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.2.55\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.8.0.dev20250319+cu128\n",
      "    Uninstalling torch-2.8.0.dev20250319+cu128:\n",
      "      Successfully uninstalled torch-2.8.0.dev20250319+cu128\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.22.0.dev20250319+cu128\n",
      "    Uninstalling torchvision-0.22.0.dev20250319+cu128:\n",
      "      Successfully uninstalled torchvision-0.22.0.dev20250319+cu128\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.6.0.dev20250319+cu128\n",
      "    Uninstalling torchaudio-2.6.0.dev20250319+cu128:\n",
      "      Successfully uninstalled torchaudio-2.6.0.dev20250319+cu128\n",
      "Successfully installed SQLAlchemy-2.0.43 accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 astor-0.8.1 backoff-2.2.1 bcrypt-4.3.0 blake3-1.0.5 build-1.3.0 cachetools-5.5.2 cbor2-5.7.0 chromadb-1.0.20 click-8.2.1 cloudpickle-3.1.1 coloredlogs-15.0.1 compressed-tensors-0.10.2 cupy-cuda12x-13.6.0 dataclasses-json-0.6.7 depyf-0.19.0 dill-0.4.0 diskcache-5.6.3 dnspython-2.7.0 durationpy-0.10 einops-0.8.1 email-validator-2.3.0 fastapi-0.116.1 fastapi-cli-0.0.10 fastapi-cloud-cli-0.1.5 fastrlock-0.8.3 flatbuffers-25.2.10 frozenlist-1.7.0 gguf-0.17.1 google-auth-2.40.3 googleapis-common-protos-1.70.0 greenlet-3.2.4 grpcio-1.74.0 hf-xet-1.1.9 httptools-0.6.4 httpx-sse-0.4.1 huggingface_hub-0.34.4 humanfriendly-10.0 importlib-metadata-8.7.0 importlib-resources-6.5.2 interegular-0.3.3 jinja2-3.1.6 jiter-0.10.0 joblib-1.5.2 jsonpatch-1.33 kubernetes-33.1.0 langchain-0.3.27 langchain-community-0.3.29 langchain-core-0.3.75 langchain-text-splitters-0.3.11 langchain_huggingface-0.3.1 langsmith-0.4.21 lark-1.2.2 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.10.12 markdown-it-py-4.0.0 marshmallow-3.26.1 mdurl-0.1.2 mistral_common-1.8.4 mmh3-5.2.0 msgpack-1.1.1 msgspec-0.19.0 multidict-6.6.4 mypy-extensions-1.1.0 ninja-1.13.0 numba-0.61.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 oauthlib-3.3.1 onnxruntime-1.22.1 openai-1.102.0 openai-harmony-0.0.4 opencv-python-headless-4.12.0.88 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 orjson-3.11.3 outlines_core-0.2.10 partial-json-parser-0.2.1.1.post6 peft-0.17.1 posthog-5.4.0 prometheus-fastapi-instrumentator-7.1.0 propcache-0.3.2 protobuf-6.32.0 py-cpuinfo-9.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybase64-1.4.2 pycountry-24.6.1 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-extra-types-2.10.5 pydantic-settings-2.10.1 pypdf-6.0.0 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.1 python-multipart-0.0.20 ray-2.49.0 regex-2025.9.1 requests-2.32.5 requests-oauthlib-2.0.0 requests-toolbelt-1.0.0 rich-14.1.0 rich-toolkit-0.15.0 rignore-0.6.4 rsa-4.9.1 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 sentencepiece-0.2.1 sentry-sdk-2.35.2 setproctitle-1.3.6 shellingham-1.5.4 soundfile-0.13.1 soxr-0.5.0.post1 starlette-0.47.3 tenacity-9.1.2 threadpoolctl-3.6.0 tiktoken-0.11.0 tokenizers-0.22.0 torch-2.7.1 torchaudio-2.7.1 torchvision-0.22.1 tqdm-4.67.1 transformers-4.56.0 triton-3.3.1 typer-0.17.3 typing-inspect-0.9.0 typing-inspection-0.4.1 uvicorn-0.35.0 uvloop-0.21.0 vllm-0.10.1.1 watchfiles-1.1.0 websockets-15.0.1 xformers-0.0.31 xgrammar-0.1.21 yarl-1.20.1 zipp-3.23.0 zstandard-0.24.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain_huggingface transformers peft accelerate vllm chromadb pypdf sentence-transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4736c7e0-3268-4c2b-bbf1-6d9d1ceb8bf6",
   "metadata": {
    "id": "4736c7e0-3268-4c2b-bbf1-6d9d1ceb8bf6",
    "outputId": "84479fc0-1010-4067-a83c-21a507e4a060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 01:47:05 [__init__.py:241] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d743472c-2226-47d1-b3a3-8e4ff8b72fe2",
   "metadata": {
    "id": "d743472c-2226-47d1-b3a3-8e4ff8b72fe2",
    "outputId": "cf14743e-fd9c-4c43-b58a-40e735e99573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF에서 추출된 문서 수: 31\n",
      "첫 번째 문서 샘플: 미국\n"
     ]
    }
   ],
   "source": [
    "pdf_path = '[GIP] 2025 국가별 ICT 시장동향_보고서_미국.pdf'\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "print(f\"PDF에서 추출된 문서 수: {len(documents)}\")\n",
    "print(f\"첫 번째 문서 샘플: {documents[0].page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9213c87b-3e70-4de8-b1ba-610867bea071",
   "metadata": {
    "id": "9213c87b-3e70-4de8-b1ba-610867bea071",
    "outputId": "e12c4540-1ff8-4fc0-ca2e-c0ed089060fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청크 수: 60\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 청킹\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"청크 수: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66439225-4efb-4556-a8a0-f3437e85df4d",
   "metadata": {
    "id": "66439225-4efb-4556-a8a0-f3437e85df4d",
    "outputId": "51c4f75a-335b-4053-8235-56add657ba15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링 전 청크 수: 60\n",
      "필터링 후 청크 수: 59\n",
      "제거된 청크 샘플 (1개): [Document(metadata={'producer': 'Hancom PDF 1.3.0.546', 'creator': 'Hwp 2020 11.0.0.8362', 'creationdate': '2025-04-04T14:14:13+09:00', 'author': 'rnc', 'moddate': '2025-04-04T14:14:13+09:00', 'pdfversion': '1.4', 'source': '[GIP] 2025 국가별 ICT 시장동향_보고서_미국.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1'}, page_content='미국')]\n",
      "--\n",
      "첫 번째 청크 샘플: Ⅰ ICT 국가 산업 현황  04(*) SUMMARY1. 국가 개황2. ICT 정부 기관3. ICT 주요 정책4. ICT 주요 법령 및 규제5. ICT 주요 기업6. 한국 협력 및 국내기업 진출 사례Ⅱ ICT 주요 동향  17(*) SUMMARY1. 인공지능2. 블록체인3. 클라우드 컴퓨팅4. 기업용 솔루션(SW)5. 빅데이터 Ⅲ 스타트업 생태계 24(*) SUMMARY1. 코그니션 AI(Cognition AI)2. 모나드 랩스(Monad Labs)3. 사이애라(Cyera)4. 플래드(Plaid)5. 웨이모(Waymo)※ 참고 문헌\n"
     ]
    }
   ],
   "source": [
    "# 길이가 10 이하인 문서 필터링\n",
    "filtered_chunks = [chunk for chunk in chunks if len(chunk.page_content) > 10]\n",
    "print(f\"필터링 전 청크 수: {len(chunks)}\")\n",
    "print(f\"필터링 후 청크 수: {len(filtered_chunks)}\")\n",
    "\n",
    "# 필터링된 청크가 있으면 제거된 내용 샘플 출력\n",
    "if len(chunks) != len(filtered_chunks):\n",
    "    removed = [chunk for chunk in chunks if len(chunk.page_content) <= 10]\n",
    "    print(f\"제거된 청크 샘플 ({len(removed)}개): {removed}\")\n",
    "    print('--')\n",
    "\n",
    "chunks = filtered_chunks\n",
    "print(f\"첫 번째 청크 샘플: {chunks[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c59fe34-9099-4351-ac6f-774eee3d8b5a",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "- 임베딩 모델 로드\n",
    "- 벡터스토어 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ad19b7e-b73b-4f7f-b959-b562fda3bb87",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "7d32e1d49d984be6b49b4f61d6b6b081",
      "342391dcfcb4453bbcaa4707566ea40b",
      "ad439ef330c94f2cb70306e67c697b57",
      "58c616f0bdcb4edbb0aef91c6101a88d",
      "d2a75a1ae77e43deb19b899b58d7d682",
      "a82b87fe0ecb4444bb888d81ee955ddd",
      "8d4e4815fc784ee49a4f2a822cb910df",
      "60d7b114cd894ecba8bd316c8f73ff7e",
      "41e31fa72c6c424b97568ddf1844b0cd",
      "07203829279a416ea5b4bd616585adbc",
      "00c6ce0088a64408becf53e6b08f3bf4",
      "5b4633e2428a416da5e29546cdeef7bc"
     ]
    },
    "id": "1ad19b7e-b73b-4f7f-b959-b562fda3bb87",
    "outputId": "5d0ace70-2ec6-41df-9d59-68f88c911817"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8682c40eaa040039f382e8039dc3c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd39efe416d440b29c48a18110d737d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147cc17cd1f14bb28fa65f22df6c8618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bbfa7c10694a7c8277e6b4259e518c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612bd8238e424db29be05deadb391b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67882f777540494983b6b3521367d89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc90263412841fab5f430bd9bce5c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fe03c46aae42f1a9c6bf42327e6484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4067cacfbcee41f8ab0c3f61b25d9705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4075b07ae3e64db7b55404f6a28312c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb614081b0f64592888d40e40b59d08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82bbf8923864838926071d4f95792bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 임베딩 모델\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={'device': 'cuda'} # GPU 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "546b83fe-8969-4c1b-be31-34e1780f048d",
   "metadata": {
    "id": "546b83fe-8969-4c1b-be31-34e1780f048d",
    "outputId": "f85e04f5-0fe8-4d27-cd55-c87a19604d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 DB가 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(chunks, embeddings)\n",
    "print(f\"벡터 DB가 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e58f780-2b49-4be5-ad6e-f19a29972f4b",
   "metadata": {
    "id": "4e58f780-2b49-4be5-ad6e-f19a29972f4b",
    "outputId": "118276d8-f077-484c-9cbf-dc11da0935c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 문서 수: 3\n",
      "첫 번째 검색 결과: 국가별 ICT 시장동향 93. ICT 주요 정책① 미국 인공지능 리더십 장벽 제거* Federal Register, Removing Barriers to American Leadership in Artificial Intelligence, 2025.01.23.\n",
      "n(주요  내용) 미국  AI 발전  저해 정 책 폐지  및 글로 벌 리더 십 강화  행동 계 획 추진-2025년 1월 23일 백악관에서 발표한 행정명령*으로, 이전 행정부의 행정명령 14110(안전하고 신뢰할 수 있는 AI 개발 및 사용)을 철회하고 해당 명령에 따른 정책과 지침 재검토 지시-AI 시스템에서 이념적 편향과 사회적 의제 배제를 강조하고, 경제 경쟁력과 국가 안보 증진에 초점을 맞춘 정책 방향 설정n(배경) 이전 행정부의 AI 정책 보다 유연한 혁신 환경 조성 필요성 대두-기존 AI 정책이 과도한 규제와 특정 가치 지향적 접근으로 기술 혁신과 경쟁력을 저해한다는 산업계 및 정치적 비판 증가-자유 시장, 세계적 수\n"
     ]
    }
   ],
   "source": [
    "# 검색 테스트\n",
    "query = \"미국의 ICT 정책의 주요 내용은 무엇인가요?\"\n",
    "docs = vectorstore.similarity_search(query, k=3)\n",
    "print(f\"검색된 문서 수: {len(docs)}\")\n",
    "print(f\"첫 번째 검색 결과: {docs[0].page_content[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b18242-6f26-45cc-a5b1-ab2f4f4ee103",
   "metadata": {},
   "source": [
    "## Retrieval-Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e8b202-2545-4762-b995-0c7185a28707",
   "metadata": {},
   "source": [
    "### Fine-tuned Model from local\n",
    "- 병합후 vLLM모델 객체로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63ab7e-b79b-4f16-babf-49ad0071d753",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "3e55744f6f5a45c590b0955a48d66d04"
     ]
    },
    "id": "ac63ab7e-b79b-4f16-babf-49ad0071d753",
    "outputId": "a1d13a9e-cf79-4630-fc61-a0b79c7fa3c8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"llama3-8b-rag-ko/checkpoint-846\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# LoRA 등 PEFT 어댑터를 base 모델에 합쳐서 \"단일 모델\"로 만듭니다.\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llama3-8b-rag-ko\")  # 베이스 모델 토크나이저\n",
    "\n",
    "# Hugging Face 형식으로 새로운 경로에 저장 (vLLM에서 로딩할 목적)\n",
    "save_path = \"./llama3-8b-rag-ko-full-merged\"\n",
    "merged_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fefbcb2-d048-4418-a2fb-6e6b0347eeab",
   "metadata": {
    "id": "4fefbcb2-d048-4418-a2fb-6e6b0347eeab",
    "outputId": "81d67927-38f6-4037-d647-c81530884c13"
   },
   "outputs": [],
   "source": [
    "# 원본 VLLM 라이브러리 직접 사용\n",
    "# VLLM 모델 설정\n",
    "vllm_model = LLM(\n",
    "    model=\"./llama3-8b-rag-ko-merged\",\n",
    "    dtype=\"bfloat16\",\n",
    "    gpu_memory_utilization=0.5   # 0.5(50%), 0.4(40%) 등 환경에 맞게 조정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb6481-436e-4f21-92e4-13f18210d06e",
   "metadata": {
    "id": "5cbb6481-436e-4f21-92e4-13f18210d06e",
    "outputId": "0b144a19-2148-408c-dc64-9ca7a1632ca9"
   },
   "outputs": [],
   "source": [
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./llama3-8b-rag-ko-merged\")\n",
    "print(f\"모델의 기본 chat template 확인: {tokenizer.chat_template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54eebbe-a4fc-444f-8c80-5a013717305c",
   "metadata": {},
   "source": [
    "### Fine-tuned Model from HuggingFaceHub\n",
    "1. 어댑터모델만 HuggingFaceHub에 업로드해둔 경우, 내려받아서 병합을 진행후, 별도의 병합모델 저장소로 push한다.\n",
    "   (기반모델로 vLLM객체를 만든후 어댑터를 generate함수 호출시마다 연동해 진행하는 것도 가능)\n",
    "2. 이미 병합된 모델을 HuggingFaceHub에 업로드해둔 경우 바로 vLLM객체로 생성한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ae4ce-5f76-4e2f-9a38-28309a0882ee",
   "metadata": {},
   "source": [
    "#### 1. 어댑터모델 병합 및 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db366f86-f4fd-4e13-8fbf-41b07633ed32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 어댑터 모델 로드\n",
    "model_id = 'shqkel/llama3-8b-rag-ko'\n",
    "\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "# 모델 병합: LoRA 등 PEFT 어댑터를 base 모델에 합쳐서 \"단일 모델\"로 만듬\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8bbf1-84f0-4c6e-ba03-6df6e07e1d30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 병합모델 저장소 업로드하기\n",
    "from huggingface_hub import login \n",
    "\n",
    "login(token=\"your_huggingface_hub_token\")\n",
    "\n",
    "repo_id = \"your_huggingface_hub_repo\"  # 미리 생성해둘 필요 없음\n",
    "merged_model.push_to_hub(repo_id, private=False, commit_message=\"Upload model weights\")  \n",
    "tokenizer.push_to_hub(repo_id, commit_message=\"Upload tokenizer files\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d466c-8b16-4f73-85e5-b66b42444f58",
   "metadata": {},
   "source": [
    "#### 2.병합모델로 vLLM객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6236b6eb-1f50-4434-9ed4-1c9078c36565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 01:51:28 [utils.py:326] non-default args: {'model': 'shqkel/llama3-8b-rag-ko-merged', 'dtype': 'bfloat16', 'gpu_memory_utilization': 0.5, 'disable_log_stats': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d848fd5cfba24215969a65bab88eb59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 01:51:36 [__init__.py:711] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 01:51:36 [__init__.py:2816] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 09-02 01:51:36 [__init__.py:1750] Using max model len 8192\n",
      "INFO 09-02 01:51:37 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cee4d5edb5e499f945b24fc1c5b80df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20bf8ce43c743258494d2dd7618f8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c731cdce8f3940ca92492e4f1ab41eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/430 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3aab6b752fa4150bbf7d3d4f5851b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/348 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c7491faa894effa975ae376d9bce9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/143 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-02 01:51:41 [__init__.py:2921] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "INFO 09-02 01:51:45 [__init__.py:241] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:51:46 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:51:46 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='shqkel/llama3-8b-rag-ko-merged', speculative_config=None, tokenizer='shqkel/llama3-8b-rag-ko-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=shqkel/llama3-8b-rag-ko-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:51:48 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m WARNING 09-02 01:51:48 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:51:48 [gpu_model_runner.py:1953] Starting to load model shqkel/llama3-8b-rag-ko-merged...\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:51:48 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:51:48 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:51:49 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:52:12 [weight_utils.py:312] Time spent downloading weights for shqkel/llama3-8b-rag-ko-merged: 23.104621 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:01,  3.22it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:01<00:03,  1.66it/s]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:02<00:03,  1.33it/s]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:02<00:02,  1.25it/s]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:03<00:01,  1.18it/s]\n",
      "Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:04<00:00,  1.17it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:05<00:00,  1.15it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:05<00:00,  1.25it/s]\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:52:18 [default_loader.py:262] Loading weights took 5.78 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:52:18 [gpu_model_runner.py:2007] Model loading took 14.9596 GiB and 29.576688 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:52:24 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/4fe8a52f1b/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:52:24 [backends.py:559] Dynamo bytecode transform time: 5.57 s\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:52:26 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:52:47 [backends.py:215] Compiling a graph for dynamic shape takes 22.85 s\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:52:53 [monitor.py:34] torch.compile takes 28.43 s in total\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:52:54 [gpu_worker.py:276] Available KV cache memory: 23.41 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:52:54 [kv_cache_utils.py:849] GPU KV cache size: 191,776 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:52:54 [kv_cache_utils.py:853] Maximum concurrency for 8,192 tokens per request: 23.41x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 27.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:52:57 [gpu_model_runner.py:2708] Graph capturing finished in 3 secs, took 0.53 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=1361)\u001b[0;0m INFO 09-02 01:52:57 [core.py:214] init engine (profile, create kv cache, warmup model) took 38.61 seconds\n",
      "INFO 09-02 01:52:58 [llm.py:298] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "# VLLM 모델 설정\n",
    "# - 방법1: LoRA 어댑터와 병합된 model_id 제공 (저장소에 config.json이 포함되어야 한다.)\n",
    "# - 방법2: LoRA를 동적 로딩으로 사용. LLM(model=BASE_MODEL_ID, enable_lora=True), vllm_model.generate(lora_request=...)\n",
    "\n",
    "# 모델 로드\n",
    "merged_model_id = 'shqkel/llama3-8b-rag-ko-merged'\n",
    "\n",
    "vllm_model = LLM(\n",
    "    model=merged_model_id,\n",
    "    dtype=\"bfloat16\",\n",
    "    gpu_memory_utilization=0.5   # 0.5(50%), 0.4(40%) 등 환경에 맞게 조정\n",
    ")\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba081bf8-49d5-4e57-8fc6-250eff9fa980",
   "metadata": {},
   "source": [
    "### vLLM 실행 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6211a4ee-0aa8-483e-9654-f00d946b7d6d",
   "metadata": {
    "id": "6211a4ee-0aa8-483e-9654-f00d946b7d6d",
    "outputId": "cf23e301-0d4a-449c-98ae-0e550b3d8dc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "apply_chat_template 예시:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 도움이 되는 AI 어시스턴트입니다.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "안녕하세요, 도와주세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply_chat_template 예시 출력\n",
    "example_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"당신은 도움이 되는 AI 어시스턴트입니다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"안녕하세요, 도와주세요.\"}\n",
    "]\n",
    "formatted_example = tokenizer.apply_chat_template(\n",
    "    example_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(\"\\napply_chat_template 예시:\")\n",
    "print(formatted_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8a121a-9fb8-4a1c-9d4f-c4d13d1e1c09",
   "metadata": {},
   "source": [
    "#### 01.검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d417280b-52bc-443f-9aa1-c6f6dcd6a1e1",
   "metadata": {
    "id": "d417280b-52bc-443f-9aa1-c6f6dcd6a1e1"
   },
   "outputs": [],
   "source": [
    "# RAG 프롬프트 템플릿 정의\n",
    "system_prompt = \"\"\"당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, 그리고 문서 내에서만 정보를 근거로 하여 답변하는 AI 비서입니다.\n",
    "아래 지침을 반드시 지켜주세요:\n",
    "\n",
    "- 답변은 반드시 docs에서 찾은 내용에 한해서만 작성해주세요. docs에 없는 내용은 추론하거나 지어내지 마세요.\n",
    "- 답변에서 인용하는 부분이 있다면, 반드시 해당 문서의 번호(예: [[doc1]], [[doc2]])로 근거를 표시해 주세요.\n",
    "- docs의 순서와 번호는 중요합니다. docs에서 인용하지 않은 정보는 답변에 포함하지 마세요.\n",
    "- 답변의 근거가 되는 문서 번호를 생략하지 말고, 항상 인용 태그([[doc1]], [[doc2]], ...)를 포함해 주세요.\n",
    "- 모든 답변은 존댓말을 사용하세요.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2b8b53b-5e41-4e21-a0c8-ffe80c190c35",
   "metadata": {
    "id": "d2b8b53b-5e41-4e21-a0c8-ffe80c190c35"
   },
   "outputs": [],
   "source": [
    "# 검색 결과 포맷팅 함수\n",
    "def format_docs(docs):\n",
    "    doc_items = [f\"doc{i+1}: {doc}\" for i, doc in enumerate(docs)]\n",
    "    docs_str = '\\n'.join(doc_items)\n",
    "    return docs_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6b596a9-c40d-4eb0-8c69-fcb0cf1ff0de",
   "metadata": {
    "id": "c6b596a9-c40d-4eb0-8c69-fcb0cf1ff0de",
    "outputId": "b5c76a73-77e9-4d7d-e71c-29b5090da9b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 미국의 ICT 정책 중 AI 관련 주요 내용은 무엇인가?\n"
     ]
    }
   ],
   "source": [
    "# 원본 VLLM을 사용한 RAG 실행 예시\n",
    "question = \"미국의 ICT 정책 중 AI 관련 주요 내용은 무엇인가?\"\n",
    "print(\"질문:\", question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bd506b3-f73d-4719-a041-9aa22bda207b",
   "metadata": {
    "id": "4bd506b3-f73d-4719-a041-9aa22bda207b"
   },
   "outputs": [],
   "source": [
    "# 검색기 설정\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83bbbfe1-f038-4ac9-b72c-4813b402d297",
   "metadata": {
    "id": "83bbbfe1-f038-4ac9-b72c-4813b402d297",
    "outputId": "e1c8f33b-8b78-4f80-93ec-20b8d8096302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 문서 수: 3\n"
     ]
    }
   ],
   "source": [
    "# 1. 검색 수행\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "print(f\"검색된 문서 수: {len(retrieved_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f72396f4-555c-44cc-938e-f9866694d51b",
   "metadata": {
    "id": "f72396f4-555c-44cc-938e-f9866694d51b",
    "outputId": "0e04aa2f-1e4d-4b36-9729-31497f80cb83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "검색 결과 (포맷팅):\n",
      "doc1: page_content='국가별 ICT 시장동향 93. ICT 주요 정책① 미국 인공지능 리더십 장벽 제거* Federal Register, Removing Barriers to American Leadership in Artificial Intelligence, 2025.01.23.\n",
      "n(주요  내용) 미국  AI 발전  저해 정 책 폐지  및 글로 벌 리더 십 강화  행동 계 획 추진-2025년 1월 23일 백악관에서 발표한 행정명령*으로, 이전 행정부의 행정명령 14110(안전하고 신뢰할 수 있는 AI 개발 및 사용)을 철회하고 해당 명령에 따른 정책과 지침 재검토 지시-AI 시스템에서 이념적 편향과 사회적 의제 배제를 강조하고, 경제 경쟁력과 국가 안보 증진에 초점을 맞춘 정책 방향 설정n(배경) 이전 행정부의 AI 정책 보다 유연한 혁신 환경 조성 필요성 대두-기존 AI 정책이 과도한 규제와 특정 가치 지향적 접근으로 기술 혁신과 경쟁력을 저해한다는 산업계 및 정치적 비판 증가-자유 시장, 세계적 수준의 연구 기관, 기업가 정신을 기반으로 한 미국 AI 혁신의 강점 회복 필요성 인식-중국, 유럽 등 글로벌 경쟁자들의 AI 기술 발전 가속화에 대응하여 미국의 리더십 위치 재확립 요구 증대n(기대 효과) AI 분야 규제 완화를 통한 글로벌 시장 주도권 확보-규제 부담 감소로 AI 분야 민간 기업의 연구개발 및 혁신 활동 가속화 전망-기업들의 AI 기술 투자 및 상용화 확대로 미국 내 AI 산업 성장과 경제적 이익 창출 예상-AI 기술 개발에서 국가 안보와 연계된 전략적 우위 확보를 통해 기술 주권 강화 및 글로벌 표준 설정 영향력 증대표 | 정책 일정 요구사항출처 : Federal Register, Removing Barriers to American Leadership in Artificial Intelligence, 2025.01.23.' metadata={'total_pages': 31, 'page': 8, 'source': '[GIP] 2025 국가별 ICT 시장동향_보고서_미국.pdf', 'pdfversion': '1.4', 'moddate': '2025-04-04T14:14:13+09:00', 'creator': 'Hwp 2020 11.0.0.8362', 'page_label': '9', 'creationdate': '2025-04-04T14:14:13+09:00', 'author': 'rnc', 'producer': 'Hancom PDF 1.3.0.546'}\n",
      "doc2: page_content='국가별 ICT 시장동향 113. ICT 주요 정책③ CHIPS  프로그램 개편* The White House, ESTABLISHING THE UNITED STATES INVESTMENT ACCELERATOR, 2025.03.31.  \n",
      "n(주요 내용) '미국 투자 가속기' 설립을 통한 반도체 제조 투자 가속화 -2025년 3월 31일 백악관에서 발표한 행정명령*으로, 미 상무부 내 '미국 투자 가속기(United States Investment Accelerator)' 신설 지시-10억 달러 이상 투자 촉진을 위한 규제 절차 간소화, 자원 접근성 향상, 국립 연구소와의 협력 증진 등 포괄적 지원 체계 구축-CHIPS 프로그램 사무소를 투자 가속기 산하로 이관하고, 이전 행정부보다 납세자에게 유리한 조건으로 협상 추진 강조n(배경) 글로벌 경쟁 심화와 공급망 문제로 자국 생산 역량 강화 필요성 제기-복잡하고 부담스러운 미국 규제 프로세스가 국내외 기업의 대규모 투자와 반도체 생산시설 구축을 어렵게 한다는 문제 인식-여러 연방, 주, 지방 법규와 중복되는 요구사항으로 인한 건설 지연 및 투자 효율성 저하 문제 해결 필요성 대두-글로벌 반도체 공급망 취약성과 경쟁국의 반도체 산업 지원 정책 확대에 대응하여 국내 생산 기반 확충 시급성 증가n(기대 효과) 반도체 생산 확대를 통한 ICT 산업 경쟁력 강화 -규제 장벽 완화와 정부 지원 체계 강화로 반도체 제조 관련 국내외 기업의 대규모 투자 유치 및 건설 기간 단축 예상-미국 내 반도체 생산 능력 확대를 통한 공급망 회복력 강화 및 첨단 기술 분야 국가 안보 이익 증진-반도체 제조 생태계 발전으로 관련 산업 일자리 창출과 지역 경제 활성화 및 글로벌 기술 경쟁에서 미국의 주도적 위치 강화표 | 미국 투자 가속기 주요 기능출처 : The White House, ESTABLISHING THE UNITED STATES INVESTMENT ACCELERATOR, 2025.03.31.' metadata={'moddate': '2025-04-04T14:14:13+09:00', 'pdfversion': '1.4', 'creator': 'Hwp 2020 11.0.0.8362', 'page_label': '11', 'author': 'rnc', 'source': '[GIP] 2025 국가별 ICT 시장동향_보고서_미국.pdf', 'producer': 'Hancom PDF 1.3.0.546', 'page': 10, 'creationdate': '2025-04-04T14:14:13+09:00', 'total_pages': 31}\n",
      "doc3: page_content='Ⅰ ICT 국가 산업 현황 103. ICT 주요 정책② 디지털 금융 기술에서의 미국 리더십 강화* The White House, STRENGTHENING AMERICAN LEADERSHIPIN DIGITAL FINANCIAL TECHNOLOGY, 2025.01.23.' metadata={'source': '[GIP] 2025 국가별 ICT 시장동향_보고서_미국.pdf', 'author': 'rnc', 'producer': 'Hancom PDF 1.3.0.546', 'total_pages': 31, 'page_label': '10', 'page': 9, 'creator': 'Hwp 2020 11.0.0.8362', 'creationdate': '2025-04-04T14:14:13+09:00', 'pdfversion': '1.4', 'moddate': '2025-04-04T14:14:13+09:00'}\n"
     ]
    }
   ],
   "source": [
    "# 2. 검색 결과 포맷팅\n",
    "docs_str = format_docs(retrieved_docs)\n",
    "print(\"\\n검색 결과 (포맷팅):\")\n",
    "print(docs_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902a95c-a724-4266-a142-c0ccf32af8a3",
   "metadata": {},
   "source": [
    "#### 02.증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc4d3104-a66b-4dbb-8795-6196c8b821c9",
   "metadata": {
    "id": "dc4d3104-a66b-4dbb-8795-6196c8b821c9",
    "outputId": "cc017f9d-d382-4369-8bd6-8f75f87e7922"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "메시지 형식:\n",
      "System: 당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, 그리고 문서 내에서만 정보를 근거로 하여 답변하는 AI 비서입니다.\n",
      "아래 지침을 반드시 지켜주세요:\n",
      "\n",
      "- 답변은 반드시 docs에서 찾은 내용에 한해서만 작성해주세요. docs에 없는 내용은 추론하거나 지어내지 마세요.\n",
      "- 답변에서 인용하는 부분이 있다면, 반드시 해당 문서의 번호(예: [[doc1]], [[doc2]])로 근거를 표시해 주세요.\n",
      "- docs의 순서와 번호는 중요합니다. docs에서 인용하지 않은 정보는 답변에 포함하지 마세요.\n",
      "- 답변의 근거가 되는 문서 번호를 생략하지 말고, 항상 인용 태그([[doc1]], [[doc2]], ...)를 포함해 주세요.\n",
      "- 모든 답변은 존댓말을 사용하세요.\n",
      "User: 질문: 미국의 ICT 정책 중 AI 관련 주요 내용은 무엇인가?\n",
      "\n",
      "docs:\n",
      "doc1: page_content='국가별 ICT 시장동향 93. ICT 주요 정책① 미국 인공지능 리더십 장벽 제거* Federal Register, Removing Barriers to American Leadership in Artificial Intelligence, 2025.01.23.\n",
      "n(주요  내용) 미국  AI 발전  저해 정 책 폐지  및 글로 벌 리더 십 강화  행동 계 획 추진-2025년 1월 23일 백악관에서 발표한 행정명령*으로, 이전 행정부의 행정명령 14110(안전하고 신뢰할 수 있는 AI 개발 및 사용)을 철회하고 해당 명령에 따른 정책과 지침 재검토 지시-AI 시스템에서 이념적 편향과 사회적 의제 배제를 강조하고, 경제 경쟁력과 국가 안보 증진에 초점을 맞춘 정책 방향 설정n(배경) 이전 행정부의 AI 정책 보다 유연한 혁신 환경 조성 필요성 대두-기존 AI 정책이 과도한 규제와 특정 가치 지향적 접근으로 기술 혁신과 경쟁력을 저해한다는 산업계 및 정치적 비판 증가-자유 시장, 세계적 수준의 연구 기관, 기업가 정신을 기반으로 한 미국 AI 혁신의 강점 회복 필요성 인식-중국, 유럽 등 글로벌 경쟁자들의 AI 기술 발전 가속화에 대응하여 미국의 리더십 위치 재확립 요구 증대n(기대 효과) AI 분야 규제 완화를 통한 글로벌 시장 주도권 확보-규제 부담 감소로 AI 분야 민간 기업의 연구개발 및 혁신 활동 가속화 전망-기업들의 AI 기술 투자 및 상용화 확대로 미국 내 AI 산업 성장과 경제적 이익 창출 예상-AI 기술 개발에서 국가 안보와 연계된 전략적 우위 확보를 통해 기술 주권 강화 및 글로벌 표준 설정 영향력 증대표 | 정책 일정 요구사항출처 : Federal Register, Removing Barriers to American Leadership in Artificial Intelligence, 2025.01.23.' metadata={'total_pages': 31, 'page': 8, 'source': '[GIP] 2025 국가별 ICT 시장동향_보고서_미국.pdf', 'pdfversion': '1.4', 'moddate': '2025-04-04T14:14:13+09:00', 'creator': 'Hwp 2020 11.0.0.8362', 'page_label': '9', 'creationdate': '2025-04-04T14:14:13+09:00', 'author': 'rnc', 'producer': 'Hancom PDF 1.3.0.546'}\n",
      "doc2: page_content='국가별 ICT 시장동향 113. ICT 주요 정책③ CHIPS  프로그램 개편* The White House, ESTABLISHING THE UNITED STATES INVESTMENT ACCELERATOR, 2025.03.31.  \n",
      "n(주요 내용) '미국 투자 가속기' 설립을 통한 반도체 제조 투자 가속화 -2025년 3월 31일 백악관에서 발표한 행정명령*으로, 미 상무부 내 '미국 투자 가속기(United States Investment Accelerator)' 신설 지시-10억 달러 이상 투자 촉진을 위한 규제 절차 간소화, 자원 접근성 향상, 국립 연구소와의 협력 증진 등 포괄적 지원 체계 구축-CHIPS 프로그램 사무소를 투자 가속기 산하로 이관하고, 이전 행정부보다 납세자에게 유리한 조건으로 협상 추진 강조n(배경) 글로벌 경쟁 심화와 공급망 문제로 자국 생산 역량 강화 필요성 제기-복잡하고 부담스러운 미국 규제 프로세스가 국내외 기업의 대규모 투자와 반도체 생산시설 구축을 어렵게 한다는 문제 인식-여러 연방, 주, 지방 법규와 중복되는 요구사항으로 인한 건설 지연 및 투자 효율성 저하 문제 해결 필요성 대두-글로벌 반도체 공급망 취약성과 경쟁국의 반도체 산업 지원 정책 확대에 대응하여 국내 생산 기반 확충 시급성 증가n(기대 효과) 반도체 생산 확대를 통한 ICT 산업 경쟁력 강화 -규제 장벽 완화와 정부 지원 체계 강화로 반도체 제조 관련 국내외 기업의 대규모 투자 유치 및 건설 기간 단축 예상-미국 내 반도체 생산 능력 확대를 통한 공급망 회복력 강화 및 첨단 기술 분야 국가 안보 이익 증진-반도체 제조 생태계 발전으로 관련 산업 일자리 창출과 지역 경제 활성화 및 글로벌 기술 경쟁에서 미국의 주도적 위치 강화표 | 미국 투자 가속기 주요 기능출처 : The White House, ESTABLISHING THE UNITED STATES INVESTMENT ACCELERATOR, 2025.03.31.' metadata={'moddate': '2025-04-04T14:14:13+09:00', 'pdfversion': '1.4', 'creator': 'Hwp 2020 11.0.0.8362', 'page_label': '11', 'author': 'rnc', 'source': '[GIP] 2025 국가별 ICT 시장동향_보고서_미국.pdf', 'producer': 'Hancom PDF 1.3.0.546', 'page': 10, 'creationdate': '2025-04-04T14:14:13+09:00', 'total_pages': 31}\n",
      "doc3: page_content='Ⅰ ICT 국가 산업 현황 103. ICT 주요 정책② 디지털 금융 기술에서의 미국 리더십 강화* The White House, STRENGTHENING AMERICAN LEADERSHIPIN DIGITAL FINANCIAL TECHNOLOGY, 2025.01.23.' metadata={'source': '[GIP] 2025 국가별 ICT 시장동향_보고서_미국.pdf', 'author': 'rnc', 'producer': 'Hancom PDF 1.3.0.546', 'total_pages': 31, 'page_label': '10', 'page': 9, 'creator': 'Hwp 2020 11.0.0.8362', 'creationdate': '2025-04-04T14:14:13+09:00', 'pdfversion': '1.4', 'moddate': '2025-04-04T14:14:13+09:00'}\n",
      "\n",
      "위의 docs 중에서만 정보를 근거로 하여, 질문에 답변해 주세요.\n",
      "답변에서 인용한 문서의 내용에는 반드시 [[doc1]], [[doc2]], ... 형식으로 인용 표시를 해주세요.\n",
      "추론이나 지어내는 답변은 삼가주시고, docs에 명시적으로 나타난 내용만 인용해 주세요.\n"
     ]
    }
   ],
   "source": [
    "# 3. 메시지 준비\n",
    "user_prompt = f\"\"\"질문: {question}\n",
    "\n",
    "docs:\n",
    "{docs_str}\n",
    "\n",
    "위의 docs 중에서만 정보를 근거로 하여, 질문에 답변해 주세요.\n",
    "답변에서 인용한 문서의 내용에는 반드시 [[doc1]], [[doc2]], ... 형식으로 인용 표시를 해주세요.\n",
    "추론이나 지어내는 답변은 삼가주시고, docs에 명시적으로 나타난 내용만 인용해 주세요.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "print(\"\\n메시지 형식:\")\n",
    "print(f\"System: {messages[0]['content']}\")\n",
    "print(f\"User: {messages[1]['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52c9e8cf-5d57-4c1f-929a-82d056685047",
   "metadata": {
    "id": "52c9e8cf-5d57-4c1f-929a-82d056685047",
    "outputId": "7bd84dcc-9af4-49ec-f057-119bf0ec7fa3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "최종 프롬프트:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, 그리고 문서 내에서만 정보를 근거로 하여 답변하는 AI 비서입니다.\n",
      "아래 지침을 반드시 지켜주세요:\n",
      "\n",
      "- 답변은 반드시 docs에서 찾은 내용에 한해서만 작성해주세요. docs에 없는 내용은 추론하거나 지어내지 마세요.\n",
      "- 답변에서 인용하는 부분이 있다면, 반드시 해당 문서의 번호(예: [[doc1]], [[doc2]])로 근거를 표시해 주세요.\n",
      "- docs의 순서와 번호는 중요합니다. docs에서 인용하지 않은 정보는 답변에 포함하지 마세요.\n",
      "- 답변의 근거가 되는 문서 번호를 생략하지 말고, 항상 인용 태그([[doc1]], [[doc2]], ...)를 포함해 주세요.\n",
      "- 모든 답변은 존댓말을 사용하세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "질문: 미국의 ICT 정책 중 AI 관련 주요 내용은 무엇인가?\n",
      "\n",
      "docs:\n",
      "doc1: page_content='국가별 ICT 시장동향 93. ICT 주요 정책① 미국 인공지능 리더십 장벽 제거* Federal Register, Removing Barriers to American Leadership in Artificial Intelligence, 2025.01.23.\n",
      "n(주요  내용) 미국  AI 발전  저해 정 책 폐지  및 글로 벌 리더 십 강화  행동 계 획 추진-2025년 1월 23일 백악관에서 발표한 행정명령*으로, 이전 행정부의 행정명령 14110(안전하고 신뢰할 수 있는 AI 개발 및 사용)을 철회하고 해당 명령에 따른 정책과 지침 재검토 지시-AI 시스템에서 이념적 편향과 사회적 의제 배제를 강조하고, 경제 경쟁력과 국가 안보 증진에 초점을 맞춘 정책 방향 설정n(배경) 이전 행정부의 AI 정책 보다 유연한 혁신 환경 조성 필요성 대두-기존 AI 정책이 과도한 규제와 특정 가치 지향적 접근으로 기술 혁신과 경쟁력을 저해한다는 산업계 및 정치적 비판 증가-자유 시장, 세계적 수준의 연구 기관, 기업가 정신을 기반으로 한 미국 AI 혁신의 강점 회복 필요성 인식-중국, 유럽 등 글로벌 경쟁자들의 AI 기술 발전 가속화에 대응하여 미국의 리더십 위치 재확립 요구 증대n(기대 효과) AI 분야 규제 완화를 통한 글로벌 시장 주도권 확보-규제 부담 감소로 AI 분야 민간 기업의 연구개발 및 혁신 활동 가속화 전망-기업들의 AI 기술 투자 및 상용화 확대로 미국 내 AI 산업 성장과 경제적 이익 창출 예상-AI 기술 개발에서 국가 안보와 연계된 전략적 우위 확보를 통해 기술 주권 강화 및 글로벌 표준 설정 영향력 증대표 | 정책 일정 요구사항출처 : Federal Register, Removing Barriers to American Leadership in Artificial Intelligence, 2025.01.23.' metadata={'total_pages': 31, 'page': 8, 'source': '[GIP] 2025 국가별 ICT 시장동향_보고서_미국.pdf', 'pdfversion': '1.4', 'moddate': '2025-04-04T14:14:13+09:00', 'creator': 'Hwp 2020 11.0.0.8362', 'page_label': '9', 'creationdate': '2025-04-04T14:14:13+09:00', 'author': 'rnc', 'producer': 'Hancom PDF 1.3.0.546'}\n",
      "doc2: page_content='국가별 ICT 시장동향 113. ICT 주요 정책③ CHIPS  프로그램 개편* The White House, ESTABLISHING THE UNITED STATES INVESTMENT ACCELERATOR, 2025.03.31.  \n",
      "n(주요 내용) '미국 투자 가속기' 설립을 통한 반도체 제조 투자 가속화 -2025년 3월 31일 백악관에서 발표한 행정명령*으로, 미 상무부 내 '미국 투자 가속기(United States Investment Accelerator)' 신설 지시-10억 달러 이상 투자 촉진을 위한 규제 절차 간소화, 자원 접근성 향상, 국립 연구소와의 협력 증진 등 포괄적 지원 체계 구축-CHIPS 프로그램 사무소를 투자 가속기 산하로 이관하고, 이전 행정부보다 납세자에게 유리한 조건으로 협상 추진 강조n(배경) 글로벌 경쟁 심화와 공급망 문제로 자국 생산 역량 강화 필요성 제기-복잡하고 부담스러운 미국 규제 프로세스가 국내외 기업의 대규모 투자와 반도체 생산시설 구축을 어렵게 한다는 문제 인식-여러 연방, 주, 지방 법규와 중복되는 요구사항으로 인한 건설 지연 및 투자 효율성 저하 문제 해결 필요성 대두-글로벌 반도체 공급망 취약성과 경쟁국의 반도체 산업 지원 정책 확대에 대응하여 국내 생산 기반 확충 시급성 증가n(기대 효과) 반도체 생산 확대를 통한 ICT 산업 경쟁력 강화 -규제 장벽 완화와 정부 지원 체계 강화로 반도체 제조 관련 국내외 기업의 대규모 투자 유치 및 건설 기간 단축 예상-미국 내 반도체 생산 능력 확대를 통한 공급망 회복력 강화 및 첨단 기술 분야 국가 안보 이익 증진-반도체 제조 생태계 발전으로 관련 산업 일자리 창출과 지역 경제 활성화 및 글로벌 기술 경쟁에서 미국의 주도적 위치 강화표 | 미국 투자 가속기 주요 기능출처 : The White House, ESTABLISHING THE UNITED STATES INVESTMENT ACCELERATOR, 2025.03.31.' metadata={'moddate': '2025-04-04T14:14:13+09:00', 'pdfversion': '1.4', 'creator': 'Hwp 2020 11.0.0.8362', 'page_label': '11', 'author': 'rnc', 'source': '[GIP] 2025 국가별 ICT 시장동향_보고서_미국.pdf', 'producer': 'Hancom PDF 1.3.0.546', 'page': 10, 'creationdate': '2025-04-04T14:14:13+09:00', 'total_pages': 31}\n",
      "doc3: page_content='Ⅰ ICT 국가 산업 현황 103. ICT 주요 정책② 디지털 금융 기술에서의 미국 리더십 강화* The White House, STRENGTHENING AMERICAN LEADERSHIPIN DIGITAL FINANCIAL TECHNOLOGY, 2025.01.23.' metadata={'source': '[GIP] 2025 국가별 ICT 시장동향_보고서_미국.pdf', 'author': 'rnc', 'producer': 'Hancom PDF 1.3.0.546', 'total_pages': 31, 'page_label': '10', 'page': 9, 'creator': 'Hwp 2020 11.0.0.8362', 'creationdate': '2025-04-04T14:14:13+09:00', 'pdfversion': '1.4', 'moddate': '2025-04-04T14:14:13+09:00'}\n",
      "\n",
      "위의 docs 중에서만 정보를 근거로 하여, 질문에 답변해 주세요.\n",
      "답변에서 인용한 문서의 내용에는 반드시 [[doc1]], [[doc2]], ... 형식으로 인용 표시를 해주세요.\n",
      "추론이나 지어내는 답변은 삼가주시고, docs에 명시적으로 나타난 내용만 인용해 주세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. 토크나이저로 프롬프트 포맷팅\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(\"\\n최종 프롬프트:\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d572219-ea3b-4724-85ac-aa350cc35985",
   "metadata": {},
   "source": [
    "#### 03.vLLM기반 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9575325-284e-4274-87ad-4035eb2b1775",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "63876394459946aabc83d2f27064ea88",
      "9d762c95d42741cba3f3bf4ad24b1857"
     ]
    },
    "id": "d9575325-284e-4274-87ad-4035eb2b1775",
    "outputId": "1dee3296-d0f3-401b-9760-0cf204a41df7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349206a50364442fa081562fbbbf3215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05a811f448b4f9ba45f872c923e057c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "최종 답변:\n",
      "미국의 ICT 정책 중 AI 관련 주요 내용은 다음과 같습니다. \n",
      "\n",
      "2025년 1월 23일, 백악관에서 발표한 행정명령 \"Removing Barriers to American Leadership in Artificial Intelligence\"는 AI 리더십 장벽을 제거하고 미국의 AI 발전을 촉진하기 위한 정책을 포함하고 있습니다. 이 행정명령은 이전 행정부의 AI 정책인 \"Safe and Secure AI Development and Use\"를 철회하고, AI 시스템에서 이념적 편향과 사회적 의제를 배제하는 것을 강조합니다. 또한, 경제 경쟁력과 국가 안보를 중점으로 한 정책 방향을 설정하고 있습니다[[doc1]].\n",
      "\n",
      "이 정책은 기존의 AI 정책이 과도한 규제와 특정 가치 지향적 접근으로 인해 기술 혁신과 경쟁력을 저해했다는 비판에 부응하여, 자유 시장과 기업가 정신을 기반으로 한 미국의 AI 혁신을 회복하려는 목적을 가지고 있습니다. 결과적으로, 이 정책은 AI 분야의 규제 완화를 통해 글로벌 시장에서의 주도권을 확보하고, AI 기술 개발에서 국가 안보와 연계된 전략적 우위를 확보하는 데 기여할 것으로 예상됩니다[[doc1]].\n"
     ]
    }
   ],
   "source": [
    "# 5. VLLM으로 생성\n",
    "\n",
    "# 생성 파라미터 설정\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "\n",
    "outputs = vllm_model.generate([prompt], sampling_params)\n",
    "response = outputs[0].outputs[0].text\n",
    "\n",
    "# 6. 최종 답변 출력\n",
    "print(\"\\n최종 답변:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b1691-3ce2-4a16-93e1-0a04b5bb3e56",
   "metadata": {},
   "source": [
    "### RAG 함수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "UyGNCK0f7MUE",
   "metadata": {
    "id": "UyGNCK0f7MUE"
   },
   "outputs": [],
   "source": [
    "# 원본 VLLM을 사용한 RAG 함수 구현\n",
    "def generate_rag_response(question, retriever, tokenizer, vllm_model, sampling_params):\n",
    "    \"\"\"\n",
    "    검색 결과와 질문을 기반으로 RAG 응답을 생성하는 함수\n",
    "    \"\"\"\n",
    "    # 검색 수행\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "    # 검색 결과 포맷팅\n",
    "    docs_str = format_docs(retrieved_docs)\n",
    "\n",
    "    # 메시지 준비\n",
    "    user_prompt = f\"\"\"질문: {question}\n",
    "\n",
    "docs:\n",
    "{docs_str}\n",
    "\n",
    "위의 docs 중에서만 정보를 근거로 하여, 질문에 답변해 주세요.\n",
    "답변에서 인용한 문서의 내용에는 반드시 [[doc1]], [[doc2]], ... 형식으로 인용 표시를 해주세요.\n",
    "추론이나 지어내는 답변은 삼가주시고, docs에 명시적으로 나타난 내용만 인용해 주세요.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    # 토크나이저로 프롬프트 포맷팅\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # VLLM으로 생성\n",
    "    outputs = vllm_model.generate([prompt], sampling_params)\n",
    "\n",
    "    # 응답 추출 및 반환\n",
    "    response = outputs[0].outputs[0].text\n",
    "    return response, docs_str, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66793ef7-a1e3-48c7-ba50-517f57f927c0",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "8b81a7a49bcc4c7bb1198e9747e45848",
      "4c8699253c4f4a97a0e25713cc43ce0a"
     ]
    },
    "id": "66793ef7-a1e3-48c7-ba50-517f57f927c0",
    "outputId": "74f8f332-3a54-4ab7-d997-57666ab20427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "질문: 미국의 디지털 인프라 투자에 대해 설명해주세요.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6019def90e75436ebe3dcf4dce684885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24a74dd8d104a2ca896da079be9bf51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 문서 수: 3\n",
      "\n",
      "검색 결과:\n",
      "doc1: page_content='국가별 ICT 시장동향 113. ICT 주요 정책③ CHIPS  프로그램 개편* The White House, ESTABLISHING THE UNITED STATES INVESTMENT ACCELERATOR, 2025.03.31.  \n",
      "n(주요 내용) '미국 투자 가속기' 설립을 통한 반도체 제조 투자 가속화 -2025년 3월 31일 백악관에서 발표한 행정명령*으로, 미 상무부 내 '미국 투자 가속기(United States Investment Accelerator)' 신설 지시-10억 달러 이상 투자 촉진을 위한 규제 절차 간소화, 자원 접근성 향상, 국립 연구소와의 협력 증진 등 포괄적 지원 체계 구축-CHIPS 프로그램 사무소를 투자 가속기 산하로 이관하고, 이전 행정부보다 납세자에게 유리한 조건으로 협상 추진 강조n(배경) 글로벌 경쟁 심화와 공급망 문제로 자국 생산 역량 강화 필요성 제기-복잡하고 부담스러운 미국 규제 프로세스가 국내외 기업의 대규모 투자와 반도체 생산시설 구축을 어렵게 한다는 문제 인식-여러 연방, 주, 지방 법규와 중복되는 요구사항으로 인한 건설 지연 및 투자 효율성 저하 문제 해결 필요성 대두-글로벌 반도체 공급망 취약성과 경쟁국의 반도체 산업 지원 정책 확대에 대응하여 국내 생산 기반 확충 시급성 증가n(기대 효과) 반도체 생산 확대를 통한 ICT 산업 경쟁력 강화 -규제 장벽 완화와 정부 지원 체계 강화로 반도체 제조 관련 국내외 기업의 대규모 투자 유치 및 건설 기간 단축 예상-미국 내 반도체 생산 능력 확대를 통한 공급망 회복력 강화 및 첨단 기술 분야 국가 안보 이익 증진-반도체 제조 생태계 발전으로 관련 산업 일자리 창출과 지역 경제 활성화 및 글로벌 기술 경쟁에서 미국의 주도적 위치 강화표 | 미국 투자 가속기 주요 기능출처 : The White House, ESTABLISHING THE UNITED STATES INVESTMENT ACCELERATOR, 2025.03.31.' metadata={'moddate': '2025-04-04T14:14:13+09:00', 'page_label': '11', 'page': 10, 'author': 'rnc', 'pdfversion': '1.4', 'source': '[GIP] 2025 국가별 ICT 시장동향_보고서_미국.pdf', 'creator': 'Hwp 2020 11.0.0.8362', 'producer': 'Hancom PDF 1.3.0.546', 'total_pages': 31, 'creationdate': '2025-04-04T14:14:13+09:00'}\n",
      "doc2: page_content='Ⅰ ICT 국가 산업 현황 103. ICT 주요 정책② 디지털 금융 기술에서의 미국 리더십 강화* The White House, STRENGTHENING AMERICAN LEADERSHIPIN DIGITAL FINANCIAL TECHNOLOGY, 2025.01.23.' metadata={'source': '[GIP] 2025 국가별 ICT 시장동향_보고서_미국.pdf', 'page': 9, 'page_label': '10', 'total_pages': 31, 'moddate': '2025-04-04T14:14:13+09:00', 'creationdate': '2025-04-04T14:14:13+09:00', 'author': 'rnc', 'creator': 'Hwp 2020 11.0.0.8362', 'producer': 'Hancom PDF 1.3.0.546', 'pdfversion': '1.4'}\n",
      "doc3: page_content='순위 이슈 건수1 미국 데이터 분석 플랫폼의 클라우드 및 SQL 엔진 혁신222미국 기업들의 데이터 엔지니어링 및 기술 채용 트렌드 부각163빅데이터와 AI 기술, 미국 내 다양한 분야에서 혁신적 접근 모색13n데이터 분석 플랫폼, 클라우드 및 SQL 엔진 중심 혁신 가속화-드레미오(Dremio)는 마이크로소프트 애저(Microsoft Azure) 지원을 통해 데이터 레이크하우스의 유연성을 활용한 데이터 접근성을 확대하여 클라우드 환경 기반 데이터 분석 역량 강화에 기여-원하우스(Onehouse)는 오픈 데이터 레이크하우스 기술에 3,500만 달러를 투자받고, 볼트론 데이터(Voltron Data)는 GPU 기반 SQL 엔진 '테세우스 (Theseus)'를 개발하며 저장-분석 통합 기술 혁신 추진-미국 질병통제예방센터(CDC)의 클라우드 기반 데이터 현대화와 오픈텍스트 (OpenText)의 고성능 분석 데이터베이스 출시로 다양한 산업 분야의 데이터 기반 의사결정 혁신이 가속화n글로벌 기업, 데이터·클라우드·AI 전문 인재 확보 경쟁 심화-아메리칸 익스프레스(American Express)는 소프트웨어 엔지니어링, 데이터 엔지니어링, 클라우드 엔지니어링 등 자바(Java), 파이썬(Python) 기반 전문 인재 채용을 통한 디지털 역량 강화에 주력-바클레이스(Barclays)와 캐피털 원(Capital One)은 금융 혁신을 위한 실시간 데이터 처리, 인공지능(AI), 클라우드 기술 활용 서비스 개발을 위한 데이터 전문가 확보 경쟁에 적극 참여-디지털 전환 가속화로 인한 데이터 중심 의사결정 추세 확산에 따라 기업 간 기술 인재 확보 경쟁이 심화되며, 이는 교육 프로그램 확대와 신규 일자리 창출 등 산업 변화 촉진n빅데이터·AI 기술, 의료·바이오헬스 등 산업 전반 혁신 주도-미국 의료 분야에서는 인공지능과 빅데이터 기술로 오피오이드 위기 대응 솔루션을 개발하고, 개인 유전 정보와 의료 기록 기반 맞춤형 의료 서비스 발전을 위한 혁신적 접근 시도-위스콘신' metadata={'page_label': '22', 'creationdate': '2025-04-04T14:14:13+09:00', 'total_pages': 31, 'creator': 'Hwp 2020 11.0.0.8362', 'page': 21, 'moddate': '2025-04-04T14:14:13+09:00', 'author': 'rnc', 'source': '[GIP] 2025 국가별 ICT 시장동향_보고서_미국.pdf', 'producer': 'Hancom PDF 1.3.0.546', 'pdfversion': '1.4'}\n",
      "===\n",
      "\n",
      "최종 답변:\n",
      "미국의 디지털 인프라 투자에 대한 정보는 다음과 같습니다.\n",
      "\n",
      "1. **미국 투자 가속기**: 2025년 3월 31일 백악관에서 발표된 행정명령에 따라, '미국 투자 가속기(United States Investment Accelerator)'가 설립되었습니다. 이 프로그램은 반도체 제조 투자 가속화를 목표로 하며, 10억 달러 이상의 투자를 촉진하기 위한 규제 절차를 간소화하고 자원 접근성을 향상시키는 등의 지원 체계를 구축하고 있습니다[[doc1]].\n",
      "\n",
      "2. **디지털 금융 기술에서의 미국 리더십 강화**: 2025년 1월 23일 발표된 정책에서는 디지털 금융 기술에서의 미국의 리더십을 강화하기 위한 노력이 포함되어 있습니다. 이는 금융 혁신을 위한 데이터 처리 및 인공지능 기술의 활용을 촉진하는 것을 목표로 하고 있습니다[[doc2]].\n",
      "\n",
      "이러한 투자와 정책들은 미국의 디지털 인프라를 강화하고, 글로벌 경쟁에서 미국의 주도적 위치를 강화하기 위한 노력의 일환으로 볼 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 다른 질문으로 테스트 - 함수 사용\n",
    "question = \"미국의 디지털 인프라 투자에 대해 설명해주세요.\"\n",
    "print(\"\\n질문:\", question)\n",
    "\n",
    "# 1. 함수 호출하여 RAG 실행\n",
    "response, docs_str, retrieved_docs = generate_rag_response(\n",
    "    question=question,\n",
    "    retriever=retriever,\n",
    "    tokenizer=tokenizer,\n",
    "    vllm_model=vllm_model,\n",
    "    sampling_params=sampling_params\n",
    ")\n",
    "\n",
    "# 2. 결과 출력\n",
    "print(f\"검색된 문서 수: {len(retrieved_docs)}\")\n",
    "print(\"\\n검색 결과:\")\n",
    "print(docs_str)\n",
    "print(\"===\")\n",
    "print(\"\\n최종 답변:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2840f07a-250a-4763-9a1f-f5780e8634db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
