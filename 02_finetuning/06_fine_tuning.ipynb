{"cells":[{"cell_type":"markdown","source":["# RAFT방식의 Finetuning"],"metadata":{"id":"g71lIjrlTXN4"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YkbRoZNeCsk-","outputId":"dad8f3cd-fc5f-4bd6-e66c-62c736d21b43"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch: 2.8.0.dev20250319+cu128\n","torchvision: 0.22.0.dev20250319+cu128\n","CUDA available?: True\n","GPU: NVIDIA A100-SXM4-80GB\n"]}],"source":["import torch\n","import torchvision\n","\n","print(\"torch:\", torch.__version__)\n","print(\"torchvision:\", torchvision.__version__)\n","print(\"CUDA available?:\", torch.cuda.is_available())\n","print(\"GPU:\", torch.cuda.get_device_name(0))"]},{"cell_type":"markdown","source":["## 환경설정"],"metadata":{"id":"gPSG-K7KTVOn"}},{"cell_type":"markdown","metadata":{"id":"XbTWzqNRV5ow"},"source":["transformers, datasets, accelerate, trl, peft 라이브러리를 설치합니다.\n","\n","* **transformers**: 대형 언어모델(LLM) 활용과 파인튜닝을 위한 핵심 라이브러리입니다.\n","* **datasets**: 다양한 데이터셋을 쉽게 불러오고 전처리하는 데 사용합니다.\n","* **accelerate**: 멀티GPU 및 분산 학습, 메모리 최적화 기능을 지원합니다.\n","* **trl**: SFT(지도학습), RLHF 등 LLM 미세조정용 Trainer와 도구를 제공합니다.\n","* **peft**: LoRA 등 저비용 파인튜닝(PEFT) 기법을 적용할 때 사용합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ryMEJe-2Ctrf","outputId":"5d6c8246-2974-4198-f006-b875b266a722","collapsed":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.0)\n","Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.18.2)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (6.31.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.7.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.1.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (20.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.3.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.8.0.dev20250319+cu128)\n","Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.61)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.57)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.57)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.8.0.87)\n","Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.3.14)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.41)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.55)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.2.55)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.5.7.53)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.3)\n","Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.25.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.55)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.61)\n","Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.0.11)\n","Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.3.0+git96316ce5)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=2.0.0->accelerate) (77.0.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install -U transformers datasets accelerate trl peft tiktoken protobuf sentencepiece scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cm_vgXevDjKw","outputId":"0be5d868-61cc-41f5-fdd0-4bdfb5f96074"},"outputs":[{"name":"stdout","output_type":"stream","text":["transformers: 4.52.4\n","datasets: 3.6.0\n","accelerate: 1.8.0\n","trl: 0.18.2\n","peft: 0.15.2\n"]}],"source":["import transformers, datasets, accelerate, trl, peft\n","\n","print(\"transformers:\", transformers.__version__)\n","print(\"datasets:\", datasets.__version__)\n","print(\"accelerate:\", accelerate.__version__)\n","print(\"trl:\", trl.__version__)\n","print(\"peft:\", peft.__version__)"]},{"cell_type":"markdown","source":["## 문서준비 및 병합\n","- klue_mrc_prompt_docs_5_answer_citations.csv\n","- klue_mrc_prompt_docs_1_4_answer_citations.csv\n","- klue_mrc_nominal_question_docs_1_5_answer_citations.csv\n","- klue_mrc_prompt_multidocs_answer_citations.csv"],"metadata":{"id":"EdeMwylYTdDN"}},{"cell_type":"code","source":["# klue_mrc_prompt_docs_5_answer_citations.csv\n","!gdown 1cybA7CWnNg9e_M73kRkVIi8nA9TG3co7\n","# klue_mrc_prompt_docs_1_4_answer_citations.csv\n","!gdown 1rikKQUmaN5yvFGcQi5xhfUFLR3slElmK\n","# klue_mrc_nominal_question_docs_1_5_answer_citations.csv\n","!gdown 1Ifvk2IFHB0x07G92P2HZHbpZhKIBAaML\n","# klue_mrc_prompt_multidocs_answer_citations.csv\n","!gdown 1TAQw4g4py9hrCAKNGG6DuTxDqgUiPCO_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bpsJS5SjhKal","executionInfo":{"status":"ok","timestamp":1751906397708,"user_tz":-540,"elapsed":11043,"user":{"displayName":"sh qkel","userId":"12967633263148061954"}},"outputId":"a8bb6445-7a09-45a8-c71e-cb98dd33b7a8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1cybA7CWnNg9e_M73kRkVIi8nA9TG3co7\n","To: /content/klue_mrc_prompt_docs_5_answer_citations.csv\n","100% 7.45M/7.45M [00:00<00:00, 56.0MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1rikKQUmaN5yvFGcQi5xhfUFLR3slElmK\n","To: /content/klue_mrc_prompt_docs_1_4_answer_citations.csv\n","100% 4.10M/4.10M [00:00<00:00, 174MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1Ifvk2IFHB0x07G92P2HZHbpZhKIBAaML\n","To: /content/klue_mrc_nominal_question_docs_1_5_answer_citations.csv\n","100% 3.37M/3.37M [00:00<00:00, 251MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1TAQw4g4py9hrCAKNGG6DuTxDqgUiPCO_\n","To: /content/klue_mrc_prompt_multidocs_answer_citations.csv\n","100% 5.33M/5.33M [00:00<00:00, 27.0MB/s]\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"id":"46BWDbgtDRYd","outputId":"f93c7da0-10fd-474e-a9ae-31563aad6550","executionInfo":{"status":"ok","timestamp":1751906404657,"user_tz":-540,"elapsed":1726,"user":{"displayName":"sh qkel","userId":"12967633263148061954"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(972, 7)\n","<class 'list'> ['1994년 11월, 배타적 경제수역(EEZ)을 정한 유엔 해양법 협약의 발효로 바다 관할권이 12해리에서 200해리까지 확대되었으나 한·일간의 거리가 400해리가 채 되지 않아 바다 경계선을 별도로 정해야 했다. 일본은 1994년 발효된 유엔해양법협약에 근거해 근해의 작은 섬들을 직선으로 연결, 영해기선을 새롭게 설정하고, 1997년 1월 1일을 기해 시행에 들어갔다. 1997년 5월 당시 유종하 외무장관은 미국방문 길에 수행기자단과의 간담회에서 “어업문제에 대한 일본의 요구가 더 이상은 버티기 어려운 상황에 이르러 있다”고 토로하기도 했다.\\n1996년 5월에 김영삼정부는 울릉도와 일본 오키 섬의 중간부분을 EEZ의 경계로 발표해 독도를 우리측 수역에 포함시킨다는 성명을 발표했었다. 1997년에 들어서 일본은 \"자신들의 직선기선을 넘어왔다\"며 한국의 어선들을 무차별적으로 나포하였고 일본의 하시모토 류타로 총리까지 나서서 \"한국어선 나포는 정당하다\"며 어선나포를 비호했고 일본은 한국정부에 \"직선기선을 인정하지 않으면 어업협정을 파기하겠다\"며 협박했다. 97년 7월 한국이 배타적 경제수역(EEZ)의 기점을 울릉도로 하는 발표를 한다. 7월 29일, 한국은 일본의 직선기선 인정 요구에 사실상 굴복하게 되었다. 그 이후에도 독도에 대한 수역을 제외하고 한국과 일본의 어업협상이 계속되었고 서로간의 마찰은 더더욱 심화되었다. 일각에서는 1997년 10월 당시 김영삼 정권은 잠정공동수역안(잠정조치수역은 독도 중간수역)을 공식적으로 받아들여 독도를 중간수역으로 하기로 일본과 합의하였다고 하지만, 당시 뉴스와 자료들 검색 결과 1997년 10월 10일, 도쿄에서 제6차 한일 실무회담을 진행하고 있었고 당시 한일양국은 당시 독도 주변수역을 제외하고 협상에 임하였고 당시 정부당국이 \"독도 영유권이 훼손되지 않는 조건하에 이 안을 철회한다\"며 독도에 대한 영유권주장은 변함이 없었다.(참조) \\n1997년 10월 22일, 일본측은 독도 주변수역을 제외하고 신어업협정을 타결하자는 의견을 한국측에 보냈다. 한국측은 별다른 응답이 없었고 1997년 11월 7일에는 한국정부가 독도에 접안시설을 건립하여 독도에 대한 입지를 확고히 하자 하시모토 류타로일본총리까지 나서서 이를 비난했고 일본정계가 들끓게 되었다. 이로 인하여 한국과 일본간에 독도에 대한 갈등은 심화되었고 한일간 어업협상은 97년 12월말까지 결론이 나오지 못했다. 일본은 12월 29일, 한국측이 어업협상에 대해 양보하지 않을경우 기존의 어업협정을 파기하여 재교섭하겠다고 선언한다. 당시 일본의 공영방송 NHK도 뚜렷한 진전이 없을 시, 일본 외상이 현행 어업협정을 파기한다는 보도를 냈고 그 시기 정권교체와 IMF사태라는 어수선한 분위기의 한국은 상황을 주도적으로 집중할 여력이 부족했다는 일부여론과 그러나 중요한 점은 국제정세는 한국의 자국내 불행한 상황과는 무관하게 인접국가들에게는 각국의 실리를 위해 그 상황을 이용할 여지가 될수 있다는 점이 언론에 보도된바 있었다.', '올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.', '궤도물리학은 계절의 지속 기간이 지구의 궤도가 지점과 분점 사이의 공간을 휩쓸고 지나가는 면적이 클수록 길어지며, 따라서 만약 이심률이 극단적으로 커진다면 원일점 쪽에서 나타나는 계절이 오래 지속될 것이다고 예측한다. 현재 지구에서는, 지구가 근일점에 접근할수록(태양에 가까워질수록) 북반구는 가을을 지나 겨울로 향하고, 한편 남반구에서는 반대되는 계절이 나타나고 있다. 결과적으로, 북반구에서는 가을과 겨울이 봄과 여름보다 살짝 짧다. 하지만, 전 지구적으로 보았을 때는 남반구는 오히려 봄과 여름이 살짝 짧음으로서 균형이 맞는다. 2006년에는 밀란코비치 주기에 따라 북반구의 여름이 겨울보다 4.66일 더 길었고, 봄은 가을보다 2.9일 더 길었다. \\n\\n장축단선의 세차운동 또한 지구의 지점과 분점의 위치를 느리게 바꾸고 있다. 참고로, 이 움직임은 지구의 \"궤도\"를 바꾸는 것이지 지구의 자전축을 바꾸는 것이 아니다(자전축 변화는 자전축의 세차운동 참조). 다음 1만 년 동안, 북반구의 겨울은 조금씩 길어질 것이고 여름은 조금씩 짧아질 것이다. 하지만, 한 쪽이 차가워짐에 따라 반대쪽은 따뜻해지는 것처럼 어떠한 영향도 반대의 영향을 받을 것이다.', '2017년까지 강원 양양과 충남 태안, 경남 남해 등 12개 해안 지역거점 12곳이 휴양·체험·생태벨트로 조성된다.국토교통부는 지난해부터 동서남해안 일대에 조성 중인 ‘휴양·체험·생태벨트’에 4곳을 추가해 총 12곳을 개발한다고 25일 발표했다.국토부는 지난해 동해 망상과 영덕 고래불, 강릉 심곡, 울산 진하, 전북 고창, 전남 진도항, 전남 고흥, 경남 거제 등 8곳을 휴양·체험·생태벨트 거점으로 개발하기로 하고 사업을 진행 중이다.새로 추가된 양양에는 오색 자연휴양체험지구가 조성된다. 올해부터 2017년까지 국비와 지방비 등 300억원을 투자해 양양군 서면 오색리 35만㎡ 일대에 캠핑장 등 자연휴양 체험시설을 짓는다. 태안군 소원면 만리포해수욕장 내 31만1853㎡에는 180억원을 들여 해안도로와 탐방로를 건설한다.또 남해군 미조면 미조리 조도와 호도 일원 24만㎡는 316억원을 투입해 접안시설과 다이어트 센터, 수상가옥 등을 조성해 남해 다이어트 보물섬으로 거듭나게 된다. 부산 해운대 동해남부선 폐선 부지 26만8555㎡에 240억원을 들여 해안 절경을 즐길 수 있는 공원 시설을 조성한다.국토부는 이번에 새로 추가된 4곳을 포함해 12곳에 2017년까지 국비와 지방지 2086억원 투입할 계획이다. 김영우 국토부 해안권발전지원과장은 “휴양·체험·생태벨트가 조성되면 국민의 여가생활 공간이 확대되고 지역 경제도 활성화돼 균형 발전의 기반이 될 것”이라며 “앞으로도 해안권별로 사업을 발굴해 개발할 계획”이라고 설명했다.', '“물빛만 봐도 동해는 한눈에 알아볼 수 있습니다.” 한국 국적선 사상 최초로 북극항로 운항에 나선 스테나폴라리스호(號)가 베링해협을 지나 동해에 들어선 19일. 젊은 시절 10년간 항해 경험이 있는 남청도 해양대 교수는 유난히 푸른 바다를 가리키며 이렇게 말했다. 이제 목적지인 광양항까지는 이틀 남았다. 스테나폴라리스는 광양항에 도착한다. 지난 9월16일 밤 러시아 우스트루가를 떠난 지 35일 만이다. 바스코 알렉산더 선장은 “올해는 작년보다 해빙(海氷) 양이 늘어난 데다 쇄빙선 일정에 차질이 생겨 예상보다 약간 늦어졌다”며 “그래도 수에즈 운하를 통과하는 (북유럽~한국) 노선에 비해 8일 정도 운항일수를 줄였다”고 했다. 남 교수는 이번 북극항로 시범운항의 성과에 대해 “전체적으로 수에즈 운하를 지나는 항로에 비해 20만달러 정도 적게 들어간 것으로 파악하고 있다. 이 정도면 경제성이 충분하다”는 견해를 피력했다. 이승헌 현대글로비스 해기사는 “북극항로는 1만5500㎞ 정도로 수에즈 운하를 지나는 항로보다 7000㎞나 짧아 연료비가 30만달러 정도 덜 들어간다”며 “하지만 쇄빙선료와 아이스파일럿비 등을 통행료로 내야 하고 (사고 위험에 대비한) 보험료도 훨씬 높기 때문에 이 부분을 얼마나 줄이느냐가 북극항로의 경제성을 좌우할 것”이라고 말했다. 북극항로 통행료에 대해 패트릭 스반 스테나해운 매니저는 “스테나해운의 북극항로 운항 실적이 많아 이번 통행료는 t당 5달러를 적용해 총 20만달러 정도 들었다”며 “하지만 운항실적이 없는 선사는 훨씬 비싼 비용을 내야 한다”고 했다. 남 교수는 “선사의 운항실적 등 복잡한 규정에 따라 통행료가 달라지는 현재의 관행이 바뀌어야 한다”고 지적했다.북극항로의 안전성에 대해서는 긍정적인 평가가 나왔다. 하지만 쇄빙선 운항시스템은 개선해야 할 점으로 지적됐다. 남 교수는 “쇄빙선이 당초 만나기로 한 시간에 나타나지 않아 사나흘씩 기다려야 하는 현실은 북극항로 활성화의 장애 요인”이라고 꼬집었다. 스반 매니저는 “여름철 서너 달은 항해가 안전하고 경제성도 있다고 본다”면서도 “해빙 등의 변수가 있기 때문에 당분간 (컨테이너선보다) 벌크선 위주로 운항이 많아질 것”이라고 내다봤다. 9월23일 북극권에 들어선 스테나폴라리스는 9월30일 처음 해빙구간(얼음바다)에 들어섰다. 쇄빙선을 기다리느라 북극해 한가운데에서 발이 묶이기도 했다.총 390㎞에 달하는 얼음바다에선 항해 속도를 평소의 절반 수준으로 줄여야 했다. 가끔 얼음이 선체에 부닥치는 소리가 들렸지만 위협적으로 느껴지지는 않았다. 11일 베링해로 들어선 뒤부터 다시 속도를 높였다. 그동안 제대로 누리지 못한 햇빛이 갑판 위에 쏟아졌다. 남 교수가 일렁이는 물결을 내려다보며 읊조렸다. “북극항로를 단순히 바닷길 하나가 새로 열리는 정도로 생각하면 안 됩니다. 북극권이 열리며 새로 생겨나는 세계 질서 속으로 가는 ‘21세기의 비단길’이 아닐까요.”']\n","<class 'list'> ['doc2']\n"]},{"output_type":"execute_result","data":{"text/plain":["                              question  \\\n","0  북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?   \n","1   성공적인 성과를 보인 지역SW서비스사업화 지원사업의 주최자는?   \n","2          개막전에서 3안타 2실점을 기록해서 패한 선수는?   \n","3              컵라면 매출에서 불닭볶음면을 이긴 상품은?   \n","4     정부에게 환경과 관련해서 우선적으로 원조 받고 있는 곳은?   \n","\n","                                                docs  \\\n","0  [1994년 11월, 배타적 경제수역(EEZ)을 정한 유엔 해양법 협약의 발효로 바...   \n","1  [아이티센(124500)그룹의 쌍용정보통신(010280)이 해군전술C4I체계 성능개...   \n","2  [시범 경기에서는 16이닝을 던져 15실점을 기록하는 등 성적이 좋지 않았지만 본인...   \n","3  [영화관 안에서만 주로 접했던 팝콘이 편의점 진열대의 윗자리를 차지하는 대표 스낵으...   \n","4  [포브스는 최근 “석탄은 기원전 315년 그리스 문헌에 대장간에서 원료로 쓰던 기록...   \n","\n","                                         user_prompt  \\\n","0  질문: 북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?\\n\\ndoc...   \n","1  질문: 성공적인 성과를 보인 지역SW서비스사업화 지원사업의 주최자는?\\n\\ndocs...   \n","2  질문: 개막전에서 3안타 2실점을 기록해서 패한 선수는?\\n\\ndocs:\\ndoc1...   \n","3  질문: 컵라면 매출에서 불닭볶음면을 이긴 상품은?\\n\\ndocs:\\ndoc1: 영화...   \n","4  질문: 정부에게 환경과 관련해서 우선적으로 원조 받고 있는 곳은?\\n\\ndocs:\\...   \n","\n","                                       system_prompt  \\\n","0  당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, ...   \n","1  당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, ...   \n","2  당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, ...   \n","3  당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, ...   \n","4  당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, ...   \n","\n","                                              answer doc_citations  type  \n","0  북태평양 기단과 오호츠크해 기단이 만나 형성되는 장마전선은 한반도 중남부를 오르내리...        [doc2]     1  \n","1  성공적인 성과를 보인 지역SW서비스사업화 지원사업의 주최자는 과학기술정보통신부입니다...        [doc5]     1  \n","2  개막전에서 3안타 2실점을 기록하여 패한 선수는 사이타마 세이부 라이온스의 와쿠이 ...        [doc1]     1  \n","3  컵라면 매출에서 불닭볶음면을 이긴 상품은 GS25의 '오모리 김치찌개라면'과 세븐일...        [doc5]     1  \n","4  정부에게 환경과 관련하여 우선적으로 원조를 받고 있는 곳은 중국입니다. 중국 정부는...        [doc2]     1  "],"text/html":["\n","  <div id=\"df-03443b92-cf1d-422a-bb22-1afec4a07a32\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>docs</th>\n","      <th>user_prompt</th>\n","      <th>system_prompt</th>\n","      <th>answer</th>\n","      <th>doc_citations</th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?</td>\n","      <td>[1994년 11월, 배타적 경제수역(EEZ)을 정한 유엔 해양법 협약의 발효로 바...</td>\n","      <td>질문: 북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?\\n\\ndoc...</td>\n","      <td>당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, ...</td>\n","      <td>북태평양 기단과 오호츠크해 기단이 만나 형성되는 장마전선은 한반도 중남부를 오르내리...</td>\n","      <td>[doc2]</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>성공적인 성과를 보인 지역SW서비스사업화 지원사업의 주최자는?</td>\n","      <td>[아이티센(124500)그룹의 쌍용정보통신(010280)이 해군전술C4I체계 성능개...</td>\n","      <td>질문: 성공적인 성과를 보인 지역SW서비스사업화 지원사업의 주최자는?\\n\\ndocs...</td>\n","      <td>당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, ...</td>\n","      <td>성공적인 성과를 보인 지역SW서비스사업화 지원사업의 주최자는 과학기술정보통신부입니다...</td>\n","      <td>[doc5]</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>개막전에서 3안타 2실점을 기록해서 패한 선수는?</td>\n","      <td>[시범 경기에서는 16이닝을 던져 15실점을 기록하는 등 성적이 좋지 않았지만 본인...</td>\n","      <td>질문: 개막전에서 3안타 2실점을 기록해서 패한 선수는?\\n\\ndocs:\\ndoc1...</td>\n","      <td>당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, ...</td>\n","      <td>개막전에서 3안타 2실점을 기록하여 패한 선수는 사이타마 세이부 라이온스의 와쿠이 ...</td>\n","      <td>[doc1]</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>컵라면 매출에서 불닭볶음면을 이긴 상품은?</td>\n","      <td>[영화관 안에서만 주로 접했던 팝콘이 편의점 진열대의 윗자리를 차지하는 대표 스낵으...</td>\n","      <td>질문: 컵라면 매출에서 불닭볶음면을 이긴 상품은?\\n\\ndocs:\\ndoc1: 영화...</td>\n","      <td>당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, ...</td>\n","      <td>컵라면 매출에서 불닭볶음면을 이긴 상품은 GS25의 '오모리 김치찌개라면'과 세븐일...</td>\n","      <td>[doc5]</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>정부에게 환경과 관련해서 우선적으로 원조 받고 있는 곳은?</td>\n","      <td>[포브스는 최근 “석탄은 기원전 315년 그리스 문헌에 대장간에서 원료로 쓰던 기록...</td>\n","      <td>질문: 정부에게 환경과 관련해서 우선적으로 원조 받고 있는 곳은?\\n\\ndocs:\\...</td>\n","      <td>당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, ...</td>\n","      <td>정부에게 환경과 관련하여 우선적으로 원조를 받고 있는 곳은 중국입니다. 중국 정부는...</td>\n","      <td>[doc2]</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03443b92-cf1d-422a-bb22-1afec4a07a32')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-03443b92-cf1d-422a-bb22-1afec4a07a32 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-03443b92-cf1d-422a-bb22-1afec4a07a32');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-a6214e3b-0d59-4ce4-b9ce-d78fd586f5de\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a6214e3b-0d59-4ce4-b9ce-d78fd586f5de')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-a6214e3b-0d59-4ce4-b9ce-d78fd586f5de button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_all","summary":"{\n  \"name\": \"df_all\",\n  \"rows\": 972,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 972,\n        \"samples\": [\n          \"\\ub2e8\\uccb4\\uc7a5 \\uc120\\uac70 \\ucd9c\\ub9c8 \\uc2dc \\ud589\\uc815\\uc5c5\\ubb34 \\uc548\\uc815 \\uc124\\uba85\\uc790\",\n          \"\\ud398\\ub974\\ub514\\ub09c\\ub3c4 2\\uc138\\uc758 \\uc804 \\ubd80\\uc778\\uc774 \\ub0b3\\uc740 \\uc790\\uc2dd\\uc758 \\uc774\\ub984\\uc740?\",\n          \"\\uc544\\uc11c\\uc2a4 \\ud328\\uc2a4 \\ud0c0\\uc6b4\\uc758 \\uc5ed\\uc0ac\\uc640 \\uc751\\uae09\\uc758\\ub8cc\\uccb4\\uacc4 \\uac1c\\ud3b8\\uc774 \\uc9c0\\uc5ed \\uc0ac\\ud68c\\uc5d0 \\ubbf8\\uce5c \\uc601\\ud5a5\\uc740 \\ubb34\\uc5c7\\uc778\\uac00\\uc694?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"docs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 972,\n        \"samples\": [\n          \"\\uc9c8\\ubb38: \\ub2e8\\uccb4\\uc7a5 \\uc120\\uac70 \\ucd9c\\ub9c8 \\uc2dc \\ud589\\uc815\\uc5c5\\ubb34 \\uc548\\uc815 \\uc124\\uba85\\uc790\\n\\ndocs:\\ndoc1: 6\\u00b74 \\uc9c0\\ubc29\\uc120\\uac70 \\uac15\\uc6d0\\uc9c0\\uc0ac\\uc640 \\ucd98\\ucc9c\\uc2dc\\uc7a5\\uc5d0 \\ucd9c\\ub9c8\\ud558\\uae30 \\uc704\\ud574 \\uc2dc\\uc7a5\\uacfc \\ubd80\\uc2dc\\uc7a5\\uc774 \\ubaa8\\ub450 \\uc0ac\\ud1f4\\ud55c \\ucd98\\ucc9c\\uc2dc. \\ucd98\\ucc9c\\uc2dc\\ub294 \\ud589\\uc815 \\uc218\\ub1cc\\ubd80\\uac00 \\uc790\\ub9ac\\ub97c \\ube44\\uc6b0\\uba74\\uc11c \\uc0ac\\uc2e4\\uc0c1 \\uc8fc\\uc694 \\ud589\\uc815\\uc5d0 \\uc190\\uc744 \\ub193\\uace0 \\uc788\\ub2e4. \\uc61b \\ubbf8\\uad70\\uae30\\uc9c0 \\ud130\\uc5d0 \\ube5b\\ud14c\\ub9c8\\ud30c\\ud06c\\ub97c \\uc870\\uc131\\ud558\\uae30 \\uc704\\ud55c \\uc6d4\\ub4dc\\ub77c\\uc774\\ud2b8\\ud30c\\ud06c \\uc0ac\\uc5c5 \\ucd94\\uc9c4\\uc744 \\ub193\\uace0 \\uc2e4\\ubb34\\uc120\\uc5d0\\uc11c \\ud63c\\uc120\\uc744 \\ube5a\\uace0 \\uc788\\ub2e4. \\uc2dc\\ub294 \\ubbfc\\uc790 \\uc0ac\\uc5c5\\uc790\\uc758 \\uc790\\uae08\\ub09c\\uc73c\\ub85c \\uc9c4\\ucc99\\ub418\\uc9c0 \\uc54a\\uc790 \\ucd5c\\uadfc \\u201c\\uc0ac\\uc5c5\\uc744 \\uc911\\ub2e8\\ud55c\\ub2e4\\u201d\\uace0 \\ud588\\ub2e4\\uac00 \\ub2e4\\uc74c \\ub0a0\\uc5d4 \\u201c\\uc0ac\\uc5c5\\ube44 \\uc911 50\\uc5b5\\uc6d0\\uc744 \\uc81c\\uc2dc\\ud558\\uba74 \\uc7ac\\uac1c\\ub97c \\uac80\\ud1a0\\ud558\\uaca0\\ub2e4\\u201d\\uace0 \\ubc29\\uce68\\uc744 \\ubc14\\uafb8\\ub294 \\ub4f1 \\uc88c\\ucda9\\uc6b0\\ub3cc\\ud558\\uace0 \\uc788\\ub2e4.\\uc774\\ucc98\\ub7fc \\ucd5c\\uadfc \\ub4e4\\uc5b4 6\\uc6d4 \\uc9c0\\ubc29\\uc120\\uac70\\uc640 7\\uc6d4 \\uad6d\\ud68c\\uc758\\uc6d0 \\ubcf4\\uad90\\uc120\\uac70\\uc5d0 \\ucd9c\\ub9c8\\ud558\\ub294 \\uc9c0\\ubc29\\uc790\\uce58\\ub2e8\\uccb4\\uc7a5\\uacfc \\ubd80\\ub2e8\\uccb4\\uc7a5\\ub4e4\\uc774 \\ub3d9\\uc2dc\\uc5d0 \\uc0ac\\ud1f4\\ud558\\uba74\\uc11c \\uc9c0\\uc790\\uccb4\\ub9c8\\ub2e4 \\uc5c5\\ubb34\\uacf5\\ubc31\\uc774 \\uc2ec\\ud55c \\uac83\\uc73c\\ub85c \\ub4dc\\ub7ec\\ub0ac\\ub2e4. 31\\uc77c \\ucd9c\\ub9c8\\ub97c \\uc704\\ud574 \\ud1f4\\uc784\\ud55c \\uc774\\uc885\\ud654 \\ub300\\uad6c \\ubd81\\uad6c\\uccad\\uc7a5\\uc744 \\ube44\\ub86f\\ud574 \\uad6d\\ud68c\\uc758\\uc6d0 \\ubcf4\\uad90\\uc120\\uac70 \\ucd9c\\ub9c8\\ub97c \\uc704\\ud574 \\uc911\\ub3c4 \\uc0ac\\ud1f4\\ud55c \\ub2e8\\uccb4\\uc7a5\\ub9cc \\ubc15\\ub9f9\\uc6b0 \\uc6b8\\uc0b0\\uc2dc\\uc7a5, \\ubc30\\ub355\\uad11 \\ubd80\\uc0b0 \\ud574\\uc6b4\\ub300\\uad6c\\uccad\\uc7a5, \\uc774\\uc885\\ubc30 \\ucda9\\uc8fc\\uc2dc\\uc7a5 \\ub4f1 4\\uba85\\uc5d0 \\uc774\\ub978\\ub2e4.\\u25cb\\ub2e8\\uccb4\\uc7a5\\u00b7\\ubd80\\ub2e8\\uccb4\\uc7a5 \\ub3d9\\uc2dc \\ud1f4\\uc784\\uc548\\uc804\\ud589\\uc815\\ubd80\\uc5d0 \\ub530\\ub974\\uba74 \\uc9c0\\ub09c\\ud574 \\ub9d0\\ubd80\\ud130 \\uc9c0\\uae08\\uae4c\\uc9c0 \\uc120\\uac70 \\ucd9c\\ub9c8\\ub97c \\uc704\\ud574 \\uc0ac\\uc9c1\\ud55c \\uc9c0\\ubc29\\uacf5\\ubb34\\uc6d0\\uc740 139\\uba85\\uc73c\\ub85c \\uc9d1\\uacc4\\ub410\\ub2e4. \\uc774\\ub294 4\\ub144 \\uc804\\ubcf4\\ub2e4 11\\uba85 \\uc904\\uc5c8\\uc9c0\\ub9cc \\uc774\\ubc88\\uc5d0\\ub294 \\ud2b9\\ud788 \\ub2e8\\uccb4\\uc7a5\\uacfc \\ubd80\\ub2e8\\uccb4\\uc7a5\\uc774 \\ub3d9\\uc2dc \\uc0ac\\ud1f4\\ud558\\ub294 \\uacbd\\uc6b0\\uac00 \\ub9ce\\uc544 \\ud589\\uc815 \\uacf5\\ubc31\\uc774 \\ub354\\uc6b1 \\ucee4\\uc9c0\\uace0 \\uc788\\ub2e4\\ub294 \\ubd84\\uc11d\\uc774\\ub2e4. \\ub2e8\\uccb4\\uc7a5\\uacfc \\ubd80\\ub2e8\\uccb4\\uc7a5\\uc774 \\ub3d9\\uc2dc\\uc5d0 \\ud1f4\\uc784\\ud574 \\uacf5\\uc11d\\uc774 \\ub41c \\uacf3\\ub9cc \\ucd98\\ucc9c\\uc2dc\\u00b7\\uc804\\uc8fc\\uc2dc\\u00b7\\ucc3d\\uc6d0\\uc2dc\\u00b7\\ub300\\uad6c \\ubd81\\uad6c \\ub4f1 4\\uacf3\\uc5d0 \\uc774\\ub978\\ub2e4.\\ub300\\uad6c \\ubd81\\uad6c\\uc758 \\uacbd\\uc6b0 \\ubc30\\uad11\\uc2dd \\ubd80\\uad6c\\uccad\\uc7a5\\uc740 \\ubd81\\uad6c\\uccad\\uc7a5 \\ucd9c\\ub9c8\\ub97c \\uc704\\ud574 \\ud55c \\ub2ec\\uc5ec \\uc804 \\uba85\\uc608\\ud1f4\\uc9c1\\ud588\\ub294\\ub370 \\uc774\\ubc88\\uc5d0 \\uc774\\uc885\\ud654 \\uad6c\\uccad\\uc7a5\\ub3c4 \\ucd9c\\ub9c8\\ub97c \\uc704\\ud574 \\ud1f4\\uc784\\ud588\\ub2e4. \\ub610 \\uc804\\ubd81 \\uc804\\uc8fc\\uc2dc\\ub294 \\uc7a5\\uc0c1\\uc9c4 \\ubd80\\uc2dc\\uc7a5\\uc774 \\uc9c0\\ub09c 2\\uc6d4 \\ub9d0 \\uc1a1\\ud558\\uc9c4 \\uc2dc\\uc7a5\\uc774 \\uc804\\ubd81\\uc9c0\\uc0ac \\ucd9c\\ub9c8\\ub97c \\uc120\\uc5b8\\ud55c \\uc9c0 3\\uc77c \\ub4a4 \\uc804\\uc8fc\\uc2dc\\uc7a5 \\ucd9c\\ub9c8\\ub97c \\uc704\\ud574 \\uba85\\uc608\\ud1f4\\uc784\\ud558\\uba74\\uc11c \\uc2dc\\uc815\\uc5c5\\ubb34\\uc5d0 \\ucc28\\uc9c8\\uc744 \\ube5a\\uace0 \\uc788\\ub2e4. \\u25cb\\u201c\\uc5c5\\ubb34 \\uc2dc\\ub289\\ub9cc \\ud558\\uba74 \\ub3fc\\u201d \\uadf8\\ub3d9\\uc548 \\ucd94\\uc9c4\\ud574\\uc654\\ub358 \\uc8fc\\uc694 \\uc9c0\\uc5ed \\ud604\\uc548\\uc0ac\\uc5c5\\uc774 \\uc120\\uac70 \\uc7c1\\uc810\\ud654\\ub418\\uba74\\uc11c \\ud604\\uc548 \\uc0ac\\uc5c5\\uc774 \\uc9c4\\ucc99\\ub418\\uc9c0 \\uc54a\\uace0 \\uc788\\ub2e4. \\uc120\\uac70\\ucca0 \\uacf5\\ubb34\\uc6d0\\ub4e4\\uc758 \\ub208\\uce58\\ubcf4\\uae30 \\ud604\\uc0c1\\uc774 \\ub098\\ud0c0\\ub098\\uace0 \\uc788\\ub294 \\uac83\\uc774\\ub2e4.\\uc804\\uc8fc\\uc2dc\\ub294 \\uc62c \\uc0c1\\ubc18\\uae30 \\uc608\\uc815\\ub41c \\ub355\\uc9c4\\ub3d9 \\uc804\\uc8fc\\uc885\\ud569\\uacbd\\uae30\\uc7a5 \\uc7ac\\uac1c\\ubc1c\\uc0ac\\uc5c5 \\uc6a9\\uc5ed \\uacb0\\uacfc \\ubc1c\\ud45c\\ub97c \\ubbf8\\ub8e8\\uace0 \\uc788\\ub2e4. \\ucc3d\\uc6d0\\uc2dc\\ub294 \\ud504\\ub85c\\uc57c\\uad6c NC\\ub2e4\\uc774\\ub178\\uc2a4 \\uc57c\\uad6c\\uc7a5 \\uac74\\ub9bd\\uc744 \\uc6d0\\uc810\\uc5d0\\uc11c \\uc7ac\\uac80\\ud1a0\\ud558\\uae30\\ub85c \\ud588\\ub2e4. \\uacbd\\ub0a8\\ub3c4\\uc758 \\ud55c \\uacf5\\ubb34\\uc6d0\\uc740 \\u201c\\uc9c0\\uc790\\uccb4\\ub9c8\\ub2e4 \\ub204\\uac00 \\ub2e8\\uccb4\\uc7a5\\uc73c\\ub85c \\uc62c\\uc9c0 \\ubaa8\\ub974\\ub294 \\uc0c1\\ud669\\uc778\\ub370 \\uc5b4\\ub290 \\uacf5\\uc9c1\\uc790\\uac00 \\uae30\\uc874 \\uc0ac\\uc5c5\\uc744 \\uacc4\\uc18d \\ud558\\ub824 \\ud558\\uaca0\\ub290\\ub0d0\\u201d\\uace0 \\ub9d0\\ud588\\ub2e4. \\ub2e8\\uccb4\\uc7a5\\ub4e4\\uc774 \\ubd88\\ucd9c\\ub9c8 \\uc120\\uc5b8\\uc744 \\ud55c \\uc9c0\\uc790\\uccb4\\ub294 \\ub354 \\u2018\\ub178\\uace8\\uc801\\u2019\\uc774\\ub77c\\ub294 \\uc9c0\\uc801\\uc774\\ub2e4. \\ubc15\\uc900\\uc601 \\uc804\\ub0a8\\uc9c0\\uc0ac\\uac00 3\\uc120 \\uc5f0\\uc784 \\uc81c\\ud55c\\uc744 \\ubc1b\\ub294 \\uac00\\uc6b4\\ub370 \\uc804\\ub0a8\\ub3c4 \\ubaa8 \\uac04\\ubd80\\ub294 \\ucd5c\\uadfc \\ube44\\uacf5\\uc2dd\\uc11d\\uc0c1\\uc5d0\\uc11c \\u201c\\ubc15 \\uc9c0\\uc0ac\\uc758 \\uc5c5\\ubb34\\uc9c0\\uc2dc\\uc5d0 \\ub300\\ud574 \\uc77c\\ud558\\ub294 \\uc2dc\\ub289\\ub9cc \\ud558\\uc9c0 \\ubb34\\ub9ac\\ud574\\uc11c \\uc77c\\ud560 \\ud544\\uc694\\uac00 \\uc5c6\\ub2e4\\u201d\\uace0 \\ubc1c\\uc5b8\\ud588\\ub2e4\\uac00 \\uc9c8\\ucc45\\uc744 \\ubc1b\\ub294 \\uacbd\\uc6b0\\uae4c\\uc9c0 \\uc788\\uc5c8\\ub2e4. \\uc804\\ubd81\\ub3c4\\ub294 \\uc815\\ubb34\\ubd80\\uc9c0\\uc0ac\\uc640 \\ud589\\uc815\\ubd80\\uc9c0\\uc0ac, \\uae30\\ud68d\\uad00\\ub9ac\\uc2e4\\uc7a5 \\ub4f1\\uc774 \\uc0ac\\ud1f4\\ud558\\uba74\\uc11c \\ub3c4\\uccad\\uc758 \\ud575\\uc2ec\\uac04\\ubd80 \\ubaa8\\ub450\\uac00 \\uacf5\\uc11d\\uc73c\\ub85c \\uc5c5\\ubb34\\uac00 \\uc774\\ub904\\uc9c0\\uc9c0 \\uc54a\\uace0 \\uc788\\ub2e4. \\uc870\\uc7ac\\ud638 \\uc6b8\\uc0b0\\ub300 \\uacbd\\uc81c\\ud559\\ubd80 \\uad50\\uc218\\ub294 \\u201c\\uc120\\uac70 \\ub54c\\ub9c8\\ub2e4 \\uc7ac\\uc5f0\\ub418\\ub294 \\uc9c0\\ubc29\\uc815\\ubd80 \\uacf5\\uc9c1\\uc0ac\\ud68c\\uc758 \\ub3d9\\uc694\\ub97c \\ub9c9\\uae30 \\uc704\\ud574\\uc11c\\ub294 \\ub2e8\\uccb4\\uc7a5 \\uc778\\uc0ac\\uad8c \\ub3c5\\uc810\\uc744 \\ucd5c\\uc18c\\ud654\\ud558\\ub294 \\ub4f1 \\ub300\\ucc45 \\ub9c8\\ub828\\uc774 \\ud544\\uc694\\ud558\\ub2e4\\u201d\\uace0 \\uac15\\uc870\\ud588\\ub2e4. \\uc6b8\\uc0b0\\u00b7\\ucc3d\\uc6d0\\u00b7\\ub300\\uad6c\\u00b7\\uc804\\uc8fc=\\ud558\\uc778\\uc2dd/\\uac15\\uc885\\ud6a8 /\\uae40\\ub355\\uc6a9/\\ucd5c\\uc131\\uad6d \\uae30\\uc790\\ndoc2: \\u201c\\uc9c1\\uc6d0\\ub4e4\\uc758 \\ub0b4\\ubd80 \\uacbd\\uc7c1\\uc744 \\ub192\\uc5ec \\ubc25\\uac12 \\uc81c\\ub300\\ub85c \\ud558\\ub294 \\uacf5\\uae30\\uc5c5\\uc73c\\ub85c \\uac70\\ub4ed\\ub0a0 \\uac83\\uc785\\ub2c8\\ub2e4.\\u201d\\uae40\\uc601\\ud559 \\ud55c\\uad6d\\ubb34\\uc5ed\\ubcf4\\ud5d8\\uacf5\\uc0ac \\uc0ac\\uc7a5(\\uc0ac\\uc9c4)\\uc740 24\\uc77c \\ud55c\\uad6d\\uacbd\\uc81c\\uc2e0\\ubb38\\uacfc\\uc758 \\uc778\\ud130\\ubdf0\\uc5d0\\uc11c \\u201c\\uacf5\\uae30\\uc5c5\\uc774 \\uacbd\\uc7c1\\ub825\\uc744 \\uac00\\uc9c0\\ub824\\uba74 \\ub0b4\\ubd80 \\uacbd\\uc7c1\\uccb4\\uc81c\\ubd80\\ud130 \\uc815\\ube44\\ud574\\uc57c \\ud55c\\ub2e4\\u201d\\uba70 \\ucd5c\\uadfc \\ub2e8\\ud589\\ud55c \\uc870\\uc9c1 \\uac1c\\ud3b8\\uacfc \\uc778\\uc0ac \\ubc30\\uacbd\\uc744 \\uc124\\uba85\\ud588\\ub2e4. \\ub300\\uac1c \\uacf5\\uae30\\uc5c5 \\uc218\\uc7a5\\ub4e4\\uc774 \\ucde8\\uc784 3\\uac1c\\uc6d4 \\uc548\\uc5d0 \\uc774\\ub97c \\uc2e4\\uc2dc\\ud558\\ub294 \\uac83\\uacfc \\ub2ec\\ub9ac \\uadf8\\ub294 \\uc870\\uc9c1 \\uad6c\\uc11d\\uad6c\\uc11d\\uc744 \\uc9c4\\ub2e8\\ud55c \\ub4a4 10\\uac1c\\uc6d4 \\ub9cc\\uc5d0 \\uc790\\uae30 \\uc0c9\\uae54\\uc744 \\ubc18\\uc601\\ud588\\ub2e4.\\uc6b0\\uc120 \\ud604\\uc7a5 \\ubc00\\ucc29\\ud615 \\uc601\\uc5c5\\uc870\\uc9c1\\uc778 \\uc9c0\\uc5ed\\ubcf8\\ubd80\\uc81c\\ub97c \\ucc3d\\ub9bd \\uc774\\ub798 \\ucc98\\uc74c\\uc73c\\ub85c \\ub3c4\\uc785\\ud588\\ub2e4. \\uae30\\uc874\\uc758 \\uad6d\\ub0b4 \\uc9c0\\uc0ac\\ub97c \\uad11\\uc5ed\\uc73c\\ub85c \\ubb36\\uc5b4\\uc11c \\uc9c0\\uc5ed\\ubcf8\\ubd80\\uc7a5\\uc774 \\uc9c1\\uc811 \\uc218\\ucd9c\\ud604\\uc7a5\\uc744 \\ucc59\\uae30\\ub3c4\\ub85d \\ud588\\ub2e4. \\uc758\\uc0ac\\uacb0\\uc815 \\uad8c\\ud55c\\uc744 \\ub300\\ud3ed \\uc704\\uc784\\ud574 \\ud604\\uc7a5\\uc758 \\uc9c0\\uc5ed\\ubcf8\\ubd80\\uc7a5\\uc774 \\uc804\\uacb0\\ud560 \\uc218 \\uc788\\uac8c \\ud588\\ub2e4.\\uae40 \\uc0ac\\uc7a5\\uc740 \\u201c\\uadf8\\ub3d9\\uc548 5\\uba85\\uc758 \\ubcf8\\ubd80\\uc7a5(\\uc0c1\\uc784\\uc774\\uc0ac \\ubcf8\\ubd80\\uc7a5 4\\uba85\\uacfc 1\\uae09 \\ubcf8\\ubd80\\uc7a5 1\\uba85)\\uc73c\\ub85c \\uc6b4\\uc601\\ub418\\ub358 \\uacbd\\uc601\\uc9c4\\uc744 6\\uba85\\uc758 \\ubcf8\\ubd80\\uc7a5(\\uc0c1\\uc784\\uc774\\uc0ac \\ubcf8\\ubd80\\uc7a5 2\\uba85\\uacfc 1\\uae09 \\ubcf8\\ubd80\\uc7a5 4\\uba85)\\uc73c\\ub85c \\uc7ac\\ud3b8\\ud574\\uc11c \\ud604\\uc7a5\\uacbd\\uc601\\uc744 \\ub4b7\\ubc1b\\uce68\\ud558\\ub3c4\\ub85d \\ud588\\ub2e4\\u201d\\uba70 \\u201c\\uc774\\ub294 \\uc0dd\\uc0b0\\uc801\\uc778 \\ub0b4\\ubd80\\uacbd\\uc7c1\\uc774 \\uc77c\\uc5b4\\ub098\\ub3c4\\ub85d \\uc720\\ub3c4\\ud558\\ub294 \\uac83\\u201d\\uc774\\ub77c\\uace0 \\ub9d0\\ud588\\ub2e4.\\uadf8\\ub3d9\\uc548\\uc740 1\\uae09 \\ubcf8\\ubd80\\uc7a5(\\uc784\\uae30 2\\ub144+1\\ub144)\\uc5d0\\uc11c \\uc0c1\\uc784\\uc774\\uc0ac \\ubcf8\\ubd80\\uc7a5(2\\ub144+1\\ub144)\\uc73c\\ub85c \\uc2b9\\uc9c4\\ud558\\uc9c0 \\ubabb\\ud558\\uba74 \\ud55c\\uc9c1\\uc778 \\uc5f0\\uad6c\\uc704\\uc6d0\\uc73c\\ub85c \\ubb3c\\ub7ec\\ub098\\uc57c \\ud588\\ub2e4. \\uadf8\\ub7ec\\ub098 \\uc774\\ubc88\\uc5d0 1\\uae09 \\ubcf8\\ubd80\\uc7a5\\uc744 3\\uba85 \\ucd94\\uac00\\ud574 \\uc9c1\\uc6d0\\ub4e4\\uc758 \\uc2b9\\uc9c4 \\uae30\\ud68c\\ub97c \\ub298\\ub824\\uc8fc\\uba74\\uc11c \\uacbd\\uc7c1\\ub3c4 \\ub354 \\uc774\\ub04c\\uc5b4\\ub0b4\\uae30\\ub85c \\ud588\\ub2e4. \\ud604\\uc7a5\\uc5d0\\uc11c \\uc794\\ubf08\\uac00 \\uad75\\uc740 4\\uba85\\uc758 \\uc9c1\\uc6d0\\ub3c4 \\ubd80\\uc7a5\\uc73c\\ub85c \\ubc1c\\ud0c1\\ud588\\ub2e4. \\uc120\\uc784\\ubd80\\uc7a5\\uc778 \\ucd1d\\ubb34\\ubd80\\uc7a5\\uc5d0\\ub294 \\uc911\\uc18c\\uae30\\uc5c5 \\uc9c0\\uc6d0\\uc5d0 \\uc624\\ub79c \\uacbd\\ud5d8\\uc744 \\uac16\\ucd98 \\uc5ec\\uc131\\ubd80\\uc7a5(\\uc774\\ubbf8\\uc601)\\uc744 \\uc784\\uba85\\ud588\\ub2e4.\\ubb34\\uc5ed\\ubcf4\\ud5d8\\uacf5\\uc0ac\\uac00 \\uc774\\ucc98\\ub7fc \\ud68d\\uae30\\uc801\\uc778 \\uc870\\uc9c1 \\uac1c\\ud3b8\\uacfc \\uc778\\uc0ac\\ub97c \\uc2e4\\uc2dc\\ud55c \\uac83\\uc740 \\uc911\\uc18c\\u00b7\\uc911\\uacac\\uae30\\uc5c5\\uc758 \\uc218\\ucd9c \\uc9c0\\uc6d0\\uc744 \\uac15\\ud654\\ud558\\uae30 \\uc704\\ud55c \\ud3ec\\uc11d\\uc774\\ub2e4. \\uadf8\\ub294 \\u201c\\uc911\\uc18c\\u00b7\\uc911\\uacac \\uc218\\ucd9c\\uae30\\uc5c5\\ub4e4\\uc758 \\uc560\\ub85c\\uc640 \\uc694\\uad6c\\uc0ac\\ud56d\\uc744 \\ubb34\\uc5ed\\ud604\\uc7a5\\uc5d0\\uc11c \\uc989\\uc2dc \\ud30c\\uc545\\ud558\\uace0 \\ud574\\uacb0\\ud574 \\ubb34\\uc5ed\\ubcf4\\ud5d8\\uc5d0 \\ub300\\ud55c \\uace0\\uac1d\\uae30\\uc5c5\\ub4e4\\uc758 \\uc811\\uadfc\\uc131\\uacfc \\uc11c\\ube44\\uc2a4 \\uc218\\uc900\\uc744 \\ub192\\uc5ec \\ub098\\uac08 \\uac83\\u201d\\uc774\\ub77c\\uace0 \\ub9d0\\ud588\\ub2e4. \\u201c\\ud2b9\\ud788 \\uc5f0\\uac04 \\uc218\\ucd9c 100\\ub9cc\\ub2ec\\ub7ec \\uc774\\ud558 7\\ub9cc3000\\uc5ec\\uac1c \\uc601\\uc138 \\uc218\\ucd9c\\uae30\\uc5c5 \\uc911 \\ubb34\\uc5ed\\ubcf4\\ud5d8\\uacf5\\uc0ac\\uc758 \\ubb34\\uc5ed\\ubcf4\\ud5d8 \\ud61c\\ud0dd\\uc744 \\ubcf4\\uace0 \\uc788\\ub294 \\uacf3\\uc744 \\ud604\\uc7ac\\uc758 1\\ub9cc\\uc5ec\\uac1c\\uc5d0\\uc11c 2\\ub9cc\\uac1c\\ub85c \\ub298\\ub9ac\\uaca0\\ub2e4\\u201d\\uace0 \\ubc1d\\ud614\\ub2e4.\\uc62c\\ud574 \\uc911\\uc18c\\u00b7\\uc911\\uacac\\uae30\\uc5c5 \\ubb34\\uc5ed\\ubcf4\\ud5d8 \\uc9c0\\uc6d0 \\ubaa9\\ud45c\\ub97c \\uc5ed\\ub300 \\ucd5c\\ub300 \\uaddc\\ubaa8\\uc778 40\\uc870\\uc6d0\\uc73c\\ub85c \\ud655\\ub300\\ud55c \\uc774\\uc720\\ub2e4. \\uae30\\uc5c5\\uc740\\ud589, \\uc678\\ud658\\uc740\\ud589, \\uad6d\\ubbfc\\uc740\\ud589\\uc73c\\ub85c\\ubd80\\ud130 \\ud2b9\\ubcc4\\uae30\\uae08\\uc744 \\ucd9c\\uc5f0\\ubc1b\\uc544 \\uc911\\uc18c \\uc218\\ucd9c\\uae30\\uc5c5\\uc744 \\uc9c0\\uc6d0\\ud558\\ub294 \\uc0ac\\uc5c5\\ub3c4 \\uacc4\\uc18d \\ub298\\ub824\\uac04\\ub2e4\\ub294 \\uacc4\\ud68d\\uc774\\ub2e4. \\ubb34\\uc5ed\\ubcf4\\ud5d8\\uacf5\\uc0ac\\ub294 \\uae30\\uae08\\uc744 \\ubb34\\uc5ed\\ubcf4\\ud5d8 \\uc9c0\\uc6d0\\uae08\\uc73c\\ub85c \\ud65c\\uc6a9\\ud560 \\uc218 \\uc788\\uace0 \\uc740\\ud589\\ub4e4\\uc740 \\uc720\\ub9dd \\uc218\\ucd9c\\uae30\\uc5c5\\uc744 \\uace0\\uac1d\\uc73c\\ub85c \\ud655\\ubcf4\\ud560 \\uc218 \\uc788\\ub2e4.\\uc218\\ucd9c \\uacbd\\ud5d8\\uc774 \\ubd80\\uc871\\ud558\\uac70\\ub098 \\uc544\\uc608 \\uc5c6\\ub294 \\uc218\\ucd9c \\ucd08\\ubcf4\\uae30\\uc5c5\\uacfc \\ub0b4\\uc218\\uae30\\uc5c5\\ub4e4\\uc774 \\uac04\\ud3b8\\ud55c \\uc808\\ucc28\\ub85c \\ucd5c\\ub300 10\\ub9cc\\ub2ec\\ub7ec\\uae4c\\uc9c0 \\uc9c0\\uc6d0\\ubc1b\\uc744 \\uc218 \\uc788\\ub294 \\u2018\\uc218\\ucd9c \\uccab\\uac78\\uc74c \\ud76c\\ub9dd\\ubcf4\\ud5d8\\u2019\\ub3c4 \\uc2dc\\ud589\\ud558\\uace0 \\uc788\\ub2e4. \\ubb34\\uc5ed\\ubcf4\\ud5d8\\uacf5\\uc0ac \\ub178\\uc0ac\\ub294 \\uc9c0\\ub09c 3\\uc6d4 \\uae08\\uc735 \\uacf5\\uae30\\uc5c5 \\ucd5c\\ucd08\\ub85c \\ubc29\\ub9cc\\uacbd\\uc601 \\ud574\\uc18c \\uc774\\ud589 \\ubc29\\uc548\\uc5d0 \\ub300\\ud574 \\ud569\\uc758\\ud588\\ub2e4. \\ub355\\ubd84\\uc5d0 \\uc9c0\\ub09c 7\\uc6d4 \\uc815\\ubd80\\uc758 \\ubc29\\ub9cc\\uacbd\\uc601 \\uc911\\uc810\\uad00\\ub9ac \\uacf5\\uacf5\\uae30\\uad00\\uc5d0\\uc11c \\ud574\\uc81c\\ub410\\ub2e4.\\ndoc3: \\uc989\\uac01\\uc801\\uc73c\\ub85c \\uc815\\uad8c\\uc744 \\uc778\\uc218\\ubc1b\\uc740 \\ud6c4 \\ud074\\ub9b0\\ud134\\uc740 1993\\ub144 \\uac00\\uc871 \\uc758\\ub8cc \\ubc95\\uc548\\uc5d0 \\uad00\\ud55c \\ub300\\uc120 \\uacf5\\uc57d\\uc5d0 \\uc989\\uac01 \\uc11c\\uba85\\ud558\\uac8c \\ub41c\\ub2e4. \\ubcf8 \\ubc95\\uc548\\uc740 \\uace0\\uc6a9\\uc778\\uc5d0\\uac8c \\uc885\\uc5c5\\uc6d0\\uc758 \\uc758\\ub8cc \\ubb38\\uc81c \\ubc1c\\uc0dd \\uc0c1\\ud669\\uc2dc \\uc758\\ub8cc \\ubcf4\\ud638\\ub97c \\ubc1b\\uc744 \\uc218 \\uc788\\ub3c4\\ub85d \\uadfc\\ub85c \\uc0c1\\ud669\\uc744 \\uac1c\\uc120\\ud558\\ub294 \\uac83\\uc744 \\uc694\\uad6c\\ud560 \\uc218 \\uc788\\ub3c4\\ub85d \\ud558\\ub294 \\uac83\\uc744 \\uace8\\uc790\\ub85c \\ud558\\ub294 \\ubcf4\\ud638 \\ubc95\\uc548\\uc774\\uc5c8\\ub2e4.\\n\\n\\uc774 \\uc815\\ucc45\\uc774 \\ub300\\uc911\\uc801\\uc774\\uc5c8\\ub358 \\ubc18\\uba74, \\ud074\\ub9b0\\ud134\\uc774 \\ub300\\uc120 \\ucd08\\uae30\\uc5d0 \\uacf5\\uc57d\\ud588\\ub358 \\uad70\\ub300 \\ub0b4 \\uc131\\uc18c\\uc218\\uc790 \\uad8c\\ub9ac \\uc815\\ucc45\\uc5d0 \\ub300\\ud55c \\uba85\\ud655\\ud558\\uc9c0 \\uc54a\\uc740 \\ud0dc\\ub3c4\\ub294 \\ubcf4\\uc218\\uc640 \\uc9c4\\ubcf4 \\uc591\\uce21\\uc73c\\ub85c\\ubd80\\ud130 \\ube44\\ud310\\uc744 \\ubc1b\\uac8c \\ub418\\uc5c8\\ub2e4. \\uc9c4\\ubcf4 \\uc9c4\\uc601\\uc740 \\uc815\\ucc45\\uc774 \\ub2e4\\uc18c \\uc2e4\\ud5d8\\uc801\\uc774\\ub77c\\uace0 \\uc8fc\\uc7a5\\ud588\\uace0 \\ubcf4\\uc218\\uc9c4\\uc601\\uc740 \\uad70\\uc0dd\\ud65c\\uc5d0\\uc11c \\ubcc4 \\ubc18\\uc751\\uc744 \\ubcf4\\uc774\\uc9c0 \\uc54a\\ub294 \\uc815\\ucc45\\uc774\\ub77c\\uace0 \\ud3c9\\uac00\\ud588\\ub2e4. \\ub9ce\\uc740 \\ud1a0\\uc758\\uac00 \\uc788\\ub358 \\ud6c4 \\ud074\\ub9b0\\ud134\\uacfc \\ud39c\\ud0c0\\uace4\\uc740 \\uc77c\\uba85 \\u2018\\ubb3b\\uc9c0\\ub3c4 \\ub9d0\\uace0 \\ub300\\ub2f5\\ub3c4 \\ud558\\uc9c0 \\ub9d0\\ub77c\\u2019\\ub294 \\uc815\\ucc45\\uc5d0 \\ud569\\uc758\\ud558\\uac8c \\ub418\\uba70 \\uadf8\\uac83\\uc740 \\uc624\\ubc14\\ub9c8 \\ub300\\ud1b5\\ub839 \\ub54c\\uae4c\\uc9c0 \\uc720\\ud6a8\\ud588\\ub2e4.\\n\\n\\ud074\\ub9b0\\ud134 \\ub300\\ud1b5\\ub839\\uc740 \\ucde8\\uc784\\ud558\\uc790\\ub9c8\\uc790 \\uacf5\\uc57d\\ub300\\ub85c \\uc5f0\\ubc29\\uacf5\\ubb34\\uc6d0 10\\ub9cc\\uba85 \\uac10\\ucd95 \\uc9c0\\uc2dc\\ub97c \\ub0b4\\ub838\\uace0, \\uace0\\uc5b4 \\ubd80\\ud1b5\\ub839\\uc5d0\\uac8c \\uc815\\ubd80\\ub97c \\uc644\\uc804\\ud788 \\uc0c8\\ub86d\\uac8c \\uc7ac\\ucc3d\\uc870\\ud558\\uae30 \\uc704\\ud55c \\ubc29\\uc548\\uc744 \\uac15\\uad6c\\ud558\\ub3c4\\ub85d \\uba85\\ub839\\ud588\\ub2e4. \\uc774\\uc5d0 \\ub530\\ub77c \\uace0\\uc5b4 \\ubd80\\ud1b5\\ub839\\uc758 \\uc8fc\\ub3c4 \\uc544\\ub798 \\uad6d\\uc815\\uc131\\uacfc\\ud3c9\\uac00\\ud300(NPR)\\uc744 \\uc124\\uce58\\ud558\\uace0 \\ubcf8\\uaca9\\uc801\\uc778 \\uac1c\\ud601 \\uc791\\uc5c5\\uc5d0 \\ucc29\\uc218\\ud588\\ub2e4. \\ub2f9\\uc2dc \\ubbf8\\uad6d\\uc758 \\ud589\\uc815 \\uac1c\\ud601\\uc740 '\\uc815\\ubcf4\\uae30\\uc220\\uc744 \\ud1b5\\ud55c \\uc815\\ubd80 \\uc7ac\\uad6c\\ucd95' \\ud504\\ub85c\\uadf8\\ub7a8\\uc744 \\ud1b5\\ud574 \\uacf5\\ubb34\\uc6d0\\uc744 30\\ub9cc\\uba85 \\uc774\\uc0c1 \\uac10\\ucd95\\ud558\\ub294 \\uc131\\uacfc\\ub97c \\uac70\\ub480\\ub2e4. \\uc774\\ub7ec\\ud55c \\ubbf8\\uad6d \\ud589\\uc815 \\uac1c\\ud601\\uc758 \\uc131\\uacf5\\uc740 \\uc815\\ubcf4\\uae30\\uc220\\uc744 \\ud65c\\uc6a9\\ud55c \\uc804\\uc790\\uc815\\ubd80\\uc758 \\uad6c\\ud604\\uc744 \\ud1b5\\ud574 \\uc774\\ub8ec \\uacb0\\uacfc\\ub77c\\ub294 \\ud3c9\\uac00\\uac00 \\uc788\\ub2e4.\\ndoc4: \\ubb3c\\ubc11 \\uc120\\uac70\\uc6b4\\ub3d9\\uc740 \\uc9c0\\ub09c\\ud574 2\\uc6d428\\uc77c \\uc2dc\\uc791\\ub410\\ub2e4. \\ud604\\uc9c1\\uc778 \\uae40\\uae30\\ubb38 \\ud68c\\uc7a5\\uc774 \\uc8fc\\uc7ac\\ud55c \\ub9c8\\uc9c0\\ub9c9 \\uc815\\uae30\\ucd1d\\ud68c\\uc5d0\\uc11c\\ub2e4. \\ub2f9\\uc2dc \\uc720\\ub825 \\ud6c4\\ubcf4\\ub85c \\uc11c\\ubcd1\\ubb38 \\uc8fc\\ubb3c\\uacf5\\uc5c5\\ud611\\ub3d9\\uc870\\ud569 \\uc774\\uc0ac\\uc7a5, \\uc774\\uc7ac\\uad11 \\uc804\\uae30\\uacf5\\uc5c5\\ud611\\ub3d9\\uc870\\ud569 \\uc774\\uc0ac\\uc7a5 \\ub4f1\\uc774 \\uac70\\ub860\\ub420 \\ub54c\\ub2e4. \\ub450 \\uc0ac\\ub78c \\ubaa8\\ub450 \\uc911\\uc559\\ud68c \\ubd80\\ud68c\\uc7a5\\uc73c\\ub85c \\uae40 \\ud68c\\uc7a5\\uc744 \\uc784\\uae30 \\ub0b4\\ub0b4 \\uc606\\uc5d0\\uc11c \\ubcf4\\uc88c\\ud588\\ub2e4. \\uae40 \\ud68c\\uc7a5\\uacfc \\uac70\\ub9ac\\ub97c \\ub450\\uace0 \\uc788\\ub358 \\ubc15\\uc8fc\\ubd09 \\ucca0\\uac15\\uad6c\\uc870\\ubb3c\\ud611\\ub3d9\\uc870\\ud569 \\uc774\\uc0ac\\uc7a5, \\uae40\\uc6a9\\uad6c \\uc804 \\uc911\\uc559\\ud68c \\ud68c\\uc7a5\\ub3c4 \\ucd9c\\ub9c8 \\uc758\\uc0ac\\ub97c \\ubc1d\\ud614\\ub2e4.\\uc9c0\\ub09c\\ud574 \\uc0c1\\ubc18\\uae30\\uae4c\\uc9c0\\ub9cc \\ud574\\ub3c4 \\uc911\\uc559\\ud68c \\uc8fc\\ubcc0\\uc5d0\\uc11c\\ub294 \\uc11c\\ubcd1\\ubb38 \\uc774\\uc0ac\\uc7a5\\uc774 \\uac00\\uc7a5 \\uc720\\ub825\\ud560 \\uac83\\uc73c\\ub85c \\uc804\\ub9dd\\ud588\\ub2e4.\\uc9c0\\uc9c0\\ubd80\\uc9c4\\ud558\\ub358 \\uc120\\uac70\\ud310\\uc774 \\uafc8\\ud2c0\\uac70\\ub9b0 \\uac83\\uc740 6\\uc6d4 \\uc911\\uc21c\\ubd80\\ud130\\uc600\\ub2e4. \\ubc15\\uc131\\ud0dd \\uc544\\uc2a4\\ucf58\\uacf5\\uc5c5\\ud611\\ub3d9\\uc870\\ud569\\uc5f0\\ud569\\ud68c\\uc7a5\\uc774 \\ucd9c\\ub9c8\\ud558\\uae30\\ub85c \\ud588\\ub2e4\\ub294 \\uc18c\\ubb38\\uc774 \\ub3cc\\uae30 \\uc2dc\\uc791\\ud55c \\uac83\\uc774\\ub2e4. \\uc5f0\\uc138\\ub300\\ub97c \\uc878\\uc5c5\\ud558\\uace0, LG\\uadf8\\ub8f9\\uc744 \\ub2e4\\ub2c8\\ub2e4 \\ucc3d\\uc5c5\\uc5d0 \\uc131\\uacf5\\ud55c \\uacbd\\ub825 \\ub4f1\\uc774 \\uc720\\uad8c\\uc790\\ub4e4\\uc758 \\uad00\\uc2ec\\uc744 \\ub04c\\uc5c8\\ub2e4. \\ud558\\uc9c0\\ub9cc \\uc5ec\\uc804\\ud788 \\uc7a0\\uc7ac\\ub825\\uc774 \\uc788\\ub2e4\\ub294 \\uc815\\ub3c4\\ub85c \\ud3c9\\uac00\\ubc1b\\ub294 \\ubd84\\uc704\\uae30\\uc600\\ub2e4.12\\uc6d4 \\ucd08\\ub85c \\uc811\\uc5b4\\ub4e4\\uba74\\uc11c \\ubc15\\uc131\\ud0dd \\ud68c\\uc7a5\\uc774 \\uc0dd\\uac01\\ubcf4\\ub2e4 \\uac15\\uc138\\ub97c \\ubcf4\\uc774\\uace0 \\uc788\\ub2e4\\ub294 \\uc18c\\ubb38\\uc774 \\ub3cc\\uc558\\ub2e4. \\ub2e4\\ud06c\\ud638\\uc2a4\\ub85c \\ub5a0\\uc624\\ub978 \\uac83\\uc774\\ub2e4. \\ud604 \\uc9d1\\ud589\\ubd80\\uc640 \\uc778\\uc5f0\\uc774 \\uc5c6\\ub2e4\\ub294 \\uc810\\uc744 \\ub0b4\\uc138\\uc6cc \\uc911\\uc559\\ud68c \\uac1c\\ud601\\uc744 \\ubc14\\ub77c\\ub294 \\uc720\\uad8c\\uc790\\ub4e4\\uc758 \\ub9c8\\uc74c\\uc744 \\uc6c0\\uc9c1\\uc600\\ub2e4. \\uc5ec\\uae30\\uc5d0 \\uc544\\uc2a4\\ucf58\\uacfc \\ub808\\ubbf8\\ucf58\\uc870\\ud569\\uc5d0 \\ud22c\\ud45c\\uad8c 32\\uac1c\\uac00 \\uc788\\ub2e4\\ub294 \\uc810\\ub3c4 \\ubd80\\uac01\\ub410\\ub2e4. \\uc0ac\\ub78c\\ub4e4\\uc740 \\uc810\\uc810 \\uc774\\ubcc0 \\uac00\\ub2a5\\uc131\\uc5d0 \\uc8fc\\ubaa9\\ud588\\ub2e4. \\uc11c\\ubcd1\\ubb38 \\uc774\\uc0ac\\uc7a5\\uacfc \\uc774\\uc7ac\\uad11 \\uc774\\uc0ac\\uc7a5\\uc5d0 \\ub300\\ud574 \\u201c\\ud655\\uc7a5\\uc131\\uc774 \\ub5a8\\uc5b4\\uc9c4\\ub2e4\\u201d\\ub294 \\ud3c9\\uac00\\uac00 \\ub098\\uc624\\uae30 \\uc2dc\\uc791\\ud55c \\uac83\\ub3c4 \\uc774\\ub54c\\ucbe4\\uc774\\ub2e4. \\uc9c0\\uc9c0\\uae30\\ubc18\\uc774 \\ud655\\ub300\\ub418\\uc9c0 \\uc54a\\uace0 \\uc788\\ub2e4\\ub294 \\uc598\\uae30\\uc600\\ub2e4.\\uc9c0\\ub09c 1\\uc6d4 \\uc911\\uc21c\\ubd80\\ud130 \\uc11c\\ubcd1\\ubb38\\u00b7\\ubc15\\uc131\\ud0dd\\u00b7\\uc774\\uc7ac\\uad11 \\ud6c4\\ubcf4 \\ub4f1 \\u20183\\ud30c\\uc804\\u2019 \\uad6c\\ub3c4\\uac00 \\ud615\\uc131\\ub410\\ub2e4. \\ubc15 \\ud68c\\uc7a5\\uc758 \\uc774\\ubcc0\\uc740 1\\uc6d429\\uc77c \\ucd94\\ucc9c \\uacb0\\uacfc\\uc5d0\\uc11c \\ucc98\\uc74c\\uc73c\\ub85c \\ubaa8\\uc2b5\\uc744 \\ub4dc\\ub7ec\\ub0c8\\ub2e4. \\uc720\\uad8c\\uc790 528\\uba85 \\uc911 10%\\uc758 \\ucd94\\ucc9c\\uc744 \\ubc1b\\uc544\\uc57c \\uc815\\uc2dd \\ud6c4\\ubcf4\\ub85c \\ub4f1\\ub85d\\ud560 \\uc218 \\uc788\\ub2e4. \\uc0ac\\uc2e4\\uc0c1\\uc758 \\uc608\\uc120\\uc5d0\\uc11c \\ubc15 \\ud68c\\uc7a5\\uc774 1\\uc704\\ub97c \\ud55c \\uac83\\uc774\\ub2e4. 27\\uc77c \\uc120\\uac70 \\ub2f9\\uc77c. \\uc624\\uc804 11\\uc2dc30\\ubd84 \\ud22c\\ud45c\\uac00 \\uc2e4\\uc2dc\\ub410\\ub2e4. 1\\ucc28 \\ud22c\\ud45c\\uc5d0\\uc11c \\uacfc\\ubc18\\uc218\\ub97c \\uc5bb\\uc740 \\ud6c4\\ubcf4\\uac00 \\uc5c6\\uc73c\\uba74 1, 2\\uc704 \\ud6c4\\ubcf4\\uac00 \\uacb0\\uc120\\ud22c\\ud45c\\ub97c \\uce58\\ub7ec\\uc57c \\ud588\\ub2e4. 518\\uba85\\uc774 \\ucc38\\uc5ec\\ud55c \\ud22c\\ud45c \\uacb0\\uacfc\\ub294 \\ubc15\\uc131\\ud0dd 154\\ud45c, \\uc774\\uc7ac\\uad11 130\\ud45c, \\uc11c\\ubcd1\\ubb38 112\\ud45c, \\ubc15\\uc8fc\\ubd09 65\\ud45c, \\uae40\\uc6a9\\uad6c 57\\ud45c \\ub4f1\\uc774\\uc5c8\\ub2e4. \\uc624\\ud6c4 2\\uc2dc \\uacb0\\uc120\\ud22c\\ud45c\\ub97c \\uc2e4\\uc2dc, \\ubc15\\uc131\\ud0dd \\ud6c4\\ubcf4\\uac00 25\\ub300 \\ud68c\\uc7a5\\uc73c\\ub85c \\ucd5c\\uc885 \\uacb0\\uc815\\ub410\\ub2e4.\\ndoc5: \\u201c\\uadfc\\ub85c\\uc2dc\\uac04 \\ub2e8\\ucd95, \\uc815\\ub144 \\uc5f0\\uc7a5, \\ud1b5\\uc0c1\\uc784\\uae08 \\ub4f1 \\ud604\\uc548\\uc774 \\uc0b0\\uc801\\ud55c\\ub370 \\ub208\\uc55e\\uc758 \\uc774\\uc775\\uc5d0 \\uc9d1\\ucc29\\ud574 \\u2018\\uc6b0\\ubb3c \\uc548 \\uac1c\\uad6c\\ub9ac\\u2019\\ub85c \\uc2dc\\uac04\\uc744 \\ud758\\ub824\\ubcf4\\ub0b4\\uba74 \\uc6b0\\ub9ac \\uacbd\\uc81c\\ub294 \\u2018\\uc1a5 \\uc548\\uc758 \\uac1c\\uad6c\\ub9ac\\u2019\\uac00 \\ub418\\uace0 \\ub9d0 \\uac83\\uc785\\ub2c8\\ub2e4.\\u201d \\uae40\\ub300\\ud658 \\ub178\\uc0ac\\uc815\\uc704\\uc6d0\\ud68c \\uc704\\uc6d0\\uc7a5\\uc774 29\\uc77c \\uc5f4\\ub9b0 \\ub178\\u00b7\\uc0ac\\u00b7\\uc815 \\ub300\\ud45c\\uc790 \\uac04\\ub2f4\\ud68c\\uc5d0\\uc11c \\ud55c \\ub9d0\\uc774\\ub2e4.\\ub178\\u00b7\\uc0ac\\u00b7\\uc815\\uc774 \\ubc18\\ub144\\uc5ec \\ub9cc\\uc5d0 \\ub178\\uc0ac\\uc815\\uc704 \\uc815\\uc0c1\\ud654\\uc5d0 \\ud569\\uc758\\ud55c \\uac83\\uc740 \\uadfc\\ub85c\\uc2dc\\uac04 \\ub2e8\\ucd95, \\ud1b5\\uc0c1\\uc784\\uae08 \\ud655\\ub300, \\uc815\\ub144 \\uc5f0\\uc7a5 \\ub4f1 \\ub178\\ub3d9\\ud604\\uc548\\uc774 \\uc0b0\\uc801\\ud55c \\uac00\\uc6b4\\ub370 \\ub354 \\uc774\\uc0c1 \\ubc29\\uce58\\ud560 \\uacbd\\uc6b0 \\uae40 \\uc704\\uc6d0\\uc7a5\\uc758 \\ub9d0\\ucc98\\ub7fc \\ubaa8\\ub450\\uc5d0\\uac8c \\ub4dd\\ub420 \\uac8c \\uc5c6\\ub2e4\\ub294 \\ud310\\ub2e8\\uc5d0 \\ub530\\ub978 \\uac83\\uc73c\\ub85c \\ubd84\\uc11d\\ub41c\\ub2e4.\\ub178\\u00b7\\uc0ac\\u00b7\\uc815 \\ub300\\ud45c\\uac00 \\ud55c\\uc790\\ub9ac\\uc5d0 \\ubaa8\\uc778 \\uac83\\uc740 \\uc9c0\\ub09c\\ud574 12\\uc6d4 \\ud55c\\uad6d\\ub178\\ucd1d\\uc774 \\uacbd\\ucc30\\uc758 \\ubbfc\\uc8fc\\ub178\\ucd1d \\uc0ac\\ubb34\\uc2e4\\uc5d0 \\ub300\\ud55c \\uacf5\\uad8c\\ub825 \\ud22c\\uc785\\uc5d0 \\ubc18\\ubc1c\\ud558\\uba70 \\ub178\\uc0ac\\uc815\\uc704\\ub97c \\ub5a0\\ub09c \\uc9c0 7\\uac1c\\uc6d4 \\ub9cc\\uc774\\ub2e4. \\uc774\\ub0a0 \\uac04\\ub2f4\\ud68c\\uc5d0\\ub294 \\uae40\\ub300\\ud658 \\uc704\\uc6d0\\uc7a5, \\ucd5c\\uacbd\\ud658 \\ubd80\\ucd1d\\ub9ac \\uacb8 \\uae30\\ud68d\\uc7ac\\uc815\\ubd80 \\uc7a5\\uad00, \\uc774\\uae30\\uad8c \\uace0\\uc6a9\\ub178\\ub3d9\\ubd80 \\uc7a5\\uad00, \\uc724\\uc0c1\\uc9c1 \\uc0b0\\uc5c5\\ud1b5\\uc0c1\\uc790\\uc6d0\\ubd80 \\uc7a5\\uad00, \\uae40\\ub3d9\\ub9cc \\ud55c\\uad6d\\ub178\\ucd1d \\uc704\\uc6d0\\uc7a5, \\uae40\\uc601\\ubc30 \\ud55c\\uad6d\\uacbd\\ucd1d \\ud68c\\uc7a5\\uc9c1\\ubb34\\ub300\\ud589, \\ubc15\\uc6a9\\ub9cc \\ub300\\ud55c\\uc0c1\\uc758 \\ud68c\\uc7a5\\uc774 \\ucc38\\uc11d\\ud588\\ub2e4. \\uac04\\ub2f4\\ud68c \\uc804\\ub0a0 \\uacf5\\uc2dd \\ubd88\\ucc38\\uc744 \\uc120\\uc5b8\\ud55c \\ubbfc\\uc8fc\\ub178\\ucd1d\\uc740 \\ucc38\\uc11d\\ud558\\uc9c0 \\uc54a\\uc558\\ub2e4.\\ub178\\uc0ac\\uc815\\uc704 \\uc815\\uc0c1\\ud654 \\ud569\\uc758 \\ub4a4\\uc5d0\\ub294 \\uc6b0\\uc120 \\ucde8\\uc784 \\ub54c\\ubd80\\ud130 \\ucd5c\\uc800\\uc784\\uae08 \\uc778\\uc0c1\\u00b7\\ube44\\uc815\\uaddc\\uc9c1 \\ucc98\\uc6b0 \\uac1c\\uc120 \\ub4f1\\uc758 \\u2018\\uce5c\\ub178\\ub3d9 \\uce74\\ub4dc\\u2019\\ub97c \\ube7c\\ub4e0 \\ucd5c\\uacbd\\ud658 \\ubd80\\ucd1d\\ub9ac\\uc640 \\uc774\\uae30\\uad8c \\uc7a5\\uad00\\uc758 \\uc5ed\\ud560\\uc774 \\ucef8\\ub2e4. \\uac04\\ub2f4\\ud68c\\ub97c \\ud558\\ub8e8 \\uc55e\\ub454 28\\uc77c \\uae30\\uc7ac\\ubd80\\uc5d0\\uc11c\\ub294 \\u201c\\uc815\\ubd80 \\ucd9c\\uc5f0\\uc5f0\\uad6c\\uae30\\uad00\\uc5d0\\uc11c \\uc77c\\ud558\\ub294 \\ube44\\uc815\\uaddc\\uc9c1 \\uc5f0\\uad6c\\uc6d0 400\\uba85\\uc774 \\ub0b4\\ub144 \\uc911 \\uc815\\uaddc\\uc9c1\\uc73c\\ub85c \\uc804\\ud658\\ub420 \\uac83\\u201d\\uc774\\ub77c\\ub294 \\uc598\\uae30\\uac00 \\ub098\\uc654\\ub2e4. \\uc774 \\uc7a5\\uad00\\uc740 \\uac19\\uc740 \\ub0a0 \\u201c\\uc591 \\ub178\\ucd1d\\uc740 \\ub300\\ud55c\\ubbfc\\uad6d \\uc804\\uccb4 \\uadfc\\ub85c\\uc790\\ub97c \\ub300\\ud45c\\ud558\\ub294 \\uadfc\\ub85c\\uc790\\uc704\\uc6d0\\uc73c\\ub85c \\ubc18\\ub4dc\\uc2dc \\ub300\\ud654\\uc5d0 \\ucc38\\uc11d\\ud574\\uc57c \\ud55c\\ub2e4\\u201d\\uba70 \\u201c\\uc7a5\\uad00 \\uc2a4\\uc2a4\\ub85c\\ub3c4 \\uc9c4\\uc815\\uc131\\uc744 \\uac16\\uace0 \\uc124\\ub4dd\\ud574 \\ub098\\uac08 \\uac83\\u201d\\uc774\\ub77c\\uace0 \\ubd84\\uc704\\uae30\\ub97c \\ub744\\uc6e0\\ub2e4. \\uc5ec\\uae30\\uc5d0 \\u2018\\uacf5\\uacf5\\ubd80\\ubb38 \\uc815\\uc0c1\\ud654\\u2019\\uc640 \\uad00\\ub828\\ud55c \\ub17c\\uc758\\uae30\\uad6c\\ub97c \\ub9cc\\ub4e4\\uc790\\ub294 \\ud55c\\uad6d\\ub178\\ucd1d\\uc758 \\uc81c\\uc548\\uc740 \\uc7a0\\uaca8 \\uc788\\ub358 \\ub178\\uc0ac\\uc815\\uc704\\uc758 \\ubb38\\uc744 \\uc5ec\\ub294 \\u2018\\uc5f4\\uc1e0\\u2019 \\uc5ed\\ud560\\uc744 \\ud588\\ub2e4. \\uadf8\\ub3d9\\uc548 \\ud55c\\uad6d\\ub178\\ucd1d\\uacfc \\ubbfc\\uc8fc\\ub178\\ucd1d\\uc740 \\ubc15\\uadfc\\ud61c \\uc815\\ubd80\\uc758 \\uacf5\\uacf5\\ubd80\\ubb38 \\uc815\\uc0c1\\ud654 \\uac1c\\ud601\\uc774 \\u201c\\uc815\\ubd80\\uc758 \\uc77c\\ubc29\\uc801\\uc778 \\uac00\\uc9dc \\uc815\\uc0c1\\ud654\\u201d\\ub77c\\uace0 \\ubc18\\ubc1c\\ud558\\uba70 \\ub300\\ud45c\\uc790\\ud68c\\uc758 \\uc18c\\uc9d1\\uc744 \\uc694\\uad6c\\ud574 \\uc654\\uc73c\\ub098 1\\uae30 \\ub0b4\\uac01\\uc758 \\ud604\\uc624\\uc11d \\ub2f9\\uc2dc \\ubd80\\ucd1d\\ub9ac\\ub294 \\u201c\\ub178\\ub3d9\\uacc4\\uc640 \\ud611\\uc758\\ud560 \\uc0ac\\uc548\\uc774 \\uc544\\ub2c8\\ub2e4\\u201d\\uba70 \\ubc88\\ubc88\\uc774 \\uac70\\uc808\\ud588\\ub2e4. \\ud55c\\uad6d\\ub178\\ucd1d\\uc758 \\uc81c\\uc548\\uc5d0 \\ub300\\ud574 \\ucd5c \\ubd80\\ucd1d\\ub9ac\\uac00 \\uae30\\uc874 \\uc785\\uc7a5\\uc744 \\uace0\\uc218\\ud588\\ub2e4\\uba74 \\uc774\\ub0a0 \\uac04\\ub2f4\\ud68c\\ub294 \\ub9d0 \\uadf8\\ub300\\ub85c \\uc0c1\\uacac\\ub840\\uc5d0 \\uadf8\\ucce4\\uc744 \\uacf5\\uc0b0\\uc774 \\ud06c\\ub2e4. \\uacf5\\uacf5\\ubd80\\ubb38 \\uc815\\uc0c1\\ud654\\ub97c \\ub458\\ub7ec\\uc2f8\\uace0 \\uc5c9\\ucf1c \\uc788\\ub358 \\ub178\\u00b7\\uc815 \\uac04 \\ub9e4\\ub4ed\\uc774 \\ud480\\ub9ac\\uba74\\uc11c \\ub178\\uc0ac\\uc815\\uc704 \\ucc38\\uc11d\\uc744 \\uac70\\ubd80\\ud574\\uc628 \\ubbfc\\uc8fc\\ub178\\ucd1d\\uc774 \\uc555\\ubc15\\uc744 \\ubc1b\\uc744 \\uac83\\uc73c\\ub85c \\ubcf4\\uc778\\ub2e4. \\ud558\\uc9c0\\ub9cc \\ub300\\ud654 \\uc7ac\\uac1c \\ubd84\\uc704\\uae30\\uc640\\ub294 \\ubcc4\\uac1c\\ub85c \\ub178\\ub3d9\\uacc4 \\ub0b4\\ubd80\\uc5d0 \\uac15\\uc131 \\ubaa9\\uc18c\\ub9ac\\ub3c4 \\ub9ce\\uc544 \\ub178\\uc0ac\\uc815\\uc704 \\uc815\\uc0c1\\ud654\\uac00 \\uc27d\\uc9c0 \\uc54a\\uc744 \\uac83\\uc774\\ub77c\\ub294 \\uc804\\ub9dd\\ub3c4 \\uc788\\ub2e4. \\uae40\\ub3d9\\ub9cc \\uc704\\uc6d0\\uc7a5\\ub3c4 \\uc774\\ub0a0 \\uac04\\ub2f4\\ud68c\\uc5d0\\uc11c \\ucd5c \\ubd80\\ucd1d\\ub9ac\\uc5d0\\uac8c \\u25b3\\uacbd\\uc81c\\uc815\\ucc45 \\ud328\\ub7ec\\ub2e4\\uc784 \\uc804\\ud658 \\u25b3\\ucd5c\\uc800\\uc784\\uae08 \\uc778\\uc0c1 \\ubc0f \\uc81c\\ub3c4\\uac1c\\uc120 \\u25b3\\uc138\\uc81c\\uac1c\\ud3b8 \\ub4f1\\uc758 \\uc815\\ucc45\\uc744 \\uac15\\ud558\\uac8c \\uc694\\uad6c\\ud588\\ub2e4.\\ub610 \\uc591\\ub300 \\ub178\\ucd1d \\uacf5\\uacf5\\ubd80\\ubb38 \\ub178\\ub3d9\\uc870\\ud569 \\uacf5\\ub3d9\\ub300\\ucc45\\uc704\\uc6d0\\ud68c(\\uacf5\\ub300\\uc704)\\uc758 \\uc6c0\\uc9c1\\uc784\\ub3c4 \\ubcc0\\uc218\\ub2e4. \\uacf5\\ub300\\uc704\\ub294 \\uc774\\ub0a0 \\ub9cc\\ub0a8\\uacfc\\ub294 \\uc0c1\\uad00\\uc5c6\\uc774 \\ub0b4\\ub2ec 27\\uc77c\\ubd80\\ud130 9\\uc6d43\\uc77c\\uae4c\\uc9c0 \\uc608\\uc815\\ub41c \\ucd1d\\ud30c\\uc5c5\\uc744 \\uc9c4\\ud589\\ud55c\\ub2e4\\ub294 \\ubc29\\uce68\\uc774\\ub2e4. \\ucd1d\\ud30c\\uc5c5\\uc774 \\ud604\\uc2e4\\ud654\\ub418\\uba74 \\ud798\\ub4e4\\uac8c \\uc870\\uc131\\ub41c \\ub178\\u00b7\\uc0ac\\u00b7\\uc815 \\ub300\\ud654 \\ubd84\\uc704\\uae30\\uac00 \\uc6d0\\uc810\\uc73c\\ub85c \\ub418\\ub3cc\\uc544\\uac08 \\uac00\\ub2a5\\uc131\\ub3c4 \\uc788\\ub2e4.\\n\\n\\uc704\\uc758 docs \\uc911\\uc5d0\\uc11c\\ub9cc \\uc815\\ubcf4\\ub97c \\uadfc\\uac70\\ub85c \\ud558\\uc5ec, \\uc9c8\\ubb38\\uc5d0 \\ub2f5\\ubcc0\\ud574 \\uc8fc\\uc138\\uc694.\\n\\ub2f5\\ubcc0\\uc5d0\\uc11c \\uc778\\uc6a9\\ud55c \\ubb38\\uc11c\\uc758 \\ub0b4\\uc6a9\\uc5d0\\ub294 \\ubc18\\ub4dc\\uc2dc [[doc1]], [[doc2]], ... \\ud615\\uc2dd\\uc73c\\ub85c \\uc778\\uc6a9 \\ud45c\\uc2dc\\ub97c \\ud574\\uc8fc\\uc138\\uc694.\\n\\ucd94\\ub860\\uc774\\ub098 \\uc9c0\\uc5b4\\ub0b4\\ub294 \\ub2f5\\ubcc0\\uc740 \\uc0bc\\uac00\\uc8fc\\uc2dc\\uace0, docs\\uc5d0 \\uba85\\uc2dc\\uc801\\uc73c\\ub85c \\ub098\\ud0c0\\ub09c \\ub0b4\\uc6a9\\ub9cc \\uc778\\uc6a9\\ud574 \\uc8fc\\uc138\\uc694.\",\n          \"\\uc9c8\\ubb38: \\ud398\\ub974\\ub514\\ub09c\\ub3c4 2\\uc138\\uc758 \\uc804 \\ubd80\\uc778\\uc774 \\ub0b3\\uc740 \\uc790\\uc2dd\\uc758 \\uc774\\ub984\\uc740?\\n\\ndocs:\\ndoc1: 1751\\ub144 \\ub098\\ud3f4\\ub9ac\\uc5d0\\uc11c \\uc2a4\\ud398\\uc778\\uc758 \\uce74\\ub97c\\ub85c\\uc2a4 3\\uc138 \\uad6d\\uc655(\\ub098\\ud3f4\\ub9ac\\uc758 \\uce74\\ub97c\\ub85c 7\\uc138, \\uc2dc\\uce60\\ub9ac\\uc544\\uc758 \\uce74\\ub97c\\ub85c 5\\uc138)\\uc758 \\uc544\\ub4e4\\ub85c \\ud0dc\\uc5b4\\ub0ac\\uc73c\\uba70 \\uc2a4\\ud398\\uc778\\uc758 \\uce74\\ub97c\\ub85c\\uc2a4 4\\uc138 \\uad6d\\uc655\\uc758 \\ub3d9\\uc0dd\\uc774\\uae30\\ub3c4 \\ud558\\ub2e4.\\n\\n1759\\ub144 \\ubd80\\uc655 \\uce74\\ub97c\\ub85c\\uc2a4 3\\uc138\\uac00 \\uc2a4\\ud398\\uc778 \\uc655\\uc704\\uc5d0 \\uc624\\ub974\\uc790 \\uc18c\\ub144\\uc73c\\ub85c \\ub098\\ud3f4\\ub9ac \\uc655\\uc704\\uc5d0 \\uc62c\\ub77c \\ud398\\ub974\\ub514\\ub09c\\ub3c4 4\\uc138\\ub77c \\ubd88\\ub838\\ub2e4. \\ub2f9\\uc2dc \\uadf8\\uc758 \\uc12d\\uc815\\uc774\\uc5c8\\ub358 \\ubca0\\ub974\\ub098\\ub974\\ub3c4 \\ud0c0\\ub204\\uce58\\ub294 \\uc790\\uc2e0\\uc774 \\uad6d\\uc815\\uc744 \\uc88c\\uc9c0\\uc6b0\\uc9c0\\ud558\\uae30 \\uc704\\ud574 \\uc758\\ub3c4\\uc801\\uc73c\\ub85c \\uc5b4\\ub9b0 \\uc655\\uc774 \\uc81c\\ub300\\ub85c \\ub41c \\uc81c\\uc655\\ud559 \\uad50\\uc721\\uc744 \\ubc1b\\uc9c0 \\ubabb\\ud558\\uac8c \\ud588\\uace0, \\ud398\\ub974\\ub514\\ub09c\\ub3c4\\ub294 \\uc0ac\\ub0e5\\uacfc \\ub09a\\uc2dc\\ub97c \\uc990\\uae30\\ub294 \\uc2a4\\ud3ec\\uce20\\ub9e8\\uc73c\\ub85c \\uc790\\ub77c\\ub09c \\ubc18\\uba74 \\uc815\\uce58\\uc5d0\\ub294 \\ubb34\\uad00\\uc2ec\\ud558\\uac8c \\ub418\\uc5c8\\ub2e4. 1767\\ub144 \\uc131\\ub144\\uc774 \\ub41c \\ud398\\ub974\\ub514\\ub09c\\ub3c4\\ub294 1768\\ub144 \\ub9c8\\ub9ac\\uc544 \\ud14c\\ub808\\uc9c0\\uc544\\uc758 \\ub538\\uc774\\uc790 \\uc624\\uc2a4\\ud2b8\\ub9ac\\uc544 \\uc5ec\\ub300\\uacf5 \\ub9c8\\ub9ac\\uc544 \\uce74\\ub864\\ub9ac\\ub098\\uc640 \\uacb0\\ud63c\\ud558\\uc600\\ub2e4. \\ub9c8\\ub9ac\\uc544 \\uce74\\ub864\\ub9ac\\ub098\\ub294 \\ud398\\ub974\\ub514\\ub09c\\ub3c4\\uc640\\ub294 \\ub2ec\\ub9ac \\ucd5c\\uace0\\uc758 \\uad50\\uc721\\uc744 \\ubc1b\\uc740 \\ub370\\ub2e4 \\uc815\\uce58\\uc801 \\uc57c\\uc2ec\\ub3c4 \\uac15\\ud588\\ub2e4. \\uadf8\\ub140\\ub294 \\ud0c0\\ub204\\uce58\\ub97c \\ud574\\uc784\\ud558\\uace0 \\uc601\\uad6d\\uc778 \\uc874 \\uc561\\ud134\\uc744 \\uadf8 \\uc790\\ub9ac\\uc5d0 \\uc549\\ud614\\ub2e4. \\ud398\\ub974\\ub514\\ub09c\\ub3c4\\ub294 \\uc815\\uce58\\uc5d0 \\uad00\\ud55c \\ubd80\\ubd84\\uc740 \\uc544\\ub0b4\\uc5d0\\uac8c \\uc77c\\uc784\\ud55c \\ucc44 \\uc810\\ucc28 \\uc790\\uc2e0\\uc774 \\uc88b\\uc544\\ud558\\ub294 \\ucde8\\ubbf8 \\uc0dd\\ud65c\\uc5d0\\ub9cc \\uc5f4\\uc911\\ud558\\uac8c \\ub418\\uc5c8\\ub2e4.\\n\\n\\uc774\\ud6c4 1793\\ub144 \\uc624\\uc2a4\\ud2b8\\ub9ac\\uc544, \\uc601\\uad6d\\uacfc \\ub3d9\\ub9f9\\uc744 \\ub9fa\\uc5b4 \\ubc18\\ud601\\uba85 \\uc138\\ub825\\uc5d0 \\uac00\\ub2f4\\ud588\\uace0 \\ud638\\ub808\\uc774\\uc1fc \\ub12c\\uc2a8 \\uc81c\\ub3c5\\uc758 \\uc601\\uad6d \\ud568\\ub300\\uac00 \\ub098\\ud3f4\\ub9ac\\uc5d0 \\ub3c4\\ucc29\\ud558\\uc790 \\uc6a9\\uae30\\ub97c \\uc5bb\\uace0 1798\\ub144 \\ud504\\ub791\\uc2a4\\uc758 \\uc9c0\\uc6d0\\uc744 \\ubc1b\\ub294 \\ub85c\\ub9c8 \\uacf5\\ud654\\uad6d\\uc744 \\uacf5\\uaca9\\ud588\\ub2e4. \\uadf8\\ub7ec\\ub098 1798\\ub144 12\\uc6d4 21\\uc77c \\ud504\\ub791\\uc2a4\\uac00 \\ub098\\ud3f4\\ub9ac\\ub97c \\uce68\\uacf5\\ud574 \\ud30c\\ub974\\ud14c\\ub178\\ud14c \\uacf5\\ud654\\uad6d\\uc744 \\uc120\\ud3ec\\ud558\\uc790 \\ud398\\ub974\\ub514\\ub09c\\ub3c4\\ub294 \\uc2dc\\uce60\\ub9ac\\uc544\\ub85c \\ub3c4\\ub9dd\\ucce4\\ub2e4. 1799\\ub144 \\uacf5\\ud654\\uad6d\\uc774 \\uc804\\ubcf5\\ub418\\uc790 \\ub2e4\\uc2dc \\ub098\\ud3f4\\ub9ac\\ub85c \\ub3cc\\uc544\\uc640 \\uc61b \\uacf5\\ud654\\uad6d \\uc9c0\\uc9c0\\uc790\\ub4e4\\uc744 \\ucc98\\ud615\\ud588\\ub2e4.\\n\\n\\uadf8\\ub7ec\\ub098 1806\\ub144 \\ub098\\ud3f4\\ub808\\uc639\\uad70\\uc774 \\ub2e4\\uc2dc \\ub098\\ud3f4\\ub9ac\\ub97c \\uc810\\ub839\\ud558\\uc790 \\ub2e4\\uc2dc \\ud314\\ub808\\ub974\\ubaa8\\ub85c \\ud53c\\ub09c\\ud588\\ub2e4. \\ub098\\ud3f4\\ub808\\uc639\\uc740 \\uc790\\uc2e0\\uc758 \\ud615 \\uc870\\uc81c\\ud504 \\ubcf4\\ub098\\ud30c\\ub974\\ud2b8\\ub97c \\ub098\\ud3f4\\ub9ac\\uc758 \\uc655\\uc73c\\ub85c \\uc549\\ud614\\uc9c0\\ub9cc \\ud398\\ub974\\ub514\\ub09c\\ub3c4\\ub294 \\uc601\\uad6d\\uc758 \\ubcf4\\ud638 \\ud558\\uc5d0 \\uc784\\uc2dc\\ub85c \\uc2dc\\uce60\\ub9ac\\uc544\\ub97c \\ub2e4\\uc2a4\\ub838\\ub2e4. \\ub098\\ud3f4\\ub808\\uc639\\uc774 \\ubab0\\ub77d\\ud558\\uc790 \\uc870\\uc81c\\ud504\\uc758 \\ub4a4\\ub97c \\uc774\\uc5b4 \\uc989\\uc704\\ud588\\ub358 \\uc870\\uc544\\ud0a4\\ub178 1\\uc138 \\ub610\\ud55c \\ud1f4\\uc704\\ud588\\uace0 \\ud398\\ub974\\ub514\\ub09c\\ub3c4\\ub294 \\ube48 \\ud68c\\uc758\\uc5d0\\uc11c \\uc815\\ud574\\uc9c4 \\ub300\\ub85c \\ub098\\ud3f4\\ub9ac\\ub85c \\ub3cc\\uc544\\uc640 1816\\ub144 12\\uc6d4 1\\uc77c \\uc591\\uc2dc\\uce60\\ub9ac\\uc544 \\uc655\\uad6d\\uc758 \\uad6d\\uc655 \\ud398\\ub974\\ub514\\ub09c\\ub3c4 1\\uc138\\ub85c \\uc989\\uc704\\ud558\\uc600\\ub2e4. 1820\\ub144 \\ud5cc\\ubc95\\uc744 \\ubd80\\uc5ec\\ud588\\uace0 1821\\ub144\\ubd80\\ud130 \\uc758\\ud68c\\uc5d0 \\ucc38\\uc11d\\ud574 \\uc624\\uc2a4\\ud2b8\\ub9ac\\uc544\\uc5d0 \\uc6d0\\uc870\\ub97c \\uc694\\uccad\\ud588\\ub2e4.\\ndoc2: \\ub9c8\\ub9ac\\uc544\\ub098\\ub294 \\ube44\\ub108\\ub178\\uc774\\uc288\\ud0c0\\ud2b8\\uc5d0\\uc11c \\uc2e0\\uc131 \\ub85c\\ub9c8 \\uc81c\\uad6d\\uc758 \\ud669\\uc81c \\ud398\\ub974\\ub514\\ub09c\\ud2b8 3\\uc138\\uc640 \\uc2a4\\ud398\\uc778\\uc758 \\ub9c8\\ub9ac\\uc544 \\uc548\\ub098\\uc758 \\ud070\\ub538\\ub85c \\ud0dc\\uc5b4\\ub0ac\\ub2e4. \\ubcf8\\ub798 \\ub9c8\\ub9ac\\uc544\\ub098\\ub294 \\uc0ac\\ucd0c(\\ud3a0\\ub9ac\\ud398 4\\uc138\\uc758 \\uc544\\ub4e4) \\ubc1c\\ud0c0\\uc0ac\\ub974 \\uce74\\ub97c\\ub85c\\uc2a4\\uc640 \\uacb0\\ud63c\\ud560 \\uc608\\uc815\\uc774\\uc5c8\\uc9c0\\ub9cc \\uadf8\\ub294 1646\\ub144 \\uc694\\uc808\\ud588\\ub2e4. \\ud3a0\\ub9ac\\ud398 4\\uc138\\uc5d0\\uac8c\\ub294 \\uc8fd\\uc740 \\ubc1c\\ud0c0\\uc0ac\\ub974 \\uce74\\ub97c\\ub85c\\uc2a4 \\uc678\\uc5d4 \\uc544\\ub4e4\\uc774 \\uc5c6\\uc5c8\\uae30 \\ub54c\\ubb38\\uc5d0(\\ucc28\\ub0a8 \\ud504\\ub780\\uc2dc\\uc2a4\\ucf54 \\ud398\\ub974\\ub09c\\ub3c4\\ub294 \\ud0dc\\uc5b4\\ub09c \\ud574\\uc5d0 \\uc8fd\\uc5c8\\ub2e4) \\ub2e4\\ub978 \\ub0a8\\uc790 \\ud6c4\\uacc4\\uc790\\uac00 \\ud544\\uc694\\ud588\\ub2e4. \\uadf8\\ub7ec\\ub098 2\\ub144 \\uc804 \\uc774\\ubbf8 \\uc655\\ube44 \\ud504\\ub791\\uc2a4\\uc758 \\uc774\\uc0ac\\ubca8\\uc744 \\uc783\\uc740 \\ud3a0\\ub9ac\\ud398 4\\uc138\\ub294 1649\\ub144 \\uc870\\uce74\\ub538\\uc778 \\ub9c8\\ub9ac\\uc544\\ub098\\ub97c \\ub450 \\ubc88\\uc9f8 \\uc655\\ube44\\ub85c \\ub9de\\uc558\\ub2e4. \\ub9c8\\ub9ac\\uc544\\ub098\\ub294 \\ub2e4\\uc12f \\uba85\\uc758 \\uc790\\uc2dd\\uc744 \\ub0b3\\uc558\\uc9c0\\ub9cc \\uadfc\\uce5c\\ud63c\\uc758 \\uc601\\ud5a5\\uc73c\\ub85c \\uadf8 \\uc911 \\uc14b\\uc774 \\uc694\\uc808\\ud588\\uace0, \\uc7a5\\ub140 \\ub9c8\\ub974\\uac00\\ub9ac\\ud0c0 \\ud14c\\ub808\\uc0ac\\uc640 \\ub9c9\\ub0b4\\uc544\\ub4e4 \\uce74\\ub97c\\ub85c\\uc2a4\\ub9cc\\uc774 \\uc0b4\\uc544\\ub0a8\\uc558\\ub2e4. \\uadf8\\ub7ec\\ub098 \\uce74\\ub97c\\ub85c\\uc2a4 2\\uc138\\ub294 \\ubc1c\\ub2ec\\uc774 \\ub2a6\\uace0 \\ubab9\\uc2dc \\ud5c8\\uc57d\\ud588\\uc73c\\uba70 \\uc678\\uc219\\ubd80 \\ub808\\uc624\\ud3f4\\ud2b8 1\\uc138\\uc5d0\\uac8c \\uc2dc\\uc9d1\\uac04 \\ub9c8\\ub974\\uac00\\ub9ac\\ud0c0 \\ud14c\\ub808\\uc0ac \\ub610\\ud55c \\uc694\\uc808\\ud588\\ub2e4.\\n\\n\\ub0a8\\ud3b8 \\ud3a0\\ub9ac\\ud398 4\\uc138\\uac00 \\uc8fd\\uc740 \\ub4a4 \\uc544\\uc9c1 \\uc5b4\\ub838\\ub358 \\uce74\\ub97c\\ub85c\\uc2a4\\uac00 \\uc989\\uc704\\ud558\\uba74\\uc11c \\ub9c8\\ub9ac\\uc544\\ub098\\ub294 \\uc12d\\uc815\\uc774 \\ub418\\uc5c8\\ub2e4. 1678\\ub144 \\ub0a8\\ud3b8\\uc758 \\uc11c\\uc790 \\ud6c4\\uc548 \\ud638\\uc138 \\ub370 \\uc544\\uc6b0\\uc2a4\\ud2b8\\ub9ac\\uc544\\uc5d0 \\uc758\\ud574 \\ud55c \\ucc28\\ub840 \\uad81\\uc815\\uc5d0\\uc11c \\ucad3\\uaca8\\ub0ac\\uc9c0\\ub9cc \\uc774\\ub4ec\\ud574 \\uadf8\\uc758 \\uc8fd\\uc74c\\uc73c\\ub85c \\ub2e4\\uc2dc \\uad81\\uc815\\uc5d0 \\ubcf5\\uadc0\\ud574, \\uc8fd\\uc744 \\ub54c\\uae4c\\uc9c0 \\uc2a4\\ud398\\uc778 \\uad81\\uc815\\uc5d0 \\uc601\\ud5a5\\ub825\\uc744 \\ud589\\uc0ac\\ud588\\ub2e4. \\uadf8\\ub140\\ub294 1696\\ub144 \\ub9c8\\ub4dc\\ub9ac\\ub4dc\\uc5d0\\uc11c \\uc720\\ubc29\\uc554\\uc73c\\ub85c \\uc8fd\\uc5c8\\ub2e4.\\ndoc3: 1642\\ub144, \\ud1a0\\uc2a4\\uce74\\ub098 \\ub300\\uacf5 \\ud398\\ub974\\ub514\\ub09c\\ub3c4 2\\uc138 \\ub370 \\uba54\\ub514\\uce58\\uc640 \\ub300\\uacf5\\ube44 \\ube44\\ud1a0\\ub9ac\\uc544 \\ub378\\ub77c \\ub85c\\ubca0\\ub808 \\uc0ac\\uc774\\uc5d0\\uc11c \\uc678\\uc544\\ub4e4\\ub85c \\ud0dc\\uc5b4\\ub0ac\\ub2e4. \\uadf8\\uc758 \\uc704\\ub85c \\ub450 \\ud615\\uc774 \\uc788\\uc5c8\\uc73c\\ub098 \\ubaa8\\ub450 \\uc77c\\ucc0d \\uc8fd\\uc5c8\\ub2e4. \\uadf8\\uc758 \\uad50\\uc721 \\ubb38\\uc81c\\ub97c \\ub450\\uace0 \\ub300\\uacf5 \\ubd80\\ubd80\\ub294 \\uc0dd\\uac01\\uc774 \\ub2ec\\ub790\\uace0, \\uacb0\\uad6d\\uc740 \\ub300\\uacf5\\ube44\\uc758 \\ub73b\\ub300\\ub85c \\uc2e0\\ud559\\uc790 \\ubc18\\ub514\\ub12c\\ub9ac\\uac00 \\ucf54\\uc2dc\\ubaa8\\ub97c \\uac00\\ub974\\uce58\\uac8c \\ub418\\uc5c8\\ub2e4. \\uc774\\ub7ec\\ud55c \\uad50\\uc721\\uc758 \\uc601\\ud5a5\\uc73c\\ub85c \\ucf54\\uc2dc\\ubaa8\\ub294 \\ub300\\ub2e8\\ud788 \\ub3c5\\uc2e4\\ud55c \\uc2e0\\uc790\\uc600\\uc73c\\uba70 \\uc885\\uad50 \\ubb38\\uc81c\\uc5d0\\ub294 \\uc801\\uadf9\\uc801\\uc774\\uc5c8\\ub358 \\ubc18\\uba74, \\uc815\\uce58 \\ubb38\\uc81c\\uc5d0\\ub294 \\ubcc4 \\uad00\\uc2ec\\uc744 \\ud45c\\ud558\\uc9c0 \\uc54a\\uc558\\ub2e4. \\uadf8\\ub294 \\ud558\\ub8e8\\uc5d0 5,6\\uad70\\ub370\\uc758 \\uad50\\ud68c\\ub97c \\ub2e4\\ub154\\uace0 \\uc720\\ub300\\uad50\\ub3c4\\uac00 \\uac00\\ud1a8\\ub9ad\\uad50\\ub3c4\\uc640 \\uacb0\\ud63c\\ud558\\uac70\\ub098 \\ud55c \\uc9d1\\uc5d0 \\uc0b4\\uac70\\ub098 \\uac00\\ud1a8\\ub9ad\\uad50\\ub3c4 \\uc720\\ubaa8\\ub97c \\uc4f0\\ub294 \\uac83\\uc744 \\uae08\\ud558\\ub294 \\ub4f1 \\uc5c4\\uaca9\\ud55c \\ubc18\\uc720\\ub300\\uc8fc\\uc758 \\ubc95\\ub839\\ub4e4\\uc744 \\uc81c\\uc815\\ud588\\ub2e4. \\ub610\\ud55c \\uc608\\uc220\\uacfc \\ud559\\ubb38\\uc5d0 \\ub300\\ud55c \\uc9c0\\uc6d0\\uc744 \\uc544\\ub07c\\uc9c0 \\uc54a\\uc558\\ub358 \\uba54\\ub514\\uce58 \\uac00\\uc758 \\uc5ed\\ub300 \\uad70\\uc8fc\\ub4e4\\uacfc\\ub294 \\ub2ec\\ub9ac \\ucf54\\uc2dc\\ubaa8 3\\uc138\\ub294 \\ubb38\\uc608\\uc5d0 \\ub300\\ud55c \\ud765\\ubbf8\\ub3c4 \\uc5c6\\uc5c8\\ub2e4. \\uadf8\\ub294 \\ub300\\uc2dd\\uac00\\ub85c \\ud3ed\\uc74c\\ud3ed\\uc2dd\\uc744 \\uc990\\uacbc\\uc73c\\uba70, \\uc5b8\\uc81c\\ub098 \\ube44\\uc6a9\\uc5d0 \\uad6c\\uc560\\ubc1b\\uc9c0 \\uc54a\\ub294 \\ud638\\ud654\\ub85c\\uc6b4 \\uc5f0\\ud68c\\ub97c \\uc990\\uacbc\\ub2e4. \\uadf8 \\ube44\\uc6a9\\uc744 \\ub300\\uae30 \\uc704\\ud574 \\ucf54\\uc2dc\\ubaa8 3\\uc138\\ub294 \\ubc31\\uc131\\ub4e4\\uc5d0\\uac8c \\ub192\\uc740 \\uc138\\uae08\\uc744 \\ubd80\\uacfc\\ud588\\uc73c\\uba70, \\uc774\\ub85c \\uc778\\ud574 \\uadf8\\uc758 \\uce58\\uc138 \\ub3d9\\uc548 \\ud1a0\\uc2a4\\uce74\\ub098 \\ub300\\uacf5\\uad6d\\uc740 \\uc810\\ucc28 \\uc1e0\\ub77d\\uc758 \\uae38\\uc744 \\uac77\\uae30 \\uc2dc\\uc791\\ud588\\ub2e4. \\ucf54\\uc2dc\\ubaa8 3\\uc138\\ub294 \\uc7a5\\uc218\\ud588\\uc9c0\\ub9cc \\ub298 \\uc790\\uc2e0\\uc774 \\uc5b8\\uc81c \\uc8fd\\uc744\\uc9c0 \\ubab0\\ub77c \\ubd88\\uc548\\ud574\\ud588\\uace0 \\ud6c4\\uacc4\\uc790 \\ubb38\\uc81c\\uc5d0 \\uc9d1\\ucc29\\ud588\\ub2e4 \\uadf8\\uc758 \\uc138 \\uc790\\ub140\\uc5d0\\uac8c\\uc11c \\ud6c4\\uacc4\\uc790\\ub97c \\uc5bb\\ub294 \\uac83\\uc774 \\uc694\\uc6d0\\ud55c \\uc77c\\uc774\\ub780 \\uac78 \\uc54c\\uc790 \\ucf54\\uc2dc\\ubaa8 3\\uc138\\ub294 40\\ub300 \\ud6c4\\ubc18\\uc774\\uc5c8\\ub358 \\ub0a8\\ub3d9\\uc0dd \\ud504\\ub780\\uccb4\\uc2a4\\ucf54 \\ub9c8\\ub9ac\\uc544 \\uc8fc\\uad50\\uc5d0\\uac8c \\uc0c1\\uc18d\\uc790\\ub97c \\ub0b3\\uc544\\uc904 \\uac83\\uc744 \\ubd80\\ud0c1\\ud588\\ub2e4 \\ud504\\ub780\\uccb4\\uc2a4\\ucf54 \\ub9c8\\ub9ac\\uc544\\ub294 \\uacfc\\uc2a4\\ud154\\ub77c \\uacf5\\uc791\\uc758 \\ub538\\uacfc \\uacb0\\ud63c\\ud588\\uc9c0\\ub9cc \\uc0c1\\uc18d\\uc790\\ub97c \\uc5bb\\uc9c0 \\ubabb\\ud558\\uace0 \\uc8fd\\uc5c8\\ub2e4. \\ucf54\\uc2dc\\ubaa8 3\\uc138\\ub294 \\ub538 \\uc548\\ub098 \\ub9c8\\ub9ac\\uc544\\uac00 \\uacf5\\uad6d\\uc744 \\uacc4\\uc2b9\\ud560 \\uc218 \\uc788\\uac8c \\ub9cc\\ub4e4\\ub824\\uace0 \\ub178\\ub825\\ud588\\uc9c0\\ub9cc \\uc774\\ub8e8\\uc5b4\\uc9c0\\uc9c0 \\uc54a\\uc558\\uace0, \\uacb0\\uad6d \\uadf8\\uc758 \\uc544\\ub4e4 \\ub300\\uc5d0\\uc11c \\uba54\\ub514\\uce58 \\uac00\\uc758 \\ud1a0\\uc2a4\\uce74\\ub098 \\ub300\\uacf5\\uad6d\\uc758 \\uc5ed\\uc0ac\\ub294 \\ub05d\\ub0ac\\ub2e4.\\ndoc4: \\ud14c\\uc174 \\uacf5\\uc791 \\uce74\\ub97c\\uacfc \\ub098\\uc0ac\\uc6b0\\ubc14\\uc77c\\ubd80\\ub974\\ud06c \\uacf5\\ub140 \\ud5e8\\ub9ac\\uc5d0\\ud0c0 \\uc0ac\\uc774\\uc5d0\\uc11c \\ud0dc\\uc5b4\\ub09c \\uc7a5\\ub140\\ub85c 1837\\ub144 1\\uc6d4 27\\uc77c \\uc591\\uc2dc\\uce60\\ub9ac\\uc544\\uc758 \\uc655 \\ud398\\ub974\\ub514\\ub09c\\ub3c4 2\\uc138\\uc640 \\uacb0\\ud63c\\ud588\\ub2e4. \\uadf8\\uc5d0\\uac8c\\ub294 \\uc774\\ubbf8 \\uc804\\ucc98 \\ub9c8\\ub9ac\\uc544 \\ud06c\\ub9ac\\uc2a4\\ud2f0\\ub098 \\ub514 \\uc0ac\\ubcf4\\uc774\\uc544 \\uc655\\ub140\\uac00 \\ub0b3\\uc740 \\uc544\\ub4e4 \\ud504\\ub780\\uccb4\\uc2a4\\ucf54\\uac00 \\uc788\\uc5c8\\ub2e4. \\ud504\\ub780\\uccb4\\uc2a4\\ucf54\\uc640\\uc758 \\uc0ac\\uc774\\ub294 \\uc591\\ud638\\ud574\\uc11c, \\uc774 \\uc758\\ubd93\\uc544\\ub4e4\\uc740 \\uadf8\\ub140\\ub97c \\uc874\\uacbd\\ud588\\uc73c\\uba70 \\uce5c\\uc5b4\\uba38\\ub2c8\\ucc98\\ub7fc \\ub530\\ub790\\ub2e4\\uace0 \\ud55c\\ub2e4. \\ub9c8\\ub9ac\\uc544\\ub294 \\uad81\\uc815 \\uc0dd\\ud65c\\uc774\\ub098 \\uc655\\ube44\\ub85c\\uc11c\\uc758 \\ucc45\\ubb34\\uc5d0\\ub294 \\ubcc4 \\uad00\\uc2ec\\uc774 \\uc5c6\\uc5c8\\uace0 \\uc790\\uc2e0\\uc758 \\ubc29\\uc5d0\\uc11c \\ubc14\\ub290\\uc9c8\\uc744 \\ud558\\uac70\\ub098 \\uc544\\uc774\\ub4e4\\uc744 \\ub3cc\\ubcf4\\ub294 \\uac83\\uc744 \\ub354 \\uc88b\\uc544\\ud588\\uc9c0\\ub9cc \\uc815\\uce58\\uc5d0\\ub294 \\uad00\\uc2ec\\uc774 \\uc788\\uc5c8\\ub2e4. \\uadf8\\ub140\\ub294 \\uc885\\uc885 \\ub0a8\\ud3b8\\uc758 \\uc815\\uce58\\uc801 \\uc870\\uc5b8\\uc790 \\uc5ed\\ud560\\uc744 \\ud588\\uace0 \\ub54c\\ub85c\\ub294 \\ubb38 \\ub108\\uba38\\ub85c \\ud68c\\uc758 \\ub0b4\\uc6a9\\uc744 \\uc5ff\\ub4e3\\uae30\\ub3c4 \\ud588\\ub2e4.\\n\\n\\ub0a8\\ud3b8\\uc774 \\uc8fd\\uace0 \\uc758\\ubd93\\uc544\\ub4e4 \\ud504\\ub780\\uccb4\\uc2a4\\ucf54\\uac00 \\uad6d\\uc655\\uc73c\\ub85c \\uc989\\uc704\\ud558\\uc790 \\uadf8\\ub140\\ub294 \\ub354\\uc6b1 \\uc801\\uadf9\\uc801\\uc73c\\ub85c \\uc815\\uce58\\uc5d0 \\uac1c\\uc785\\ud558\\uac8c \\ub418\\uc5c8\\ub294\\ub370 \\ud504\\ub780\\uccb4\\uc2a4\\ucf54\\uc758 \\uc655\\ube44 \\ub9c8\\ub9ac\\uc544 \\uc18c\\ud53c\\uc544\\ub294 \\uc774\\uac83\\uc744 \\uc88b\\uac8c \\uc0dd\\uac01\\ud558\\uc9c0 \\uc54a\\uc544 \\uace0\\ubd80\\uac04\\uc758 \\uac08\\ub4f1\\uc744 \\ube5a\\uc5c8\\ub2e4. \\uc774\\ud6c4 \\uac00\\ub9ac\\ubc1c\\ub514\\uac00 \\uc774\\ub044\\ub294 \\ubd89\\uc740 \\uc154\\uce20 \\ubd80\\ub300\\uc758 \\uc6d0\\uc815\\uc73c\\ub85c \\uc655\\uad6d\\uc774 \\ubd95\\uad34\\ud558\\uae30 \\uc2dc\\uc791\\ud558\\uc790 \\ub9c8\\ub9ac\\uc544 \\ud14c\\ub808\\uc0ac\\ub294 \\uc544\\uc774\\ub4e4\\uacfc \\ud568\\uaed8 \\ub098\\ud3f4\\ub9ac\\ub97c \\ub5a0\\ub098 \\uc678\\uad6d\\uc73c\\ub85c \\ub3c4\\ud53c\\ud588\\ub2e4. \\uac00\\uc5d0\\ud0c0\\uc640 \\ub85c\\ub9c8\\ub97c \\uac70\\uccd0 \\uad50\\ud669 \\ube44\\uc624 9\\uc138\\uc758 \\ubcf4\\ud638\\ub85c \\ud034\\ub9ac\\ub0a0\\ub808 \\uad81\\uc804\\uc5d0 \\uc815\\ucc29\\ud558\\uc600\\uace0 \\uc774\\ud6c4 \\ud504\\ub780\\uccb4\\uc2a4\\ucf54 \\ubd80\\ubd80\\uc640\\ub3c4 \\uc7ac\\ud68c\\ud558\\uc600\\ub2e4. 1867\\ub144 \\ud504\\ub780\\uccb4\\uc2a4\\ucf54\\uac00 \\uc9c0\\ucf1c\\ubcf4\\ub294 \\uac00\\uc6b4\\ub370 \\ud2f0\\ud478\\uc2a4\\ub85c \\uc0ac\\ub9dd\\ud588\\ub2e4.\\ndoc5: \\uae30\\uc6d0\\uc804 142\\ub144 \\ud4cc\\uc2a4\\ucf58\\uc740 \\ud074\\ub808\\uc624\\ud30c\\ud2b8\\ub77c 2\\uc138\\ub97c \\ubc84\\ub9ac\\uace0 \\ud074\\ub808\\uc624\\ud30c\\ud2b8\\ub77c 3\\uc138\\uc640 \\uacb0\\ud63c\\ud588\\ub2e4. \\uc774\\uac83\\uc774 \\uacc4\\uae30\\uac00 \\ub418\\uc5b4, \\uae30\\uc6d0\\uc804 131\\ub144 \\ud074\\ub808\\uc624\\ud30c\\ud2b8\\ub77c 2\\uc138\\uc640 \\ub0b4\\uc804\\uc5d0 \\ud729\\uc2f8\\uc600\\ub2e4. \\uae30\\uc6d0\\uc804 127\\ub144\\uc5d0\\ub294 \\ud074\\ub808\\uc624\\ud30c\\ud2b8\\ub77c 2\\uc138\\ub97c \\uc2dc\\ub9ac\\uc544\\ub85c \\uc7a0\\uc2dc \\ubab0\\uc544\\ub0b4\\uc5c8\\uc73c\\ub098, \\uae30\\uc6d0\\uc804 124\\ub144\\uc5d0 \\ud074\\ub808\\uc624\\ud30c\\ud2b8\\ub77c 2\\uc138\\uc640 \\ud654\\ud574\\ub97c \\ud558\\uc5ec 3\\uba85(\\ud4cc\\uc2a4\\ucf58, \\ud074\\ub808\\uc624\\ud30c\\ud2b8\\ub77c 2\\uc138, \\ud074\\ub808\\uc624\\ud30c\\ud2b8\\ub77c 3\\uc138)\\uc758 \\uc9c0\\ubc30 \\uccb4\\uc81c\\ub85c \\ub3cc\\uc544\\uc654\\ub2e4.\\n\\n\\uae30\\uc6d0\\uc804 116\\ub144 \\ud4cc\\uc2a4\\ucf58\\uc740 \\uc720\\uc5b8\\uc5d0\\uc11c, \\ud074\\ub808\\uc624\\ud30c\\ud2b8\\ub77c 3\\uc138\\uac00 \\uc9c0\\uba85\\ud558\\ub294 \\uc0ac\\ub78c\\uc744 \\ud6c4\\uacc4\\uc790\\ub85c \\ud55c\\ub2e4\\uace0 \\ud558\\uc600\\uae30 \\ub54c\\ubb38\\uc5d0 \\uadf8\\ub140\\ub294 \\uc790\\uc2e0\\uc774 \\ub9c8\\uc74c\\uc5d0 \\ub4e4\\uc5b4\\ud558\\ub294 \\uc544\\ub4e4\\uc778 \\uc54c\\ub809\\uc0b0\\ub4dc\\ub85c\\uc2a4\\uc758 \\uc639\\ub9bd\\uc744 \\uaf80\\ud588\\ub2e4. \\uadf8\\ub7ec\\ub098 \\uc54c\\ub809\\uc0b0\\ub4dc\\ub9ac\\uc544\\uc758 \\uc2dc\\ubbfc\\uc740 \\uc7a5\\uc790\\uc778 \\ub77c\\ud280\\ub85c\\uc2a4\\ub97c \\uc6d0\\ud558\\uc600\\uc73c\\ubbc0\\ub85c \\uc5b4\\uca54 \\uc218 \\uc5c6\\uc774 \\ub77c\\ud280\\ub85c\\uc2a4\\ub97c \\uacf5\\ub3d9\\uc758 \\ud30c\\ub77c\\uc624\\ub85c \\uc784\\uba85\\ud588\\ub2e4. \\uadf8\\ub798\\ub3c4 \\uc12d\\uc815\\uc73c\\ub85c\\uc11c \\uc2e4\\uad8c\\uc744 \\uc7a5\\uc545\\ud55c \\uadf8\\ub140\\ub294, \\ub208\\uc5d0 \\uac70\\uc2ac\\ub9ac\\ub294 \\ub77c\\ud280\\ub85c\\uc2a4\\uc758 \\uc544\\ub0b4 \\ud074\\ub808\\uc624\\ud30c\\ud2b8\\ub77c 4\\uc138\\ub97c \\ucd94\\ubc29\\ud558\\uc600\\ub2e4. \\ub610\\ud55c \\uc790\\uc2dd\\ub4e4\\uacfc \\ubd88\\ud654\\uac00 \\uacc4\\uc18d\\ub418\\ub294 \\uac00\\uc6b4\\ub370 \\uae30\\uc6d0\\uc804 110\\ub144\\uc5d0\\ub294 \\uc54c\\ub809\\uc0b0\\ub4dc\\ub85c\\uc2a4\\ub97c \\uc639\\ub9bd\\ud558\\uc600\\ub2e4\\uac00 \\uadf8 \\ub2e4\\uc74c \\ud574\\uc5d0 \\ub77c\\ud280\\ub85c\\uc2a4\\ub97c \\ubcf5\\uadc0 \\uc2dc\\ud0a4\\ub294 \\ub4f1 \\ud63c\\ub780\\uc744 \\ubd88\\ub7ec\\uc654\\ub2e4.\\n\\n\\uae30\\uc6d0\\uc804 107\\ub144 \\uadf8\\ub140\\ub294 \\ub610 \\ub2e4\\uc2dc \\uc54c\\ub809\\uc0b0\\ub4dc\\ub85c\\uc2a4\\ub97c \\uc639\\ub9bd\\ud558\\uc600\\ub2e4. \\uadf8\\ub7ec\\ub098 \\uae30\\uc6d0\\uc804 101\\ub144\\uc5d0 \\uc5b4\\uba38\\ub2c8\\uc758 \\uc815\\uce58\\uac1c\\uc785\\uc5d0 \\uc2e0\\ubb3c\\uc774 \\ub09c \\uc54c\\ub809\\uc0b0\\ub4dc\\ub85c\\uc2a4\\ub294 \\uadf8\\ub140\\ub97c \\uc0b4\\ud574\\ud558\\uace0 \\ud615\\uc778 \\ub77c\\ud280\\ub85c\\uc2a4\\uc640 \\ud654\\ud574\\ud55c\\ub2e4.\\n\\n\\uc704\\uc758 docs \\uc911\\uc5d0\\uc11c\\ub9cc \\uc815\\ubcf4\\ub97c \\uadfc\\uac70\\ub85c \\ud558\\uc5ec, \\uc9c8\\ubb38\\uc5d0 \\ub2f5\\ubcc0\\ud574 \\uc8fc\\uc138\\uc694.\\n\\ub2f5\\ubcc0\\uc5d0\\uc11c \\uc778\\uc6a9\\ud55c \\ubb38\\uc11c\\uc758 \\ub0b4\\uc6a9\\uc5d0\\ub294 \\ubc18\\ub4dc\\uc2dc [[doc1]], [[doc2]], ... \\ud615\\uc2dd\\uc73c\\ub85c \\uc778\\uc6a9 \\ud45c\\uc2dc\\ub97c \\ud574\\uc8fc\\uc138\\uc694.\\n\\ucd94\\ub860\\uc774\\ub098 \\uc9c0\\uc5b4\\ub0b4\\ub294 \\ub2f5\\ubcc0\\uc740 \\uc0bc\\uac00\\uc8fc\\uc2dc\\uace0, docs\\uc5d0 \\uba85\\uc2dc\\uc801\\uc73c\\ub85c \\ub098\\ud0c0\\ub09c \\ub0b4\\uc6a9\\ub9cc \\uc778\\uc6a9\\ud574 \\uc8fc\\uc138\\uc694.\",\n          \"\\uc9c8\\ubb38: \\uc544\\uc11c\\uc2a4 \\ud328\\uc2a4 \\ud0c0\\uc6b4\\uc758 \\uc5ed\\uc0ac\\uc640 \\uc751\\uae09\\uc758\\ub8cc\\uccb4\\uacc4 \\uac1c\\ud3b8\\uc774 \\uc9c0\\uc5ed \\uc0ac\\ud68c\\uc5d0 \\ubbf8\\uce5c \\uc601\\ud5a5\\uc740 \\ubb34\\uc5c7\\uc778\\uac00\\uc694?\\n\\ndocs:\\ndoc1: \\uc774 \\ud0c0\\uc6b4\\uacfc \\uace0\\uac1c\\uc758 \\uc774\\ub984\\uc740 \\uc544\\uc11c \\ub354\\ub4e4\\ub9ac \\ub3d5\\uc2a8 \\uacbd(1841-1934)\\uc5d0\\uac8c\\uc11c \\ub530\\uc628 \\uac83\\uc774\\ub2e4. \\uc544\\uc11c \\ub3d5\\uc2a8\\uc740 \\uadf8\\ub294 \\uc11c\\ud574\\uc548\\uacfc \\ubd84\\uc218\\ub839\\uc744 \\uc774\\ub8e8\\uace0 \\uc788\\ub294 \\uc640\\uc774\\ub9c8\\uce74\\ub9ac\\ub9ac \\ubc16\\uc758 \\uc4f8\\ub9cc\\ud55c \\uace0\\uac1c\\uac00 \\uc788\\ub294 \\uc9c0 \\uc54c\\uc544\\ubcf4\\uae30 \\uc704\\ud574 \\ucd5c\\uace0 \\uce21\\ub7c9\\uc0ac \\ud1a0\\ub9c8\\uc2a4 \\uce74\\uc2a4\\uc5d0 \\uc758\\ud574 \\uc77c\\ud558\\uac8c \\ub418\\uc5c8\\ub2e4. 1864\\ub144 \\uadf8\\uc758 \\ud615\\uc81c\\uc778 \\uc5d0\\ub4dc\\uc6cc\\ub4dc\\uac00 \\uadf8\\uc640 \\ud569\\ub958\\ub97c \\ud558\\uc5ec \\uc544\\ud2f0\\ub77c \\uac15\\uc758 \\uacc4\\uace1\\uc73c\\ub85c \\ub3d9\\ubc18\\ud558\\uac8c \\ub41c\\ub2e4. \\uc544\\uc11c\\ub294 \\ub9c8\\uc624\\ub9ac\\uc871 \\uc0ac\\ub0e5\\uafbc\\ub4e4\\uc774 \\uc0ac\\ub0e5\\uc744 \\ud558\\ub294 \\uace0\\uac1c \\uc874\\uc7ac\\uc5d0 \\ub300\\ud55c \\uc815\\ubcf4\\ub97c \\uc11c\\ud574\\uc548\\uc758 \\ub9c8\\uc624\\ub9ac\\uc871 \\ucd94\\uc7a5 \\ub77c\\ud0c0\\ud478\\ud788\\uc5d0\\uac8c \\ub4e4\\uc5c8\\ub2e4. \\uc544\\uc11c\\uac00 \\ud06c\\ub77c\\uc774\\uc2a4\\ud2b8\\ucc98\\uce58\\ub85c \\ub3cc\\uc544\\uc654\\uc744 \\ub54c, \\uadf8\\ub294 \\uadf8\\uac00 \\ud6a1\\ub2e8\\ud55c \\uc9c0\\uc5ed\\uc758 \\uc2a4\\ucf00\\uce58\\ub97c \\ud588\\uace0, \\ub9ac\\ud3ec\\ud2b8\\uc640 \\ud568\\uaed8 \\uce74\\uc2a4\\uc5d0\\uac8c \\uc81c\\ucd9c\\ud588\\ub2e4. \\uc544\\uc11c \\ub3d5\\uc2a8\\uc740 \\uadf8 \\uace0\\uac1c\\uc758 \\uc774\\ub984\\uc744 \\uc9d3\\uc9c0 \\uc54a\\uc558\\uc73c\\uba70, \\uc11c\\ucabd\\uc73c\\ub85c \\ub9e4\\uc6b0 \\uac00\\ud30c\\ub974\\ub2e4\\ub294 \\uac83\\uc744 \\uc54c\\uac8c \\ub418\\uc5c8\\ub2e4.\\n\\n\\uace8\\ub4dc\\ub7ec\\uc2dc\\uac00 \\uc2dc\\uc791\\ub418\\uc790, \\uc0ac\\uc5c5\\uc704\\uc6d0\\ud68c\\ub294 \\uce94\\ud130\\ubca0\\ub9ac\\uc5d0\\uc11c \\uc11c\\ud574\\uc548\\uc73c\\ub85c \\uac00\\ub294 \\ub354 \\ub098\\uc740 \\uace0\\uac1c\\ub97c \\ubc1c\\uacac\\ud558\\ub294 \\uc0ac\\ub78c\\uc5d0\\uac8c\\ub294 200\\ud30c\\uc6b4\\ub4dc\\uc758 \\uc0c1\\uae08\\uc744 \\uc8fc\\uaca0\\ub2e4\\uace0 \\uc81c\\uc548\\ud588\\ub2e4. \\ub3d9\\uc2dc\\uc5d0 \\uc544\\uc11c\\uc758 \\uc544\\ubc84\\uc9c0\\uc778 \\uc5d0\\ub4dc\\uc6cc\\ub4dc \\ub3d5\\uc2a8\\uc774 \\ud0c0\\ub77c\\ub9c8\\uce74\\uc6b0, \\uc640\\uc774\\ub9c8\\uce74\\ub9ac\\ub9ac, \\uadf8\\ub9ac\\uace0 \\ud6c4\\ub8e8\\ub204\\uc774 \\ub4f1\\uc758 \\uace0\\uac1c\\ub97c \\uc870\\uc0ac\\ud558\\uae30 \\uc704\\ud574 \\ud30c\\uacac\\ub418\\uc5c8\\ub2e4. \\uadf8\\ub9ac\\uace0 \\ubaa8\\ub4e0 \\uacc4\\uace1\\uc758 \\uba38\\ub9ac \\ubd80\\ubd84\\uc5d0 \\uc788\\ub294 \\uacc4\\uace1\\uc744 \\uc870\\uc0ac\\ud55c \\ub4a4\\uc5d0 \\uc544\\uc11c\\uc2a4 \\ud328\\uc2a4\\uac00 \\ub2f9\\uc2dc\\ub85c\\ub294 \\uac00\\uc7a5 \\uc9c1\\uc811 \\ud6a1\\ub2e8\\uc5d0 \\uc801\\ud569\\ud558\\ub2e4\\uace0 \\ubcf4\\uace0\\ub97c \\ud588\\ub2e4.\\n\\n\\uc774 \\ud0c0\\uc6b4\\uc740 \\uc6d0\\ub798 \\uc624\\ud2f0\\ub77c \\ud130\\ub110\\uc744 \\uc9d3\\uae30 \\uc704\\ud55c \\uae30\\uc9c0\\ub85c \\uac74\\uc124\\ub418\\uc5c8\\uc73c\\uba70, 1908\\ub144 1\\uc6d4 14\\uc77c \\uc2dc\\uc791\\ub418\\uc5c8\\ub2e4. \\uc544\\uc11c\\uc2a4 \\ud328\\uc2a4 \\ud0c0\\uc6b4\\uc5d0 \\ucca0\\ub3c4\\uac00 \\uc5f0\\uacb0\\ub41c \\uac83\\uc740 1914\\ub144\\uc774\\uc5c8\\uace0, \\uc6e8\\uc2a4\\ud2b8\\ub79c\\ub4dc \\ubd80\\ubd84\\uc774 \\uc624\\ud2f0\\ub77c\\ubcf4\\ub2e4\\ub294 \\ube60\\ub974\\uac8c \\uc791\\uc5c5\\ub418\\uc5c8\\ub2e4. \\ud130\\ub110\\uc758 \\uac74\\uc124\\uc740 \\ub9e4\\uc6b0 \\ub290\\ub838\\ub2e4. \\uc774 \\ud130\\ub110\\uc774 \\uc644\\uc131\\ub41c \\uac83\\uc740 1923\\ub144\\uc758 \\uc77c\\uc774\\uc5c8\\ub2e4.\\n\\n\\ub370\\ube14\\uc2a4 \\ud380\\uce58\\ubcfc \\ud3ed\\ud3ec \\uc544\\ub798\\uc5d0 \\ud130\\ub110 \\uacf5\\uc0ac\\uc640 \\ud0c0\\uc6b4\\uc758 \\uc804\\uae30\\ub97c \\uc81c\\uacf5\\ud558\\uae30 \\uc704\\ud574 \\ubc1c\\uc804\\uc18c\\uac00 \\uc9c0\\uc5b4\\uc84c\\ub2e4.\\n\\n1929\\ub144\\uc5d0\\ub294 \\uc544\\uc11c\\uc2a4 \\ud328\\uc2a4 \\uad6d\\ub9bd\\uacf5\\uc6d0\\uc774 \\ub9cc\\ub4e4\\uc5b4\\uc84c\\ub2e4.\\ndoc2: \\ub0b4\\ub144\\ubd80\\ud130 \\uc911\\uc99d\\ud658\\uc790\\ub4e4\\uc758 \\uc751\\uae09\\uc9c4\\ub8cc\\ube44\\uac00 \\ub0ae\\uc544\\uc9c4\\ub2e4. \\uacbd\\uc99d\\ud658\\uc790\\ub4e4\\uc740 \\uc751\\uae09\\uc2e4\\uc5d0 6\\uc2dc\\uac04 \\uc774\\uc0c1 \\uba38\\ubb3c\\ub7ec\\ub3c4 \\uc9c4\\ub8cc\\ube44\\ub97c \\ud560\\uc778\\ubc1b\\uc744 \\uc218 \\uc5c6\\ub2e4. \\uc804\\uad6d \\uc5b4\\ub514\\uc11c\\ub4e0 \\ud55c \\uc2dc\\uac04 \\uc548\\uc5d0 \\uc751\\uae09\\uce58\\ub8cc\\ub97c \\ubc1b\\uc744 \\uc218 \\uc788\\ub3c4\\ub85d \\uad8c\\uc5ed\\uc751\\uae09\\uc13c\\ud130\\ub3c4 21\\uacf3 \\ub298\\uc5b4\\ub09c\\ub2e4.\\ubcf4\\uac74\\ubcf5\\uc9c0\\ubd80\\ub294 10\\uc77c \\uc774 \\uac19\\uc740 \\ub0b4\\uc6a9\\uc758 \\u2018\\uc911\\uc99d\\uc751\\uae09\\ud658\\uc790 \\uc0dd\\uc874\\uc728 \\ud5a5\\uc0c1\\uc744 \\uc704\\ud55c \\uc751\\uae09\\uc758\\ub8cc\\uccb4\\uacc4 \\uac1c\\ud3b8\\ubc29\\uc548\\u2019\\uc744 \\ubc1c\\ud45c\\ud588\\ub2e4. \\uc784\\ud638\\uadfc \\ubcf5\\uc9c0\\ubd80 \\uc751\\uae09\\uc758\\ub8cc\\uacfc\\uc7a5\\uc740 \\u201c\\uc911\\uc99d\\uc751\\uae09\\ud658\\uc790\\uac00 \\ubcd1\\uc2e4\\uc774 \\uc5c6\\uc5b4 \\uc5ec\\ub7ec \\ubcd1\\uc6d0\\uc744 \\uc804\\uc804\\ud558\\uba70 \\uc2dc\\uac04\\uc744 \\ud5c8\\ube44\\ud558\\ub294 \\uc77c\\uc774 \\uc5c6\\ub3c4\\ub85d \\uc751\\uae09\\uc758\\ub8cc\\uccb4\\uacc4\\ub97c \\uc911\\uc99d\\ud658\\uc790 \\uc911\\uc2ec\\uc73c\\ub85c \\uac1c\\ud3b8\\ud560 \\uacc4\\ud68d\\u201d\\uc774\\ub77c\\uace0 \\ub9d0\\ud588\\ub2e4.\\uc911\\uc99d\\ud658\\uc790\\ub294 \\uc751\\uae09\\uc2e4 \\uccb4\\ub958\\uc2dc\\uac04, \\uc785\\uc6d0 \\uc5ec\\ubd80\\uc640 \\uc0c1\\uad00\\uc5c6\\uc774 \\uc751\\uae09\\uc9c4\\ub8cc\\ube44 \\ubcf8\\uc778\\ubd80\\ub2f4\\ub960 20%\\ub97c \\uc801\\uc6a9\\ubc1b\\uac8c \\ub41c\\ub2e4. \\ud604\\uc7ac\\ub294 \\uc751\\uae09\\uc2e4\\uc744 \\uac70\\uccd0 \\ud574\\ub2f9 \\ubcd1\\uc6d0\\uc5d0 \\uc785\\uc6d0\\ud55c \\ud658\\uc790\\ub098 6\\uc2dc\\uac04 \\uc774\\uc0c1 \\uc751\\uae09\\uc2e4\\uc5d0 \\uba38\\ubb34\\ub978 \\uc911\\uc99d\\ud658\\uc790\\ub9cc \\uc751\\uae09\\uc9c4\\ub8cc\\ube44 \\ubcf8\\uc778\\ubd80\\ub2f4\\ub960 20%\\uac00 \\uc801\\uc6a9\\ub41c\\ub2e4. \\uce58\\ub8cc\\ub9cc \\ubc1b\\uace0 \\ud1f4\\uc6d0\\ud558\\uac70\\ub098 \\ub2e4\\ub978 \\ubcd1\\uc6d0\\uc73c\\ub85c \\uc774\\uc1a1\\ub418\\uba74 \\uc911\\uc99d\\ud658\\uc790\\ub77c\\ub3c4 \\uc751\\uae09\\uc9c4\\ub8cc\\ube44\\uc758 50~60%(\\uc0c1\\uae09\\uc885\\ud569\\ubcd1\\uc6d0 60%, \\uc77c\\ubc18\\uc885\\ud569\\ubcd1\\uc6d0 50%)\\ub97c \\ubd80\\ub2f4\\ud574\\uc57c \\ud55c\\ub2e4.\\uacbd\\uc99d\\ud658\\uc790\\ub294 \\uc77c\\uad04\\uc801\\uc73c\\ub85c \\uc751\\uae09\\uc9c4\\ub8cc\\ube44 \\ubcf8\\uc778\\ubd80\\ub2f4\\ub960 50~60%\\uac00 \\uc801\\uc6a9\\ub41c\\ub2e4. \\ud604\\uc7ac\\ub294 \\uacbd\\uc99d\\ud658\\uc790\\ub77c\\ub3c4 6\\uc2dc\\uac04 \\uc774\\uc0c1 \\uc751\\uae09\\uc2e4\\uc5d0 \\uba38\\ubb34\\ub974\\uba74 \\uc751\\uae09\\uc9c4\\ub8cc\\ube44 \\ubcf8\\uc778\\ubd80\\ub2f4\\ub960\\uc744 20%\\ub85c \\ud560\\uc778\\ud574\\uc900\\ub2e4. \\uc774 \\ub54c\\ubb38\\uc5d0 \\uacbd\\uc99d\\ud658\\uc790\\ub4e4\\uc774 \\uc751\\uae09\\uc2e4\\uc5d0 \\ubd88\\ud544\\uc694\\ud558\\uac8c \\uc7a5\\uc2dc\\uac04 \\uba38\\ubb34\\ub294 \\uc77c\\uc774 \\ub9ce\\uc558\\ub2e4. \\uc784 \\uacfc\\uc7a5\\uc740 \\u201c\\uc751\\uae09\\uc2e4 \\uacfc\\ubc00\\ud654\\ub97c \\uac00\\uc911\\uc2dc\\ud0a4\\ub294 \\ubd80\\uc791\\uc6a9\\uc744 \\ubc29\\uc9c0\\ud558\\uae30 \\uc704\\ud55c \\ubaa9\\uc801\\u201d\\uc774\\ub77c\\uace0 \\uc124\\uba85\\ud588\\ub2e4.\\ndoc3: \\uc0ac\\uace0 \\ubc1c\\uc0dd \\uc9c1\\ud6c4 70\\uc5ec\\uba85\\uc758 \\uc5ed\\ubb34\\uc6d0\\ub4e4\\uacfc \\ub2e4\\ub978 \\uc0ac\\uac74\\uc5d0 \\ucd9c\\ub3d9\\uc911\\uc774\\ub358 \\uacbd\\ucc30\\uad00 30\\uc5ec\\uba85\\uc774 \\uc989\\uc2dc \\uc0ac\\uace0 \\ud604\\uc7a5\\uc5d0 \\ud22c\\uc785\\ub418\\uc5b4 \\uc0ac\\ub9dd\\uc790 \\ubc0f \\ubd80\\uc0c1\\uc790\\ub4e4\\uc744 \\uc778\\uadfc \\ubcd1\\uc6d0\\uc73c\\ub85c \\ubd84\\uc0b0 \\uc774\\uc1a1\\ud558\\ub3c4\\ub85d \\uc815\\ub9ac\\ud558\\uc600\\uace0, \\uae40\\uc77c\\ud658 \\uad50\\ud1b5\\ubd80\\uc7a5\\uad00 \\ub4f1 \\uad50\\ud1b5\\ubd80 \\uad00\\ub8cc\\uac00 \\ucd1d\\ucd9c\\ub3d9\\ud558\\uc5ec \\uc0ac\\uac74 \\ud604\\uc7a5\\uc758 \\uc218\\uc2b5\\ucc45\\uc744 \\uc9c0\\ud718\\ud558\\uc600\\ub2e4. \\uc11c\\uc6b8\\uc5ed\\uc5d0\\ub294 \\uace7\\ubc14\\ub85c \\uc0ac\\uace0\\ub300\\ucc45\\ubcf8\\ubd80\\uac00 \\uc124\\uce58\\ub418\\uc5c8\\uace0 \\uc0ac\\ub9dd\\uc790\\ub4e4\\uc758 \\ud569\\ub3d9\\uc704\\ub839\\uc81c\\ub97c \\uc0ac\\uace0 \\ub2e4\\uc74c\\ub0a0\\uc778 27\\uc77c \\uc624\\ud6c4 3\\uc2dc \\uad50\\ud1b5\\ubd80 \\uc55e \\uad11\\uc7a5\\uc5d0\\uc11c \\uce58\\ub974\\uace0 \\uc0ac\\ub9dd\\uc790\\ub4e4\\uc758 \\uc2dc\\uc2e0\\uc744 \\uc720\\uac00\\uc871\\uc5d0\\uac8c \\uc778\\uacc4\\ud558\\uace0 \\uc704\\uc790\\ub8cc\\ub97c \\uc9c0\\ubd88\\ud558\\uae30\\ub85c \\uacb0\\uc815\\ud558\\uc600\\ub2e4.\\n\\n\\uc774 \\uc0ac\\uace0\\ub85c \\uc778\\ud558\\uc5ec \\ubbf8\\ucc98 \\ud0d1\\uc2b9\\ud558\\uc9c0 \\ubabb\\ud55c 2,300\\uba85\\uc758 \\uc2b9\\uac1d\\ub4e4\\uc740 \\ub2e4\\uc74c\\ub0a0 \\uc624\\uc804 0\\uc2dc 2\\ubd84\\uacbd \\ucd9c\\ubc1c\\ud558\\ub294 \\uc784\\uc2dc \\uc5f4\\ucc28\\uc5d0 \\ud0d1\\uc2b9\\ud558\\uc600\\uc73c\\uba70 \\ub098\\uba38\\uc9c0 \\uc2b9\\uac1d\\ub4e4\\uc740 \\uc774\\uc5b4\\uc11c \\uc0c8\\ubcbd 2\\uc2dc 3\\ubd84\\uacbd \\ucd9c\\ubc1c\\ud558\\ub294 \\uc784\\uc2dc \\uc5f4\\ucc28\\uc5d0 \\ud0d1\\uc2b9\\ud558\\uc600\\ub2e4. \\n\\n\\uc0ac\\uace0 \\ub2e4\\uc74c\\ub0a0\\uc778 1\\uc6d4 28\\uc77c \\uc624\\uc804 \\uc11c\\uc6b8\\uc9c0\\uac80\\uc740 \\uc11c\\uc6b8\\uc5ed\\uc5d0\\uc11c \\uc77c\\uc5b4\\ub09c \\uc555\\uc0ac \\uc0ac\\uace0\\uc640 \\uad00\\ub828\\ud558\\uc5ec \\uc11c\\uc6b8\\uc5ed \\uad6c\\ub0b4\\uc758 \\uc0ac\\uac74 \\ud604\\uc7a5\\uc744 \\ub2f5\\uc0ac\\ud558\\uc600\\uace0 \\uc11c\\uc6b8\\uc5ed\\uc7a5\\uacfc \\uc11c\\uc6b8\\uc5ed \\uc5ec\\uac1d\\uc8fc\\uc784 \\ub450 \\uba85\\uc744 \\uc5c5\\ubb34\\uc0c1\\uacfc\\uc2e4\\uce58\\uc0ac \\ud610\\uc758\\ub85c \\uc6b0\\uc120 \\uc785\\uac74\\ud558\\uc600\\ub2e4. \\uac19\\uc740 \\ub0a0 \\uc624\\ud6c4 \\uad6d\\ud68c \\uad50\\ud1b5\\uccb4\\uc2e0\\uc704\\uc6d0\\ud68c\\ub294 \\uc784\\uc2dc\\uae34\\uae09\\uc704\\uc6d0\\ud68c\\ub97c \\uc5f4\\uace0 \\ucd5c\\uc778\\uaddc \\ub2f9\\uc2dc \\ub0b4\\ubb34\\ubd80 \\uc7a5\\uad00\\uc744 \\ucd08\\uce58\\ud558\\uc5ec \\uc0ac\\uac74\\uc758 \\uc804\\ub9d0\\uacfc \\uc0ac\\uac74\\uacfc \\uad00\\ub828\\ub41c \\uacbd\\ucc30 \\uc218\\uc0ac\\uc5d0 \\ub300\\ud574 \\uc11c\\uba74\\ubcf4\\uace0\\ub97c \\ud558\\uace0 \\uad50\\ud1b5\\ubd80 \\ub0b4\\ubd80\\uc758 \\uc778\\uc0ac\\uc870\\uce58\\ub97c \\ub2e8\\ud589\\ud560 \\uac83\\uc744 \\uc694\\uad6c\\ud558\\uc600\\ub2e4. \\n\\n2\\uc6d4 2\\uc77c \\uc11c\\uc6b8\\uc911\\uc559\\uc9c0\\ubc29\\ubc95\\uc6d0\\uc740 2\\uc6d4 1\\uc77c \\uc11c\\uc6b8\\uc9c0\\uac80\\uc774 \\uc11c\\uc6b8\\uc5ed\\uc7a5\\uacfc \\uc11c\\uc6b8\\uc5ed \\uc5ec\\uac1d\\uc8fc\\uc784\\uc5d0 \\ub300\\ud574 \\uc5c5\\ubb34\\uc0c1\\uacfc\\uc2e4\\uce58\\uc0ac\\uc0c1 \\ud610\\uc758\\ub85c \\uc2e0\\uccad\\ud55c \\uad6c\\uc18d\\uc601\\uc7a5\\uc744 \\uc9d1\\ud589\\ud558\\uc600\\uc73c\\uba70 \\uc774\\ub0a0 \\ubc24 11\\uc2dc \\uc11c\\uc6b8\\uc11c\\ub300\\ubb38\\uacbd\\ucc30\\uc11c\\uc5d0 \\uc218\\uac10\\ub418\\uc5c8\\ub2e4. \\n\\n\\uc774 \\uc0ac\\uac74\\uc744 \\uacc4\\uae30\\ub85c \\uc784\\ud765\\uc21c \\uc11c\\uc6b8\\ud2b9\\ubcc4\\uc2dc\\uc7a5\\uc740 \\uc608\\uc815\\uc5d0 \\uc5c6\\ub358 \\ub3d9\\ub300\\ubb38\\uc6b4\\ub3d9\\uc7a5\\uacfc \\ud6a8\\ucc3d\\uc6b4\\ub3d9\\uc7a5, \\uc7a5\\ucda9\\uccb4\\uc721\\uad00\\uc758 \\uac1c\\uc870 \\uacc4\\ud68d\\uc744 \\ubc1c\\ud45c\\ud558\\uc600\\uc73c\\uba70, \\ucca0\\ub3c4\\uccad \\ub2f9\\uad6d\\uc740 2\\uc6d4 10\\uc77c\\ubd80\\ud130 \\uc6b4\\ud589\\ub418\\ub294 \\ud638\\ub0a8\\uc120 \\uc5f4\\ucc28\\uc758 \\uc99d\\ud3b8\\uc744 \\uc2dc\\ud589\\ud55c\\ub2e4\\uace0 \\ubc1d\\ud614\\ub2e4. \\n\\n\\uc774\\ud6c4 4\\uc6d4 1\\uc77c \\uce58\\ub7ec\\uc9c4 \\uacf5\\ud310\\uc5d0\\uc11c \\uc11c\\uc6b8\\uc5ed\\uc7a5\\uc5d0\\uac8c \\ubb34\\uc8c4, \\uc11c\\uc6b8\\uc5ed \\uc5ec\\uac1d\\uc8fc\\uc784\\uc5d0 \\ub300\\ud574 \\uae08\\uace0 1\\ub144 6\\uac1c\\uc6d4\\uc744 \\uc120\\uace0\\ud558\\uc600\\ub2e4.\\n\\n12\\uc6d4 28\\uc77c, \\uc11c\\uc6b8\\uc5ed \\uc555\\uc0ac \\uc0ac\\uace0\\uac00 \\ub3d9\\uc544\\uc77c\\ubcf4\\uc0ac\\uac00 \\uc120\\uc815\\ud55c 1960\\ub144\\ub3c4 \\uad6d\\ub0b4 10\\ub300 \\ub274\\uc2a4 \\uc911 \\ud558\\ub098\\ub85c \\uc120\\uc815\\ub418\\uc5c8\\ub2e4.\\ndoc4: \\uc815\\ubd80\\ub294 1980\\ub144\\ub300 \\ud6c4\\ubc18\\ubd80\\ud130 \\ub300\\ud615 \\ucc38\\uc0ac\\uac00 \\ubc8c\\uc5b4\\uc9c8 \\ub54c\\ub9c8\\ub2e4 \\uc7ac\\ub09c \\uad00\\ub9ac \\ucee8\\ud2b8\\ub864\\ud0c0\\uc6cc\\ub97c \\uc2e0\\uc124\\ud558\\ub294 \\ub4f1 \\uc787\\ub530\\ub978 \\uc7ac\\ubc1c \\ubc29\\uc9c0 \\ub300\\ucc45\\uc744 \\ub0b4\\ub1a8\\ub2e4. 1980\\ub144\\ub300 \\ud6c4\\ubc18\\uae4c\\uc9c0 \\uc815\\ubd80 \\ubd80\\ucc98\\uc5d0\\uc11c \\uc7ac\\ub09c\\uad00\\ub9ac \\ucee8\\ud2b8\\ub864\\ud0c0\\uc6cc\\ub294 \\uac74\\uc124\\ubd80(\\ud604 \\uad6d\\ud1a0\\uad50\\ud1b5\\ubd80)\\uac00 \\ub9e1\\uc558\\ub2e4. \\ud558\\uc9c0\\ub9cc 1989\\ub144 \\ub178\\uc6d0\\uad6c \\uc6d4\\uacc4\\ubcc0\\uc804\\uc18c \\ud654\\uc7ac \\ubc1c\\uc0dd\\uc73c\\ub85c \\uc11c\\uc6b8 \\ub3d9\\ubd81\\uc9c0\\uc5ed \\uc8fc\\ud0dd\\uac00 \\ubc0f \\ubcd1\\uc6d0\\uc5d0 \\uc804\\uae30 \\uacf5\\uae09\\uc774 \\ub04a\\uae30\\ub294 \\uc0ac\\ud0dc\\uac00 \\ubc8c\\uc5b4\\uc84c\\uc744 \\ub2f9\\uc2dc \\uad00\\ub828 \\uc5c5\\ubb34\\uac00 \\uac74\\uc124\\ubd80, \\uc0b0\\uc790\\ubd80, \\ubcf4\\uac74\\ubd80, \\ub0b4\\ubb34\\ubd80 \\ub4f1\\uc73c\\ub85c \\ub098\\ub258\\uba74\\uc11c \\uc0ac\\uace0 \\uc218\\uc2b5\\uc5d0 \\ud63c\\ub780\\uc744 \\uacaa\\uc5c8\\ub2e4. \\uc774\\uc5d0 \\ub530\\ub77c \\uc815\\ubd80\\ub294 \\uc774\\ub4ec\\ud574 4\\uc6d4 \\uc7ac\\ub09c\\uad00\\ub9ac \\ucc45\\uc784\\uc744 \\ub0b4\\ubb34\\ubd80(\\ud604 \\uc548\\uc804\\ud589\\uc815\\ubd80)\\ub85c \\ub118\\uacbc\\ub2e4. \\uc774\\ud6c4\\uc5d0\\ub3c4 1993\\ub144 \\uc11c\\ud574\\ud6fc\\ub9ac\\ud638 \\uce68\\ubab0 \\uc0ac\\uac74, 1994\\ub144 \\uc131\\uc218\\ub300\\uad50 \\ubd95\\uad34, 1995\\ub144 \\ub300\\uad6c \\uac00\\uc2a4\\ud3ed\\ubc1c \\uc0ac\\uac74 \\ubc0f \\uc0bc\\ud48d\\ubc31\\ud654\\uc810 \\ubd95\\uad34 \\ub4f1\\uc758 \\ub300\\ud615 \\ucc38\\uc0ac\\uac00 \\uc787\\ub530\\ub77c \\ubc1c\\uc0dd\\ud588\\ub2e4. \\uc815\\ubd80\\ub294 \\uc0bc\\ud48d\\ubc31\\ud654\\uc810 \\ucc38\\uc0ac(\\uc0ac\\uc9c4) \\uc9c1\\ud6c4\\uc778 1995\\ub144 7\\uc6d4 \\uc7ac\\ub09c\\uad00\\ub9ac\\ubc95\\uc744 \\uc81c\\uc815\\ud558\\uace0, \\ub2f9\\uc2dc \\ub0b4\\ubb34\\ubd80 \\uc0b0\\ud558\\uc5d0 \\uc7ac\\ub09c\\uad00\\ub9ac\\uad6d\\uc744 \\uc2e0\\uc124\\ud588\\ub2e4. \\ud558\\uc9c0\\ub9cc 1998\\ub144 \\uc678\\ud658\\uc704\\uae30\\uac00 \\ucc3e\\uc544\\uc624\\uc790 \\uc815\\ubd80\\ub294 \\ube44\\uc6a9 \\uc808\\uac10\\uc744 \\uc704\\ud574 \\uc7ac\\ub09c\\uad00\\ub9ac\\uad6d\\uc744 \\uc7ac\\ub09c\\uad00\\ub9ac\\uacfc\\ub85c \\ucd95\\uc18c\\ud588\\ub2e4. 2002\\ub144 \\ud0dc\\ud48d \\ub8e8\\uc0ac\\uac00 \\ud55c\\ubc18\\ub3c4\\ub97c \\uac15\\ud0c0\\ud574 \\ub9c9\\ub300\\ud55c \\uc778\\uba85\\u00b7\\uc7ac\\uc0b0 \\ud53c\\ud574\\ub97c \\uc785\\ud78c \\ub370 \\uc774\\uc5b4 2003\\ub144\\uc5d4 192\\uba85\\uc774 \\uc0ac\\ub9dd\\ud55c \\ub300\\uad6c \\uc9c0\\ud558\\ucca0 \\ucc38\\uc0ac\\uac00 \\ubc1c\\uc0dd\\ud588\\ub2e4. \\ub2f9\\uc2dc \\ub178\\ubb34\\ud604 \\uc815\\ubd80\\ub294 2004\\ub144 6\\uc6d4 \\uc7ac\\ub09c \\uad00\\ub9ac \\uc804\\ub2f4\\uae30\\uad6c\\ub85c \\uc18c\\ubc29\\ubc29\\uc7ac\\uccad\\uc744 \\uc2e0\\uc124\\ud588\\ub2e4. \\uc774\\uba85\\ubc15 \\uc815\\ubd80\\ub294 2008\\ub144 \\ucd9c\\ubc94 \\ub54c \\uc548\\ubcf4 \\ubd84\\uc57c\\ub294 \\uccad\\uc640\\ub300\\uac00, \\uc7ac\\ub09c \\ubd84\\uc57c\\ub294 \\ub2f9\\uc2dc \\ud589\\uc815\\uc548\\uc804\\ubd80\\uc640 \\uc18c\\ubc29\\ubc29\\uc7ac\\uccad\\uc774 \\ub9e1\\ub3c4\\ub85d \\ud588\\ub2e4. \\ubc15\\uadfc\\ud61c \\ub300\\ud1b5\\ub839\\uc740 \\uc9c0\\ub09c\\ud574 3\\uc6d4 NSC \\uc0ac\\ubb34\\ucc98\\ub97c \\ubd80\\ud65c\\uc2dc\\ucf1c \\uc548\\ubcf4 \\ubd84\\uc57c \\ucee8\\ud2b8\\ub864\\ud0c0\\uc6cc \\uc5ed\\ud560\\uc744 \\ub9e1\\uacbc\\uc9c0\\ub9cc \\uc7ac\\ub09c \\ubd84\\uc57c\\ub294 \\uc5ec\\uae30\\uc11c \\ube60\\uc9c4 \\ucc44 \\uc548\\ud589\\ubd80\\uac00 \\ub9e1\\uc558\\ub2e4.\\ndoc5: \\ucda9\\ub82c\\uc0ac\\ub294 \\uc784\\uc9c4\\uc65c\\ub780(1592) \\ub54c \\ub098\\ub77c\\ub97c \\uc9c0\\ud0a4\\ub2e4 \\uc21c\\uc808\\ud55c \\ub3d9\\ub798\\ubd80\\uc0ac \\uc1a1\\uc0c1\\ud604\\u00b7\\ubd80\\uc0b0\\uc9c4\\ucca8\\uc0ac \\uc815\\ubc1c \\ub4f1\\uc744 \\ube44\\ub86f\\ud55c \\ubd80\\uc0b0\\uc9c0\\ubc29\\uc758 \\uad00\\ub9ac\\u00b7\\uad70\\uc778\\u00b7\\ubc31\\uc131 91\\uba85\\uc758 \\ub10b\\uc744 \\ubaa8\\uc154\\ub193\\uc740 \\uacf3\\uc774\\ub2e4. \\ud6a8\\uc885 3\\ub144(1652) \\ub3d9\\ub798\\ubd80\\uc0ac \\uc724\\ubb38\\uac70\\uac00 \\uc1a1\\uc0c1\\ud604\\uc758 \\ud559\\ubb38\\uacfc \\ucda9\\uc808\\uc744 \\uae30\\ub9ac\\uae30 \\uc704\\ud574 \\uc9c0\\uc740 \\uc548\\ub77d\\uc11c\\uc6d0\\uc758 \\uc8fc\\uad00\\uc73c\\ub85c \\ub9e4\\ub144 \\uc74c\\ub825 2\\uc6d4\\uacfc 8\\uc6d4\\uc5d0 \\uc81c\\uc0ac\\ub97c \\uc9c0\\ub0b4\\uace0 \\uc788\\ub2e4. \\n\\n\\ucda9\\ub82c\\uc0ac\\uc81c\\ud5a5\\uc740 \\uc120\\uc870 38\\ub144(1605) \\ub2f9\\uc2dc\\uc758 \\ub3d9\\ub798\\ubd80\\uc0ac \\uc724\\ud6e4\\uc774 \\ub3d9\\ub798\\uc74d\\uc131 \\ub0a8\\ubb38 \\ubc16\\uc758 \\ub18d\\uc8fc\\uc0b0\\uc5d0 \\uc1a1\\uc0c1\\ud604\\uc758 \\uc704\\ud328\\ub97c \\ubaa8\\uc2e0 \\uc1a1\\uacf5\\uc0ac(\\u5b8b\\u516c\\u7960)\\ub97c \\uc9c0\\uc5b4 \\ub9e4\\ub144 \\uc81c\\uc0ac\\ub97c \\uc9c0\\ub0b8\\ub370\\uc11c \\ube44\\ub86f\\ub418\\uc5c8\\ub2e4. 1652\\ub144 \\ub3d9\\ub798\\ubd80\\uc0ac \\uc724\\ubb38\\uac70\\ub294 \\uc548\\ub77d\\uc11c\\uc6d0\\uc744 \\uc9d3\\ub294 \\ud55c\\ud3b8 \\uc1a1\\uacf5\\uc0ac\\uac00 \\uc881\\uace0, \\uc131\\ubb38 \\uac00\\uae4c\\uc774\\uc5d0 \\uc788\\uc5b4 \\uc2dc\\ub044\\ub7ec\\uc6c0\\uc744 \\ud55c\\ud0c4\\ud558\\uc5ec \\ud604\\uc7ac\\uc758 \\uc790\\ub9ac\\ub85c \\uc0ac\\ub2f9\\uc744 \\uc774\\uc804\\ud558\\uc600\\uc73c\\uba70 \\uc0c8\\ub85c \\uc0ac\\ub2f9\\uc744 \\uc9c0\\uc5b4 \\ucda9\\ub82c\\uc0ac\\ub77c \\uc774\\ub984\\ud558\\uc600\\ub2e4. \\uadf8 \\ud6c4 \\ucda9\\ub82c\\uc0ac\\uc81c\\ud5a5\\uc740 \\uc624\\ub298\\ub0a0\\uc5d0 \\uc774\\ub974\\uae30\\uae4c\\uc9c0 350\\uc5ec \\ub144 \\ub3d9\\uc548 \\ub04a\\uc774\\uc9c0 \\uc54a\\uace0 \\uacc4\\uc18d\\ub418\\uace0 \\uc788\\ub2e4.\\n\\n\\ucda9\\ub82c\\uc0ac\\ub294 \\ub098\\ub77c\\ub97c \\uc9c0\\ud0a4\\ub2e4 \\uc21c\\uc808\\ud55c \\uc21c\\uad6d\\uc120\\uc5f4\\uc758 \\ub10b\\uc744 \\ubaa8\\uc2ec\\uc5d0 \\uc788\\uc5b4\\uc11c \\uc704\\ub85c\\ub294 \\ubd80\\uc0ac\\u00b7\\uad70\\uc218\\u00b7\\ucca8\\uc0ac\\ub97c \\ube44\\ub86f\\ud558\\uc5ec \\uc544\\ub798\\ub85c\\ub294 \\uc885\\uacfc \\uc560\\ucca9\\uae4c\\uc9c0 \\uadf8 \\uacf5\\uc744 \\uae30\\ub9ac\\uc5b4 \\ubaa8\\uc2dc\\uace0 \\uc788\\ub294\\ub370 \\uc774\\ub294 \\ub2e4\\ub978 \\uc9c0\\uc5ed\\uc5d0\\uc11c \\ubcfc \\uc218 \\uc5c6\\ub294 \\ud2b9\\uc9d5\\uc774\\uba70, \\uc81c\\ud5a5 \\ub54c \\uc5ec\\uc131 \\uc81c\\uc0ac\\uad00\\uc774 \\ucc38\\uc5ec\\ud558\\ub294 \\uac83\\ub3c4 \\ub4dc\\ubb38 \\uc77c\\uc774\\ub2e4. \\ub610\\ud55c \\uc624\\ub79c \\uc138\\uc6d4 \\ubcc0\\ud568\\uc5c6\\uc774 \\uc61b \\ubc95\\uc2dd\\ub300\\ub85c \\uc81c\\ud5a5\\uc744 \\uc9c0\\ub0b4\\ub294 \\uac83\\ub3c4 \\ud2b9\\uc9d5\\uc774\\ub77c \\ud558\\uaca0\\ub2e4.\\n\\n\\uc704\\uc758 docs \\uc911\\uc5d0\\uc11c\\ub9cc \\uc815\\ubcf4\\ub97c \\uadfc\\uac70\\ub85c \\ud558\\uc5ec, \\uc9c8\\ubb38\\uc5d0 \\ub2f5\\ubcc0\\ud574 \\uc8fc\\uc138\\uc694.\\n\\ub2f5\\ubcc0\\uc5d0\\uc11c \\uc778\\uc6a9\\ud55c \\ubb38\\uc11c\\uc758 \\ub0b4\\uc6a9\\uc5d0\\ub294 \\ubc18\\ub4dc\\uc2dc [[doc1]], [[doc2]], ... \\ud615\\uc2dd\\uc73c\\ub85c \\uc778\\uc6a9 \\ud45c\\uc2dc\\ub97c \\ud574\\uc8fc\\uc138\\uc694.\\n\\ucd94\\ub860\\uc774\\ub098 \\uc9c0\\uc5b4\\ub0b4\\ub294 \\ub2f5\\ubcc0\\uc740 \\uc0bc\\uac00\\uc8fc\\uc2dc\\uace0, docs\\uc5d0 \\uba85\\uc2dc\\uc801\\uc73c\\ub85c \\ub098\\ud0c0\\ub09c \\ub0b4\\uc6a9\\ub9cc \\uc778\\uc6a9\\ud574 \\uc8fc\\uc138\\uc694.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"system_prompt\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\ub2f9\\uc2e0\\uc740 \\uc8fc\\uc5b4\\uc9c4 \\uc5ec\\ub7ec \\ubb38\\uc11c(docs)\\ub97c \\ubc14\\ud0d5\\uc73c\\ub85c, \\uc0ac\\uc6a9\\uc790\\uc758 \\uc9c8\\ubb38\\uc5d0 \\ucd5c\\ub300\\ud55c \\uc815\\ud655\\ud558\\uac8c, \\uadf8\\ub9ac\\uace0 \\ubb38\\uc11c \\ub0b4\\uc5d0\\uc11c\\ub9cc \\uc815\\ubcf4\\ub97c \\uadfc\\uac70\\ub85c \\ud558\\uc5ec \\ub2f5\\ubcc0\\ud558\\ub294 AI \\ube44\\uc11c\\uc785\\ub2c8\\ub2e4.\\n\\uc544\\ub798 \\uc9c0\\uce68\\uc744 \\ubc18\\ub4dc\\uc2dc \\uc9c0\\ucf1c\\uc8fc\\uc138\\uc694:\\n\\n- \\ub2f5\\ubcc0\\uc740 \\ubc18\\ub4dc\\uc2dc docs\\uc5d0\\uc11c \\ucc3e\\uc740 \\ub0b4\\uc6a9\\uc5d0 \\ud55c\\ud574\\uc11c\\ub9cc \\uc791\\uc131\\ud574\\uc8fc\\uc138\\uc694. docs\\uc5d0 \\uc5c6\\ub294 \\ub0b4\\uc6a9\\uc740 \\ucd94\\ub860\\ud558\\uac70\\ub098 \\uc9c0\\uc5b4\\ub0b4\\uc9c0 \\ub9c8\\uc138\\uc694.\\n- \\ub2f5\\ubcc0\\uc5d0\\uc11c \\uc778\\uc6a9\\ud558\\ub294 \\ubd80\\ubd84\\uc774 \\uc788\\ub2e4\\uba74, \\ubc18\\ub4dc\\uc2dc \\ud574\\ub2f9 \\ubb38\\uc11c\\uc758 \\ubc88\\ud638(\\uc608: [[doc1]], [[doc2]])\\ub85c \\uadfc\\uac70\\ub97c \\ud45c\\uc2dc\\ud574 \\uc8fc\\uc138\\uc694.\\n- docs\\uc758 \\uc21c\\uc11c\\uc640 \\ubc88\\ud638\\ub294 \\uc911\\uc694\\ud569\\ub2c8\\ub2e4. docs\\uc5d0\\uc11c \\uc778\\uc6a9\\ud558\\uc9c0 \\uc54a\\uc740 \\uc815\\ubcf4\\ub294 \\ub2f5\\ubcc0\\uc5d0 \\ud3ec\\ud568\\ud558\\uc9c0 \\ub9c8\\uc138\\uc694.\\n- \\ub2f5\\ubcc0\\uc758 \\uadfc\\uac70\\uac00 \\ub418\\ub294 \\ubb38\\uc11c \\ubc88\\ud638\\ub97c \\uc0dd\\ub7b5\\ud558\\uc9c0 \\ub9d0\\uace0, \\ud56d\\uc0c1 \\uc778\\uc6a9 \\ud0dc\\uadf8([[doc1]], [[doc2]], ...)\\ub97c \\ud3ec\\ud568\\ud574 \\uc8fc\\uc138\\uc694.\\n- \\ubaa8\\ub4e0 \\ub2f5\\ubcc0\\uc740 \\uc874\\ub313\\ub9d0\\uc744 \\uc0ac\\uc6a9\\ud558\\uc138\\uc694.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 972,\n        \"samples\": [\n          \"\\ub2e8\\uccb4\\uc7a5 \\uc120\\uac70 \\ucd9c\\ub9c8 \\uc2dc \\ud589\\uc815\\uc5c5\\ubb34 \\uc548\\uc815\\uc5d0 \\ub300\\ud574 \\uc124\\uba85\\ub4dc\\ub9ac\\uba74, \\uc9c0\\ubc29\\uc790\\uce58\\ub2e8\\uccb4\\uc7a5\\uacfc \\ubd80\\ub2e8\\uccb4\\uc7a5\\uc774 \\ub3d9\\uc2dc\\uc5d0 \\uc0ac\\ud1f4\\ud558\\ub294 \\uacbd\\uc6b0\\uac00 \\ub9ce\\uc544 \\ud589\\uc815 \\uacf5\\ubc31\\uc774 \\uc2ec\\uac01\\ud55c \\uc0c1\\ud669\\uc785\\ub2c8\\ub2e4. \\uc608\\ub97c \\ub4e4\\uc5b4 \\ucd98\\ucc9c\\uc2dc\\uc5d0\\uc11c\\ub294 \\uc2dc\\uc7a5\\uacfc \\ubd80\\uc2dc\\uc7a5\\uc774 \\ubaa8\\ub450 \\uc0ac\\ud1f4\\ud558\\uba74\\uc11c \\uc8fc\\uc694 \\ud589\\uc815\\uc5c5\\ubb34\\uac00 \\uc0ac\\uc2e4\\uc0c1 \\uc911\\ub2e8\\ub418\\uc5c8\\uace0, \\uc2e4\\ubb34\\uc120\\uc5d0\\uc11c \\ud63c\\uc120\\uc774 \\ube5a\\uc5b4\\uc9c0\\uace0 \\uc788\\uc2b5\\ub2c8\\ub2e4. \\ub610\\ud55c, \\uc804\\uc8fc\\uc2dc, \\ucc3d\\uc6d0\\uc2dc, \\ub300\\uad6c \\ubd81\\uad6c \\ub4f1\\uc5d0\\uc11c\\ub3c4 \\ub2e8\\uccb4\\uc7a5\\uacfc \\ubd80\\ub2e8\\uccb4\\uc7a5\\uc774 \\ub3d9\\uc2dc\\uc5d0 \\ud1f4\\uc784\\ud574 \\uc5c5\\ubb34 \\uacf5\\ubc31\\uc774 \\ubc1c\\uc0dd\\ud558\\uace0 \\uc788\\uc2b5\\ub2c8\\ub2e4. \\uc774\\ub85c \\uc778\\ud574 \\uc8fc\\uc694 \\uc9c0\\uc5ed \\ud604\\uc548\\uc0ac\\uc5c5\\uc774 \\uc9c4\\ucc99\\ub418\\uc9c0 \\uc54a\\uace0, \\uacf5\\ubb34\\uc6d0\\ub4e4\\uc774 \\uc120\\uac70\\ucca0\\uc5d0 \\ub208\\uce58 \\ubcf4\\uba70 \\uc5c5\\ubb34\\ub97c \\uc81c\\ub300\\ub85c \\uc218\\ud589\\ud558\\uc9c0 \\uc54a\\ub294 \\ud604\\uc0c1\\ub3c4 \\ub098\\ud0c0\\ub098\\uace0 \\uc788\\uc2b5\\ub2c8\\ub2e4. \\uc774\\ub7ec\\ud55c \\ubb38\\uc81c\\ub97c \\ud574\\uacb0\\ud558\\uae30 \\uc704\\ud574\\uc11c\\ub294 \\ub2e8\\uccb4\\uc7a5 \\uc778\\uc0ac\\uad8c \\ub3c5\\uc810 \\ucd5c\\uc18c\\ud654 \\ub4f1 \\ub300\\ucc45 \\ub9c8\\ub828\\uc774 \\ud544\\uc694\\ud558\\ub2e4\\ub294 \\uc9c0\\uc801\\uc774 \\uc788\\uc2b5\\ub2c8\\ub2e4[[doc1]].\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc_citations\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":2}],"source":["import pandas as pd\n","import ast\n","\n","# 파일명과 타입 정의\n","file_info = [\n","    ('klue_mrc_prompt_docs_5_answer_citations.csv', 1),\n","    ('klue_mrc_prompt_docs_1_4_answer_citations.csv', 2),\n","    ('klue_mrc_nominal_question_docs_1_5_answer_citations.csv', 3),\n","    ('klue_mrc_prompt_multidocs_answer_citations.csv', 4),\n","]\n","\n","# 통일할 컬럼명\n","final_columns = ['question', 'docs', 'user_prompt', 'system_prompt', 'answer', 'doc_citations']\n","\n","# 파일별로 읽어서 정리\n","dfs = []\n","for fname, type_id in file_info:\n","    df = pd.read_csv(fname)\n","    df['type'] = type_id\n","    # 컬럼명 통일 (nominal_question, question_only → question으로 맞추기)\n","    for candidate in ['nominal_question', 'question_only']:\n","        if candidate in df.columns and 'question' not in df.columns:\n","            df = df.rename(columns={candidate: 'question'})\n","    # 필요한 컬럼만 선택 (일부 파일에서 누락될 수 있음)\n","    cols = [col for col in final_columns if col in df.columns] + ['type']\n","    df = df[cols]\n","    # 누락된 컬럼은 빈 값으로 채우기\n","    for col in final_columns:\n","        if col not in df.columns:\n","            df[col] = ''\n","    # 최종 컬럼 순서 적용\n","    df = df[final_columns + ['type']]\n","    dfs.append(df)\n","\n","# 하나로 합치기\n","df_all = pd.concat(dfs, ignore_index=True)\n","\n","# docs와 doc_citations 컬럼을 리스트로 변환\n","for col in ['docs', 'doc_citations']:\n","    df_all[col] = df_all[col].apply(\n","        lambda x: ast.literal_eval(x) if isinstance(x, str) and x and x != '' else []\n","    )\n","\n","# 결과 확인\n","print(df_all.shape)\n","print(type(df_all.loc[0, 'docs']), df_all.loc[0, 'docs'])            # <class 'list'>\n","print(type(df_all.loc[0, 'doc_citations']), df_all.loc[0, 'doc_citations'])  # <class 'list'>\n","df_all.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0pVzD6ilFSyL","outputId":"8433eb1e-e20a-4a6c-d86e-4dcab23add2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["type별 데이터 분포:\n","type 1: 469건\n","type 2: 285건\n","type 3: 193건\n","type 4: 462건\n"]}],"source":["print(\"type별 데이터 분포:\")\n","for t, n in df_all['type'].value_counts().sort_index().items():\n","    print(f\"type {t}: {n}건\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KYyGfGKLFosu","outputId":"65b3e383-c9de-485a-d80e-73c378860450"},"outputs":[{"name":"stdout","output_type":"stream","text":["train 데이터 shape: (1126, 7)\n","test 데이터 shape: (283, 7)\n","type\n","1    375\n","2    228\n","3    154\n","4    369\n","Name: count, dtype: int64\n","type\n","1    94\n","2    57\n","3    39\n","4    93\n","Name: count, dtype: int64\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","train_dfs = []\n","test_dfs = []\n","\n","for t in sorted(df_all['type'].unique()):\n","    sub = df_all[df_all['type'] == t].sample(frac=1, random_state=42).reset_index(drop=True)  # 셔플\n","    train, test = train_test_split(sub, test_size=0.2, random_state=42)\n","    train_dfs.append(train)\n","    test_dfs.append(test)\n","\n","# 합치기\n","df_train = pd.concat(train_dfs, ignore_index=True)\n","df_test = pd.concat(test_dfs, ignore_index=True)\n","\n","print(f\"train 데이터 shape: {df_train.shape}\")\n","print(f\"test 데이터 shape: {df_test.shape}\")\n","print(df_train['type'].value_counts().sort_index())\n","print(df_test['type'].value_counts().sort_index())"]},{"cell_type":"markdown","source":["## OpenAI 형식 변환 및 jsonl 저장"],"metadata":{"id":"X4-j_oUnmoai"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kN6Mdf2KFzlL"},"outputs":[],"source":["system_prompt = \"\"\"당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, 그리고 문서 내에서만 정보를 근거로 하여 답변하는 AI 비서입니다.\n","아래 지침을 반드시 지켜주세요:\n","\n","- 답변은 반드시 docs에서 찾은 내용에 한해서만 작성해주세요. docs에 없는 내용은 추론하거나 지어내지 마세요.\n","- 답변에서 인용하는 부분이 있다면, 반드시 해당 문서의 번호(예: [[doc1]], [[doc2]])로 근거를 표시해 주세요.\n","- docs의 순서와 번호는 중요합니다. docs에서 인용하지 않은 정보는 답변에 포함하지 마세요.\n","- 답변의 근거가 되는 문서 번호를 생략하지 말고, 항상 인용 태그([[doc1]], [[doc2]], ...)를 포함해 주세요.\n","- 모든 답변은 존댓말을 사용하세요.\"\"\"\n","\n","# OpenAI format으로 데이터 변환을 위한 함수\n","def format_data(row):\n","    \"\"\"\n","    DataFrame의 row에서 OpenAI 파인튜닝용 messages 구조(dict)를 생성합니다.\n","    \"\"\"\n","    docs = row['docs']\n","    # 문서에 번호 붙이기\n","    doc_items = [f\"doc{i+1}: {doc}\" for i, doc in enumerate(docs)]\n","    docs_str = '\\n'.join(doc_items)\n","\n","    user_prompt = f\"\"\"질문: {row['question']}\n","\n","docs:\n","{docs_str}\n","\n","위의 docs 중에서만 정보를 근거로 하여, 질문에 답변해 주세요.\n","답변에서 인용한 문서의 내용에는 반드시 [[doc1]], [[doc2]], ... 형식으로 인용 표시를 해주세요.\n","추론이나 지어내는 답변은 삼가주시고, docs에 명시적으로 나타난 내용만 인용해 주세요.\"\"\"\n","\n","    return {\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": system_prompt},\n","            {\"role\": \"user\", \"content\": user_prompt},\n","            {\"role\": \"assistant\", \"content\": str(row['answer']).strip()}\n","        ]\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I9oG2seNLSEW"},"outputs":[],"source":["import json\n","\n","# 변환\n","train_data = df_train.apply(format_data, axis=1).tolist()\n","test_data = df_test.apply(format_data, axis=1).tolist()\n","\n","# 저장\n","with open('klue_mrc_rag_train.jsonl', 'w', encoding='utf-8') as f:\n","    for item in train_data:\n","        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n","\n","with open('klue_mrc_rag_test.jsonl', 'w', encoding='utf-8') as f:\n","    for item in test_data:\n","        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"baoZjFzUPfhn","outputId":"65deb6d1-6ee7-4ee7-c176-b7b2c8a10ced"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"messages\": [\n","    {\n","      \"role\": \"system\",\n","      \"content\": \"당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, 그리고 문서 내에서만 정보를 근거로 하여 답변하는 AI 비서입니다.\\n아래 지침을 반드시 지켜주세요:\\n\\n- 답변은 반드시 docs에서 찾은 내용에 한해서만 작성해주세요. docs에 없는 내용은 추론하거나 지어내지 마세요.\\n- 답변에서 인용하는 부분이 있다면, 반드시 해당 문서의 번호(예: [[doc1]], [[doc2]])로 근거를 표시해 주세요.\\n- docs의 순서와 번호는 중요합니다. docs에서 인용하지 않은 정보는 답변에 포함하지 마세요.\\n- 답변의 근거가 되는 문서 번호를 생략하지 말고, 항상 인용 태그([[doc1]], [[doc2]], ...)를 포함해 주세요.\\n- 모든 답변은 존댓말을 사용하세요.\"\n","    },\n","    {\n","      \"role\": \"user\",\n","      \"content\": \"질문: 멕시코 연방 정부는 메탈클래드에게 얼마를 보상하라고 하였나?\\n\\ndocs:\\ndoc1: 소송가액만 약 5조원에 달하는 론스타펀드가 한국 정부를 상대로 제기한 투자자-국가소송(ISD)의 첫 심리가 15일(현지시간) 미국 워싱턴 국제투자분쟁해결센터(ICSID)에서 열린다. ISD는 1987년 애플 홍콩법인이 스리랑카 정부를 상대로 낸 것을 시작으로 주로 선진국 투자자들이 개발도상국 정부를 제소해왔다.한국 정부가 ISD에 휘말린 것은 이번이 처음이다. 결과가 미칠 파장이 만만치 않은 만큼 정부는 국무조정실 주도로 태스크포스를 구성했다. 세금 회피를 위해 벨기에 등에 페이퍼컴퍼니를 세운 뒤 외환은행을 인수한 론스타는 소송 주체도 페이퍼컴퍼니를 내세운 것으로 알려져 정부는 이 점을 집중 공략할 것으로 전해졌다.○한국 정부의 첫 ISD 사건론스타가 ICSID에 중재를 신청한 것은 2012년 11월21일이다. 신청인(원고)은 LSF-KEB홀딩스, 스타홀딩스 등 8곳이다. 이들 법인의 근거지는 룩셈부르크 한 곳, 나머지는 벨기에다. 뚜렷한 실체가 없는 페이퍼컴퍼니 형태의 특수목적법인(SPC)을 상대로 한국 정부가 싸워야 한다는 얘기다. 사건번호는 ‘ARB/12/37’. 2012년에 제기된 37번째 중재 사건을 뜻한다. 2013년 5월10일 한국 정부와 론스타가 추천한 중재인을 포함해 모두 3명으로 중재 재판부가 구성됐다. 한덕수 전 국무총리, 전광우·김석동 전 금융위원장 등 정부와 금융계 전직 고위 인사 26명이 증인 신문에 대거 참여한다.론스타는 두 가지 이유로 한국 정부에 46억7900만달러(약 5조1000억원)를 청구했다. 첫 번째는 한국 정부가 HSBC에 외환은행을 매각하려던 론스타펀드의 계획을 고의로 지연시켜 피해를 입혔다는 것이다. 론스타는 이 때문에 HSBC와 매각 협상을 벌일 때보다 훨씬 싼 3조9157억원에 외환은행을 하나은행에 팔았다고 주장하고 있다.두 번째 쟁점은 과세 문제다. 외환은행 등의 매각 과정에서 발생한 양도차익에 세금을 부과한 국세청의 조치가 부당하다는 것이 론스타의 주장이다. 론스타는 외환은행을 인수하고 매각한 주체가 벨기에·룩셈부르크 법인으로 ‘한-벨기에·룩셈부르크 투자협정’에 이중과세 금지 조항이 있는 만큼 한국에 세금을 낼 필요가 없다고 강조하고 있다.이에 대해 정부는 론스타 자회사들이 실체 없는 ‘유령회사’로 투자협정으로 보호할 대상이 아니라고 맞서고 있다.○투자자보다 정부 승률 조금 높아최대 관심사는 중재 결과다. 과세 부문에 대한 심리는 오는 6월29일 열릴 예정이어서 일러야 내년 상반기에 결론이 나온다. 국제연합무역개발회의(UNCTAD)에 따르면 1987~2007년 판정이 내려진 119건 가운데 40건은 투자자가, 42건은 정부가 승소했다. 37건은 쌍방 합의로 끝났다. 정부 측 승률이 35%에 불과하다.하지만 2010~2012년 ICSID에 회부된 중재 신청의 결과는 정부 쪽에 유리하게 나오고 있다. ISID에 올라온 90건 중 결론이 난 사례는 22건이다. 이 가운데 정부가 승소한 건이 12개고, 투자자 승소는 2건에 불과하다. 결론이 난 사례들만 놓고 보면 승소율이 54%로 올라간다. 임병덕 법무법인 한별 고문은 “영미법상 국제중재재판에 에스토펠(禁反言·말바꾸기 금지)이라는 실체법상의 원칙이 광범위하게 적용된다는 점이 한국 정부가 넘어야 할 가장 큰 장애물일 것”이라고 말했다. 박동휘 기자 donghuip@hankyung.com ◆투자자-국가소송(ISD)\\ndoc2: 마이크로소프트의 창업자 빌 게이츠가 주목할 기업이라고 평가한 모뉴엘이 법정관리를 신청하자 은행들과 한국무역보험공사 간 책임공방이 일고 있다. 모뉴엘에 대출해준 은행들과 대출을 보증한 무역보험공사가 수천억원에 달하는 대출금을 회수하지 못할 것으로 보이자 책임을 떠넘기고 있는 것.모뉴엘이 시중은행들로부터 대출받은 금액은 6700억원 정도로 추정된다. 이 중 무보가 보증을 해준 금액은 약 3300억원 정도로 추산된다. 무보 관계자는 22일 “보증금액은 공식적으로 아직 밝힐 수 없다”고 말했다.모뉴엘이 무보가 발급한 보증을 담보 삼아 은행들에서 대출받은 금액을 갚지 못하면 무보가 대출금에 이자까지 더해 전액 물어줘야 한다. 무보는 이후 모뉴엘의 제품을 사간 수입 업자를 찾아 구상권을 행사하는 절차를 밟는다. 무보 측은 하지만 “이는 정상적인 상황에서 진행하는 절차이고, 지금과 같은 상황에서는 먼저 책임 소재를 가려야 한다”는 입장이다.은행들은 수출거래 내역을 제대로 파악하지 않고 보증을 해준 무보의 책임이 크다고 주장하고 있다. 모뉴엘 채권은행 관계자들은 “은행은 수출 관련 심사를 서류상으로 확인하는 것일 뿐 현장에선 하기 힘들다”며 “그건 보증을 해준 무보의 역할”이라고 했다.반면 무보 관계자는 “무보는 은행들로부터 받은 수출실적 증명서와 수출 대금이 오간 은행들의 통장을 받아 보증 심사를 한다”고 맞받았다. “수출 대금이 실제로 오간 통장 내역은 은행들이 알고 있다”며 “은행들은 이 기업에 신용대출 등 다른 거래도 하고 있어 은행들이 현장을 확인해야 할 사항”이라고 주장했다.모뉴엘이 지난 20일 법정관리를 신청했지만 이날까지도 은행들이 무보에 사고통지를 하지 않은 것을 두고도 주장이 엇갈렸다. 무보 측은 “은행들로부터 사고통지를 받아야 조사에 정식으로 착수하는데, 아직까지 은행들이 공식적인 사고통지를 하지 않고 있다”며 “이는 은행들이 담보 등을 확보하며 손실을 최대한 줄인 뒤 무보에 알리겠다는 것으로밖에 생각되지 않는다”고 비난했다. 하지만 은행들은 “사고통지는 한 달 이내에만 하면 되는 것”이라고 주장했다.\\ndoc3: 마르코스 아르투로, 카를로스, 알프레도, 그리고 엑토르 벨트란 레이바의 4형제가 조직한 마약 범죄 카르텔이다. 2004 ~ 2005년, 아르투로 벨트란은 멕시코 북동부 지역의 마약 운송로를 차지하기 위한 다툼에서 시날로아를 위해 운영한 강력한 암살조직을 이끌었다.\\n이 조직은 뇌물과 협박으로 멕시코 정치, 법조계와 경찰기구에 침투하여 마약 진압작전에 대한 중요 정보를 빼내었고 심지어는 멕시코의 인터폴에도 잠입하였다.\\n2009년 13월에 카르텔의 리더인 아르투로가 멕시코 해병대에게 살해당하자 카르텔 내부에서는 엑토르 벨트란 레이바와 아르투로의 최고 수하였던 에드가르 발데스 비야레알과의 세력 다툼이 발생한다. 동시에 카르텔은 내부 분열로 사우스 퍼시픽 카르텔, 라 마노 콘 오호스, 아카플코 독립 카르텔, 그리고 라 바레도라 등의 소규모 집단으로 나뉘게 되고 후자 2개 조직은 다시 상위 카르텔 내부 분열에 개입하게 되었다.\\n멕시코 연방 경찰은 이 카르텔이 완전히 와해된 것으로 여기고 있으며 마지막 지도자였던 엑토르는 더 이상 활동 징후가 없는 도망자 신세에 있는 것으로 알려지고 있다. 미국은 엑토르에게 5백만 불의 현상금을 걸었으며 멕시코 정부는 210만 불의 현상금을 건 상태이다.\\ndoc4: 메탈클래드는 멕시코가 연방 및 주 차원에서 인가된 매립장 건설 및 영업 행위에 대해 과달카사르 군 정부가 인가를 지연 및 반려하는 것을 방기하였으며, 이에 따라 북미자유무역협정 11장의 1110조 에 규정된 수용(expropriation)에 상당한 조치가 성립되었다고 주장하였다. 이러한 수용에 상당한 조치임이 인정되면 멕시코 정부는 그에 상응하는 공정 시장가격에 따른 대가를 지불해야 한다.\\n\\n이에 대해 국제투자분쟁해결센터 중재판정부는 앞서 1105조 위반 사유와 같이 멕시코의 행정절차에 있어 투명성이 보장되지 않았으므로 수용에 상당한 조치가 이뤄졌다고 결론을 내렸다. 아울러 유독성 폐기물 매립장 건설 인가권은 연방 정부의 고유 권한이며, 군 정부는 시설의 물리적 결함 등의 사유로만 제한적으로 인가를 거부할 수 있음에도 권한을 남용하여 건설 인가를 반려했다고 지적하였다.\\n\\n반면 캐나다 브리티시컬럼비아주 대법원은 역시 앞서 1105조 위반이 분쟁 범위를 넘어서고 있으므로, 이를 근거로 수용에 상당한 조치가 이뤄졌다는 결론을 내려서는 안된다고 판결하였다.\\ndoc5: 메탈클래드 대 멕시코 연방 정부 사건은 국제투자분쟁해결센터(ICSID) 및 캐나다 브리티시컬럼비아주 대법원에서 열린 투자자-국가 소송 사건이다. 미국의 폐기물 관리 업체인 메탈클래드(Metalclad) 사가 멕시코 연방 정부 및 산루이스포토시주 정부로부터 얻은 폐기물 매립장 허가가 관할 과달카사르 군에서 거부되자, 이로 인해 발생한 손해에 대해 멕시코 연방 정부를 상대로 북미자유무역협정 11장에 규정된 투자자 국가 분쟁 해결 제도에 따라 손해배상 청구 소송을 제기하면서 시작되었다. 이에 대해 중재를 맡은 국제투자분쟁해결센터는 멕시코 연방 정부가 북미자유무역협정 1105조에 규정된 상대국 투자자에 대한 공정하고 동등한 처분 원칙의 위반 및 1110조에 규정된 수용에 상당한 조치를 했다는 결론을 내리고, 멕시코 연방 정부가 메탈클래드에게 1,668만 5천 달러를 배상하도록 결정하였다. 멕시코 연방 정부는 역시 북미자유무역협정에 보장된 권리에 따라, 캐나다 브리티시컬럼비아주 대법원에 이 처분의 법적 정당성에 대한 심의를 요청하였다. 이에 대해 캐나다 브리티시컬럼비아주 대법원은 북미자유무역협정 1105조 위반에 대해서는 국제투자분쟁해결센터 중재판정부의 결정 권한 밖임을 지적하였으나, 1110조의 수용 상당 조치에 대해서는 일부 책임을 인정하여 110만 달러가 감액된 1,560만 달러의 배상 판결을 확정했다.\\n\\n위의 docs 중에서만 정보를 근거로 하여, 질문에 답변해 주세요.\\n답변에서 인용한 문서의 내용에는 반드시 [[doc1]], [[doc2]], ... 형식으로 인용 표시를 해주세요.\\n추론이나 지어내는 답변은 삼가주시고, docs에 명시적으로 나타난 내용만 인용해 주세요.\"\n","    },\n","    {\n","      \"role\": \"assistant\",\n","      \"content\": \"멕시코 연방 정부는 메탈클래드에게 1,668만 5천 달러를 배상하도록 결정하였으며, 이후 캐나다 브리티시컬럼비아주 대법원에서 이 금액이 110만 달러 감액되어 최종적으로 1,560만 달러의 배상 판결이 확정되었습니다[[doc5]].\"\n","    }\n","  ]\n","}\n"]}],"source":["print(json.dumps(train_data[0], ensure_ascii=False, indent=2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8mg9JsytL4am"},"outputs":[],"source":["# 한 줄에 하나의 json (OpenAI 형식)\n","train_data = []\n","with open('klue_mrc_rag_train.jsonl', encoding='utf-8') as f:\n","    for line in f:\n","        train_data.append(json.loads(line))\n","\n","# test도 동일\n","test_data = []\n","with open('klue_mrc_rag_test.jsonl', encoding='utf-8') as f:\n","    for line in f:\n","        test_data.append(json.loads(line))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CIsjoxfKOAAZ","outputId":"bc7136b4-3470-439e-f10f-503c3292c9a2"},"outputs":[{"data":{"text/plain":["[{'role': 'system',\n","  'content': '당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, 그리고 문서 내에서만 정보를 근거로 하여 답변하는 AI 비서입니다.\\n아래 지침을 반드시 지켜주세요:\\n\\n- 답변은 반드시 docs에서 찾은 내용에 한해서만 작성해주세요. docs에 없는 내용은 추론하거나 지어내지 마세요.\\n- 답변에서 인용하는 부분이 있다면, 반드시 해당 문서의 번호(예: [[doc1]], [[doc2]])로 근거를 표시해 주세요.\\n- docs의 순서와 번호는 중요합니다. docs에서 인용하지 않은 정보는 답변에 포함하지 마세요.\\n- 답변의 근거가 되는 문서 번호를 생략하지 말고, 항상 인용 태그([[doc1]], [[doc2]], ...)를 포함해 주세요.\\n- 모든 답변은 존댓말을 사용하세요.'},\n"," {'role': 'user',\n","  'content': '질문: 멕시코 연방 정부는 메탈클래드에게 얼마를 보상하라고 하였나?\\n\\ndocs:\\ndoc1: 소송가액만 약 5조원에 달하는 론스타펀드가 한국 정부를 상대로 제기한 투자자-국가소송(ISD)의 첫 심리가 15일(현지시간) 미국 워싱턴 국제투자분쟁해결센터(ICSID)에서 열린다. ISD는 1987년 애플 홍콩법인이 스리랑카 정부를 상대로 낸 것을 시작으로 주로 선진국 투자자들이 개발도상국 정부를 제소해왔다.한국 정부가 ISD에 휘말린 것은 이번이 처음이다. 결과가 미칠 파장이 만만치 않은 만큼 정부는 국무조정실 주도로 태스크포스를 구성했다. 세금 회피를 위해 벨기에 등에 페이퍼컴퍼니를 세운 뒤 외환은행을 인수한 론스타는 소송 주체도 페이퍼컴퍼니를 내세운 것으로 알려져 정부는 이 점을 집중 공략할 것으로 전해졌다.○한국 정부의 첫 ISD 사건론스타가 ICSID에 중재를 신청한 것은 2012년 11월21일이다. 신청인(원고)은 LSF-KEB홀딩스, 스타홀딩스 등 8곳이다. 이들 법인의 근거지는 룩셈부르크 한 곳, 나머지는 벨기에다. 뚜렷한 실체가 없는 페이퍼컴퍼니 형태의 특수목적법인(SPC)을 상대로 한국 정부가 싸워야 한다는 얘기다. 사건번호는 ‘ARB/12/37’. 2012년에 제기된 37번째 중재 사건을 뜻한다. 2013년 5월10일 한국 정부와 론스타가 추천한 중재인을 포함해 모두 3명으로 중재 재판부가 구성됐다. 한덕수 전 국무총리, 전광우·김석동 전 금융위원장 등 정부와 금융계 전직 고위 인사 26명이 증인 신문에 대거 참여한다.론스타는 두 가지 이유로 한국 정부에 46억7900만달러(약 5조1000억원)를 청구했다. 첫 번째는 한국 정부가 HSBC에 외환은행을 매각하려던 론스타펀드의 계획을 고의로 지연시켜 피해를 입혔다는 것이다. 론스타는 이 때문에 HSBC와 매각 협상을 벌일 때보다 훨씬 싼 3조9157억원에 외환은행을 하나은행에 팔았다고 주장하고 있다.두 번째 쟁점은 과세 문제다. 외환은행 등의 매각 과정에서 발생한 양도차익에 세금을 부과한 국세청의 조치가 부당하다는 것이 론스타의 주장이다. 론스타는 외환은행을 인수하고 매각한 주체가 벨기에·룩셈부르크 법인으로 ‘한-벨기에·룩셈부르크 투자협정’에 이중과세 금지 조항이 있는 만큼 한국에 세금을 낼 필요가 없다고 강조하고 있다.이에 대해 정부는 론스타 자회사들이 실체 없는 ‘유령회사’로 투자협정으로 보호할 대상이 아니라고 맞서고 있다.○투자자보다 정부 승률 조금 높아최대 관심사는 중재 결과다. 과세 부문에 대한 심리는 오는 6월29일 열릴 예정이어서 일러야 내년 상반기에 결론이 나온다. 국제연합무역개발회의(UNCTAD)에 따르면 1987~2007년 판정이 내려진 119건 가운데 40건은 투자자가, 42건은 정부가 승소했다. 37건은 쌍방 합의로 끝났다. 정부 측 승률이 35%에 불과하다.하지만 2010~2012년 ICSID에 회부된 중재 신청의 결과는 정부 쪽에 유리하게 나오고 있다. ISID에 올라온 90건 중 결론이 난 사례는 22건이다. 이 가운데 정부가 승소한 건이 12개고, 투자자 승소는 2건에 불과하다. 결론이 난 사례들만 놓고 보면 승소율이 54%로 올라간다. 임병덕 법무법인 한별 고문은 “영미법상 국제중재재판에 에스토펠(禁反言·말바꾸기 금지)이라는 실체법상의 원칙이 광범위하게 적용된다는 점이 한국 정부가 넘어야 할 가장 큰 장애물일 것”이라고 말했다. 박동휘 기자 donghuip@hankyung.com ◆투자자-국가소송(ISD)\\ndoc2: 마이크로소프트의 창업자 빌 게이츠가 주목할 기업이라고 평가한 모뉴엘이 법정관리를 신청하자 은행들과 한국무역보험공사 간 책임공방이 일고 있다. 모뉴엘에 대출해준 은행들과 대출을 보증한 무역보험공사가 수천억원에 달하는 대출금을 회수하지 못할 것으로 보이자 책임을 떠넘기고 있는 것.모뉴엘이 시중은행들로부터 대출받은 금액은 6700억원 정도로 추정된다. 이 중 무보가 보증을 해준 금액은 약 3300억원 정도로 추산된다. 무보 관계자는 22일 “보증금액은 공식적으로 아직 밝힐 수 없다”고 말했다.모뉴엘이 무보가 발급한 보증을 담보 삼아 은행들에서 대출받은 금액을 갚지 못하면 무보가 대출금에 이자까지 더해 전액 물어줘야 한다. 무보는 이후 모뉴엘의 제품을 사간 수입 업자를 찾아 구상권을 행사하는 절차를 밟는다. 무보 측은 하지만 “이는 정상적인 상황에서 진행하는 절차이고, 지금과 같은 상황에서는 먼저 책임 소재를 가려야 한다”는 입장이다.은행들은 수출거래 내역을 제대로 파악하지 않고 보증을 해준 무보의 책임이 크다고 주장하고 있다. 모뉴엘 채권은행 관계자들은 “은행은 수출 관련 심사를 서류상으로 확인하는 것일 뿐 현장에선 하기 힘들다”며 “그건 보증을 해준 무보의 역할”이라고 했다.반면 무보 관계자는 “무보는 은행들로부터 받은 수출실적 증명서와 수출 대금이 오간 은행들의 통장을 받아 보증 심사를 한다”고 맞받았다. “수출 대금이 실제로 오간 통장 내역은 은행들이 알고 있다”며 “은행들은 이 기업에 신용대출 등 다른 거래도 하고 있어 은행들이 현장을 확인해야 할 사항”이라고 주장했다.모뉴엘이 지난 20일 법정관리를 신청했지만 이날까지도 은행들이 무보에 사고통지를 하지 않은 것을 두고도 주장이 엇갈렸다. 무보 측은 “은행들로부터 사고통지를 받아야 조사에 정식으로 착수하는데, 아직까지 은행들이 공식적인 사고통지를 하지 않고 있다”며 “이는 은행들이 담보 등을 확보하며 손실을 최대한 줄인 뒤 무보에 알리겠다는 것으로밖에 생각되지 않는다”고 비난했다. 하지만 은행들은 “사고통지는 한 달 이내에만 하면 되는 것”이라고 주장했다.\\ndoc3: 마르코스 아르투로, 카를로스, 알프레도, 그리고 엑토르 벨트란 레이바의 4형제가 조직한 마약 범죄 카르텔이다. 2004 ~ 2005년, 아르투로 벨트란은 멕시코 북동부 지역의 마약 운송로를 차지하기 위한 다툼에서 시날로아를 위해 운영한 강력한 암살조직을 이끌었다.\\n이 조직은 뇌물과 협박으로 멕시코 정치, 법조계와 경찰기구에 침투하여 마약 진압작전에 대한 중요 정보를 빼내었고 심지어는 멕시코의 인터폴에도 잠입하였다.\\n2009년 13월에 카르텔의 리더인 아르투로가 멕시코 해병대에게 살해당하자 카르텔 내부에서는 엑토르 벨트란 레이바와 아르투로의 최고 수하였던 에드가르 발데스 비야레알과의 세력 다툼이 발생한다. 동시에 카르텔은 내부 분열로 사우스 퍼시픽 카르텔, 라 마노 콘 오호스, 아카플코 독립 카르텔, 그리고 라 바레도라 등의 소규모 집단으로 나뉘게 되고 후자 2개 조직은 다시 상위 카르텔 내부 분열에 개입하게 되었다.\\n멕시코 연방 경찰은 이 카르텔이 완전히 와해된 것으로 여기고 있으며 마지막 지도자였던 엑토르는 더 이상 활동 징후가 없는 도망자 신세에 있는 것으로 알려지고 있다. 미국은 엑토르에게 5백만 불의 현상금을 걸었으며 멕시코 정부는 210만 불의 현상금을 건 상태이다.\\ndoc4: 메탈클래드는 멕시코가 연방 및 주 차원에서 인가된 매립장 건설 및 영업 행위에 대해 과달카사르 군 정부가 인가를 지연 및 반려하는 것을 방기하였으며, 이에 따라 북미자유무역협정 11장의 1110조 에 규정된 수용(expropriation)에 상당한 조치가 성립되었다고 주장하였다. 이러한 수용에 상당한 조치임이 인정되면 멕시코 정부는 그에 상응하는 공정 시장가격에 따른 대가를 지불해야 한다.\\n\\n이에 대해 국제투자분쟁해결센터 중재판정부는 앞서 1105조 위반 사유와 같이 멕시코의 행정절차에 있어 투명성이 보장되지 않았으므로 수용에 상당한 조치가 이뤄졌다고 결론을 내렸다. 아울러 유독성 폐기물 매립장 건설 인가권은 연방 정부의 고유 권한이며, 군 정부는 시설의 물리적 결함 등의 사유로만 제한적으로 인가를 거부할 수 있음에도 권한을 남용하여 건설 인가를 반려했다고 지적하였다.\\n\\n반면 캐나다 브리티시컬럼비아주 대법원은 역시 앞서 1105조 위반이 분쟁 범위를 넘어서고 있으므로, 이를 근거로 수용에 상당한 조치가 이뤄졌다는 결론을 내려서는 안된다고 판결하였다.\\ndoc5: 메탈클래드 대 멕시코 연방 정부 사건은 국제투자분쟁해결센터(ICSID) 및 캐나다 브리티시컬럼비아주 대법원에서 열린 투자자-국가 소송 사건이다. 미국의 폐기물 관리 업체인 메탈클래드(Metalclad) 사가 멕시코 연방 정부 및 산루이스포토시주 정부로부터 얻은 폐기물 매립장 허가가 관할 과달카사르 군에서 거부되자, 이로 인해 발생한 손해에 대해 멕시코 연방 정부를 상대로 북미자유무역협정 11장에 규정된 투자자 국가 분쟁 해결 제도에 따라 손해배상 청구 소송을 제기하면서 시작되었다. 이에 대해 중재를 맡은 국제투자분쟁해결센터는 멕시코 연방 정부가 북미자유무역협정 1105조에 규정된 상대국 투자자에 대한 공정하고 동등한 처분 원칙의 위반 및 1110조에 규정된 수용에 상당한 조치를 했다는 결론을 내리고, 멕시코 연방 정부가 메탈클래드에게 1,668만 5천 달러를 배상하도록 결정하였다. 멕시코 연방 정부는 역시 북미자유무역협정에 보장된 권리에 따라, 캐나다 브리티시컬럼비아주 대법원에 이 처분의 법적 정당성에 대한 심의를 요청하였다. 이에 대해 캐나다 브리티시컬럼비아주 대법원은 북미자유무역협정 1105조 위반에 대해서는 국제투자분쟁해결센터 중재판정부의 결정 권한 밖임을 지적하였으나, 1110조의 수용 상당 조치에 대해서는 일부 책임을 인정하여 110만 달러가 감액된 1,560만 달러의 배상 판결을 확정했다.\\n\\n위의 docs 중에서만 정보를 근거로 하여, 질문에 답변해 주세요.\\n답변에서 인용한 문서의 내용에는 반드시 [[doc1]], [[doc2]], ... 형식으로 인용 표시를 해주세요.\\n추론이나 지어내는 답변은 삼가주시고, docs에 명시적으로 나타난 내용만 인용해 주세요.'},\n"," {'role': 'assistant',\n","  'content': '멕시코 연방 정부는 메탈클래드에게 1,668만 5천 달러를 배상하도록 결정하였으며, 이후 캐나다 브리티시컬럼비아주 대법원에서 이 금액이 110만 달러 감액되어 최종적으로 1,560만 달러의 배상 판결이 확정되었습니다[[doc5]].'}]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["train_data[0]['messages']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RPR5Q0zJjyni","outputId":"a204ccfe-44ca-4f70-9d40-5e76a5753375"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'datasets.arrow_dataset.Dataset'>\n","<class 'datasets.arrow_dataset.Dataset'>\n"]}],"source":["from datasets import Dataset   # Hugging Face Datasets 라이브러리에서 Dataset 클래스를 가져옴.\n","\n","# 리스트 형태로 준비된 train_dataset, test_dataset을 Hugging Face의 Dataset 객체로 변환.\n","# Dataset 객체는 데이터 로딩·샘플링·배치 작업 등 다양한 기능을 효율적으로 지원\n","train_dataset = Dataset.from_list(train_data)\n","test_dataset = Dataset.from_list(test_data)\n","\n","print(type(train_dataset))\n","print(type(test_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UgyCHibXkOB-","outputId":"6fe46e37-739f-4144-c8b2-f5a5226385de"},"outputs":[{"data":{"text/plain":["{'messages': [{'content': '당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, 그리고 문서 내에서만 정보를 근거로 하여 답변하는 AI 비서입니다.\\n아래 지침을 반드시 지켜주세요:\\n\\n- 답변은 반드시 docs에서 찾은 내용에 한해서만 작성해주세요. docs에 없는 내용은 추론하거나 지어내지 마세요.\\n- 답변에서 인용하는 부분이 있다면, 반드시 해당 문서의 번호(예: [[doc1]], [[doc2]])로 근거를 표시해 주세요.\\n- docs의 순서와 번호는 중요합니다. docs에서 인용하지 않은 정보는 답변에 포함하지 마세요.\\n- 답변의 근거가 되는 문서 번호를 생략하지 말고, 항상 인용 태그([[doc1]], [[doc2]], ...)를 포함해 주세요.\\n- 모든 답변은 존댓말을 사용하세요.',\n","   'role': 'system'},\n","  {'content': '질문: 멕시코 연방 정부는 메탈클래드에게 얼마를 보상하라고 하였나?\\n\\ndocs:\\ndoc1: 소송가액만 약 5조원에 달하는 론스타펀드가 한국 정부를 상대로 제기한 투자자-국가소송(ISD)의 첫 심리가 15일(현지시간) 미국 워싱턴 국제투자분쟁해결센터(ICSID)에서 열린다. ISD는 1987년 애플 홍콩법인이 스리랑카 정부를 상대로 낸 것을 시작으로 주로 선진국 투자자들이 개발도상국 정부를 제소해왔다.한국 정부가 ISD에 휘말린 것은 이번이 처음이다. 결과가 미칠 파장이 만만치 않은 만큼 정부는 국무조정실 주도로 태스크포스를 구성했다. 세금 회피를 위해 벨기에 등에 페이퍼컴퍼니를 세운 뒤 외환은행을 인수한 론스타는 소송 주체도 페이퍼컴퍼니를 내세운 것으로 알려져 정부는 이 점을 집중 공략할 것으로 전해졌다.○한국 정부의 첫 ISD 사건론스타가 ICSID에 중재를 신청한 것은 2012년 11월21일이다. 신청인(원고)은 LSF-KEB홀딩스, 스타홀딩스 등 8곳이다. 이들 법인의 근거지는 룩셈부르크 한 곳, 나머지는 벨기에다. 뚜렷한 실체가 없는 페이퍼컴퍼니 형태의 특수목적법인(SPC)을 상대로 한국 정부가 싸워야 한다는 얘기다. 사건번호는 ‘ARB/12/37’. 2012년에 제기된 37번째 중재 사건을 뜻한다. 2013년 5월10일 한국 정부와 론스타가 추천한 중재인을 포함해 모두 3명으로 중재 재판부가 구성됐다. 한덕수 전 국무총리, 전광우·김석동 전 금융위원장 등 정부와 금융계 전직 고위 인사 26명이 증인 신문에 대거 참여한다.론스타는 두 가지 이유로 한국 정부에 46억7900만달러(약 5조1000억원)를 청구했다. 첫 번째는 한국 정부가 HSBC에 외환은행을 매각하려던 론스타펀드의 계획을 고의로 지연시켜 피해를 입혔다는 것이다. 론스타는 이 때문에 HSBC와 매각 협상을 벌일 때보다 훨씬 싼 3조9157억원에 외환은행을 하나은행에 팔았다고 주장하고 있다.두 번째 쟁점은 과세 문제다. 외환은행 등의 매각 과정에서 발생한 양도차익에 세금을 부과한 국세청의 조치가 부당하다는 것이 론스타의 주장이다. 론스타는 외환은행을 인수하고 매각한 주체가 벨기에·룩셈부르크 법인으로 ‘한-벨기에·룩셈부르크 투자협정’에 이중과세 금지 조항이 있는 만큼 한국에 세금을 낼 필요가 없다고 강조하고 있다.이에 대해 정부는 론스타 자회사들이 실체 없는 ‘유령회사’로 투자협정으로 보호할 대상이 아니라고 맞서고 있다.○투자자보다 정부 승률 조금 높아최대 관심사는 중재 결과다. 과세 부문에 대한 심리는 오는 6월29일 열릴 예정이어서 일러야 내년 상반기에 결론이 나온다. 국제연합무역개발회의(UNCTAD)에 따르면 1987~2007년 판정이 내려진 119건 가운데 40건은 투자자가, 42건은 정부가 승소했다. 37건은 쌍방 합의로 끝났다. 정부 측 승률이 35%에 불과하다.하지만 2010~2012년 ICSID에 회부된 중재 신청의 결과는 정부 쪽에 유리하게 나오고 있다. ISID에 올라온 90건 중 결론이 난 사례는 22건이다. 이 가운데 정부가 승소한 건이 12개고, 투자자 승소는 2건에 불과하다. 결론이 난 사례들만 놓고 보면 승소율이 54%로 올라간다. 임병덕 법무법인 한별 고문은 “영미법상 국제중재재판에 에스토펠(禁反言·말바꾸기 금지)이라는 실체법상의 원칙이 광범위하게 적용된다는 점이 한국 정부가 넘어야 할 가장 큰 장애물일 것”이라고 말했다. 박동휘 기자 donghuip@hankyung.com ◆투자자-국가소송(ISD)\\ndoc2: 마이크로소프트의 창업자 빌 게이츠가 주목할 기업이라고 평가한 모뉴엘이 법정관리를 신청하자 은행들과 한국무역보험공사 간 책임공방이 일고 있다. 모뉴엘에 대출해준 은행들과 대출을 보증한 무역보험공사가 수천억원에 달하는 대출금을 회수하지 못할 것으로 보이자 책임을 떠넘기고 있는 것.모뉴엘이 시중은행들로부터 대출받은 금액은 6700억원 정도로 추정된다. 이 중 무보가 보증을 해준 금액은 약 3300억원 정도로 추산된다. 무보 관계자는 22일 “보증금액은 공식적으로 아직 밝힐 수 없다”고 말했다.모뉴엘이 무보가 발급한 보증을 담보 삼아 은행들에서 대출받은 금액을 갚지 못하면 무보가 대출금에 이자까지 더해 전액 물어줘야 한다. 무보는 이후 모뉴엘의 제품을 사간 수입 업자를 찾아 구상권을 행사하는 절차를 밟는다. 무보 측은 하지만 “이는 정상적인 상황에서 진행하는 절차이고, 지금과 같은 상황에서는 먼저 책임 소재를 가려야 한다”는 입장이다.은행들은 수출거래 내역을 제대로 파악하지 않고 보증을 해준 무보의 책임이 크다고 주장하고 있다. 모뉴엘 채권은행 관계자들은 “은행은 수출 관련 심사를 서류상으로 확인하는 것일 뿐 현장에선 하기 힘들다”며 “그건 보증을 해준 무보의 역할”이라고 했다.반면 무보 관계자는 “무보는 은행들로부터 받은 수출실적 증명서와 수출 대금이 오간 은행들의 통장을 받아 보증 심사를 한다”고 맞받았다. “수출 대금이 실제로 오간 통장 내역은 은행들이 알고 있다”며 “은행들은 이 기업에 신용대출 등 다른 거래도 하고 있어 은행들이 현장을 확인해야 할 사항”이라고 주장했다.모뉴엘이 지난 20일 법정관리를 신청했지만 이날까지도 은행들이 무보에 사고통지를 하지 않은 것을 두고도 주장이 엇갈렸다. 무보 측은 “은행들로부터 사고통지를 받아야 조사에 정식으로 착수하는데, 아직까지 은행들이 공식적인 사고통지를 하지 않고 있다”며 “이는 은행들이 담보 등을 확보하며 손실을 최대한 줄인 뒤 무보에 알리겠다는 것으로밖에 생각되지 않는다”고 비난했다. 하지만 은행들은 “사고통지는 한 달 이내에만 하면 되는 것”이라고 주장했다.\\ndoc3: 마르코스 아르투로, 카를로스, 알프레도, 그리고 엑토르 벨트란 레이바의 4형제가 조직한 마약 범죄 카르텔이다. 2004 ~ 2005년, 아르투로 벨트란은 멕시코 북동부 지역의 마약 운송로를 차지하기 위한 다툼에서 시날로아를 위해 운영한 강력한 암살조직을 이끌었다.\\n이 조직은 뇌물과 협박으로 멕시코 정치, 법조계와 경찰기구에 침투하여 마약 진압작전에 대한 중요 정보를 빼내었고 심지어는 멕시코의 인터폴에도 잠입하였다.\\n2009년 13월에 카르텔의 리더인 아르투로가 멕시코 해병대에게 살해당하자 카르텔 내부에서는 엑토르 벨트란 레이바와 아르투로의 최고 수하였던 에드가르 발데스 비야레알과의 세력 다툼이 발생한다. 동시에 카르텔은 내부 분열로 사우스 퍼시픽 카르텔, 라 마노 콘 오호스, 아카플코 독립 카르텔, 그리고 라 바레도라 등의 소규모 집단으로 나뉘게 되고 후자 2개 조직은 다시 상위 카르텔 내부 분열에 개입하게 되었다.\\n멕시코 연방 경찰은 이 카르텔이 완전히 와해된 것으로 여기고 있으며 마지막 지도자였던 엑토르는 더 이상 활동 징후가 없는 도망자 신세에 있는 것으로 알려지고 있다. 미국은 엑토르에게 5백만 불의 현상금을 걸었으며 멕시코 정부는 210만 불의 현상금을 건 상태이다.\\ndoc4: 메탈클래드는 멕시코가 연방 및 주 차원에서 인가된 매립장 건설 및 영업 행위에 대해 과달카사르 군 정부가 인가를 지연 및 반려하는 것을 방기하였으며, 이에 따라 북미자유무역협정 11장의 1110조 에 규정된 수용(expropriation)에 상당한 조치가 성립되었다고 주장하였다. 이러한 수용에 상당한 조치임이 인정되면 멕시코 정부는 그에 상응하는 공정 시장가격에 따른 대가를 지불해야 한다.\\n\\n이에 대해 국제투자분쟁해결센터 중재판정부는 앞서 1105조 위반 사유와 같이 멕시코의 행정절차에 있어 투명성이 보장되지 않았으므로 수용에 상당한 조치가 이뤄졌다고 결론을 내렸다. 아울러 유독성 폐기물 매립장 건설 인가권은 연방 정부의 고유 권한이며, 군 정부는 시설의 물리적 결함 등의 사유로만 제한적으로 인가를 거부할 수 있음에도 권한을 남용하여 건설 인가를 반려했다고 지적하였다.\\n\\n반면 캐나다 브리티시컬럼비아주 대법원은 역시 앞서 1105조 위반이 분쟁 범위를 넘어서고 있으므로, 이를 근거로 수용에 상당한 조치가 이뤄졌다는 결론을 내려서는 안된다고 판결하였다.\\ndoc5: 메탈클래드 대 멕시코 연방 정부 사건은 국제투자분쟁해결센터(ICSID) 및 캐나다 브리티시컬럼비아주 대법원에서 열린 투자자-국가 소송 사건이다. 미국의 폐기물 관리 업체인 메탈클래드(Metalclad) 사가 멕시코 연방 정부 및 산루이스포토시주 정부로부터 얻은 폐기물 매립장 허가가 관할 과달카사르 군에서 거부되자, 이로 인해 발생한 손해에 대해 멕시코 연방 정부를 상대로 북미자유무역협정 11장에 규정된 투자자 국가 분쟁 해결 제도에 따라 손해배상 청구 소송을 제기하면서 시작되었다. 이에 대해 중재를 맡은 국제투자분쟁해결센터는 멕시코 연방 정부가 북미자유무역협정 1105조에 규정된 상대국 투자자에 대한 공정하고 동등한 처분 원칙의 위반 및 1110조에 규정된 수용에 상당한 조치를 했다는 결론을 내리고, 멕시코 연방 정부가 메탈클래드에게 1,668만 5천 달러를 배상하도록 결정하였다. 멕시코 연방 정부는 역시 북미자유무역협정에 보장된 권리에 따라, 캐나다 브리티시컬럼비아주 대법원에 이 처분의 법적 정당성에 대한 심의를 요청하였다. 이에 대해 캐나다 브리티시컬럼비아주 대법원은 북미자유무역협정 1105조 위반에 대해서는 국제투자분쟁해결센터 중재판정부의 결정 권한 밖임을 지적하였으나, 1110조의 수용 상당 조치에 대해서는 일부 책임을 인정하여 110만 달러가 감액된 1,560만 달러의 배상 판결을 확정했다.\\n\\n위의 docs 중에서만 정보를 근거로 하여, 질문에 답변해 주세요.\\n답변에서 인용한 문서의 내용에는 반드시 [[doc1]], [[doc2]], ... 형식으로 인용 표시를 해주세요.\\n추론이나 지어내는 답변은 삼가주시고, docs에 명시적으로 나타난 내용만 인용해 주세요.',\n","   'role': 'user'},\n","  {'content': '멕시코 연방 정부는 메탈클래드에게 1,668만 5천 달러를 배상하도록 결정하였으며, 이후 캐나다 브리티시컬럼비아주 대법원에서 이 금액이 110만 달러 감액되어 최종적으로 1,560만 달러의 배상 판결이 확정되었습니다[[doc5]].',\n","   'role': 'assistant'}]}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset[0]   # 전처리·포맷팅된 학습 데이터가 정확히 기대하는 구조와 내용(messages, role, content 등)으로 정상적으로 만들어졌는지 확인"]},{"cell_type":"markdown","metadata":{"id":"MfGq23UKSK-7"},"source":["## BaseModel - NCSOFT/Llama-VARCO-8B-Instruct\n","\n","* **Hugging Face Model Hub**에 등록된\n","  **NCSOFT**(엔씨소프트)에서 공개한\n","  **Llama 계열 8B(약 80억 파라미터)의 Instruction(지시문 튜닝) 기반 언어모델**입니다.\n","\n","\n","**각 단어의 의미**\n","\n","* **NCSOFT/**\n","\n","  * 모델을 공개한 개발사 또는 연구기관(Hugging Face에서 계정 또는 조직 이름)\n","* **Llama-VARCO-8B**\n","\n","  * Meta Llama 기반 구조 + VARCO라는 엔씨 고유 튜닝 전략이 적용된 8B(80억 파라미터) 모델임을 뜻합니다.\n","* **Instruct**\n","\n","  * 단순 언어모델(base)이 아니라,\n","    \"명령어/지시문(prompt)에 답변을 잘 하도록 추가로 파인튜닝된 모델\"이라는 뜻입니다.\n","  * 즉, ChatGPT나 챗봇처럼 \"질문하면 답하는\" 데 최적화된 버전입니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WYowsvXOQvwp"},"source":["### torch\\_dtype\n","**torch\\_dtype**는 모델을 메모리에 올릴 때 사용할 \\*\\*파라미터 데이터 타입(숫자 표현 방식)\\*\\*을 지정하는 옵션입니다.\n","\n","**주요 옵션 값과 의미**\n","\n","* **torch.float32**\n","\n","  * 32비트 부동소수점(가장 정밀, 가장 많은 메모리 사용)\n","  * 전통적으로 가장 많이 사용\n","\n","* **torch.float16**\n","\n","  * 16비트 부동소수점(메모리 절약, 빠른 연산, 일부 정밀도 손실 가능)\n","  * 최신 GPU에서 대형 모델 학습/추론 시 널리 사용\n","\n","* **torch.bfloat16**\n","\n","  * 16비트 부동소수점이지만 float16보다 더 넓은 표현 범위 제공(정밀도 손실 적음)\n","  * NVIDIA A100, H100 등 최신 GPU에서 매우 효율적\n","  * 속도·메모리 효율은 float16처럼 좋고, 안정성은 float32에 가깝다\n","\n","**실전에서 왜 쓸까?**\n","\n","* 모델이 너무 커서 GPU 메모리가 부족할 때\n","  torch\\_dtype를 float16, bfloat16으로 설정하면\n","  **메모리 사용량을 절반 이하로 줄일 수 있음**\n","* 연산 속도도 크게 빨라집니다.\n","* 최신 모델/실습에서는 float32보다\n","  bfloat16 또는 float16이 기본으로 쓰입니다.\n","\n","**정리:**\n","torch\\_dtype는\n","**모델의 크기, 연산 속도, 실습 환경(GPU) 최적화를 위해\n","모델 파라미터가 어떤 숫자 타입으로 올라갈지 정하는 중요한 옵션**입니다.\n","(메모리 부족/속도 저하 시, float16/bfloat16으로 바꾸면 실습이 쉬워집니다.)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["091f46ee8cb54730960c07438c7840ad","623eb2ffbbc945aab1b9cf20fe1eb850","9f588df2191d40cbb146ff3586a9761a","736cc4a9e5ae44af9c7fed01cb1ce092","9799358b20ec41e3b83196d7271016fb","dd003bf41d204b2c8875a4ac79b73fe9","a71646030f184630a59165b8062536d6","cfafa78cce68498298a6ace7c694f193","6b8ca8ede83743b4a1c28314c6a54be3","9b1b73e4a5634622ac66c4c5bcfe28ca","7c982aefe044420c9a17294f592ebcd8","b403646c2b9b4e959bed043ebe13f228"]},"id":"touIPE4RkQVr","outputId":"f8af3a67-56f9-4440-dad1-0e3d784bfa62"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b403646c2b9b4e959bed043ebe13f228","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Hugging Face transformers 라이브러리의 AutoModelForCausalLM 클래스. \"Causal Language Model\"(GPT류, Llama류 등) 텍스트 생성용 사전학습 모델을 이름만 지정하면 자동으로 불러오는 도구.\n","# AutoTokenizer 클래스는 동일 사전학습 모델에 맞는 토크나이저를 자동으로 불러옵니다.텍스트를 숫자 토큰으로 바꾸고, 다시 텍스트로 되돌릴 때 사용합니다.\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# 사용할 사전학습(프리트레인드) 언어모델의 이름.\n","# Hugging Face Hub에 등록된 Llama-VARCO-8B-Instruct 모델을 불러옵니다.\n","pretrained_model_name = \"NCSOFT/Llama-VARCO-8B-Instruct\"\n","\n","# Hugging Face의 AutoModelForCausalLM 클래스를 사용해, pretrained_model_name에 해당하는 사전학습 모델을 로딩.\n","model = AutoModelForCausalLM.from_pretrained(\n","    pretrained_model_name,\n","    device_map=\"auto\",           # GPU 환경을 자동 감지해 최적의 디바이스에 모델을 할당\n","    torch_dtype=torch.bfloat16,  # 모델 파라미터를 bfloat16(16비트 부동소수점) 형식으로 올려, 메모리 효율과 연산 속도를 높임.\n",")\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)  # 같은 pretrained_model_name을 사용해 해당 모델의 토크나이저도 함께 불러옴.\n","                                                                  # 토크나이저는 텍스트를 숫자 토큰으로 변환하고, 생성 결과를 다시 텍스트로 복원하는 역할을 수행."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4D1utP8WkYqf","outputId":"d7ddf5a8-061c-44fa-94c5-02596f875fca"},"outputs":[{"name":"stdout","output_type":"stream","text":["<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","\n","당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, 그리고 문서 내에서만 정보를 근거로 하여 답변하는 AI 비서입니다.\n","아래 지침을 반드시 지켜주세요:\n","\n","- 답변은 반드시 docs에서 찾은 내용에 한해서만 작성해주세요. docs에 없는 내용은 추론하거나 지어내지 마세요.\n","- 답변에서 인용하는 부분이 있다면, 반드시 해당 문서의 번호(예: [[doc1]], [[doc2]])로 근거를 표시해 주세요.\n","- docs의 순서와 번호는 중요합니다. docs에서 인용하지 않은 정보는 답변에 포함하지 마세요.\n","- 답변의 근거가 되는 문서 번호를 생략하지 말고, 항상 인용 태그([[doc1]], [[doc2]], ...)를 포함해 주세요.\n","- 모든 답변은 존댓말을 사용하세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","질문: 멕시코 연방 정부는 메탈클래드에게 얼마를 보상하라고 하였나?\n","\n","docs:\n","doc1: 소송가액만 약 5조원에 달하는 론스타펀드가 한국 정부를 상대로 제기한 투자자-국가소송(ISD)의 첫 심리가 15일(현지시간) 미국 워싱턴 국제투자분쟁해결센터(ICSID)에서 열린다. ISD는 1987년 애플 홍콩법인이 스리랑카 정부를 상대로 낸 것을 시작으로 주로 선진국 투자자들이 개발도상국 정부를 제소해왔다.한국 정부가 ISD에 휘말린 것은 이번이 처음이다. 결과가 미칠 파장이 만만치 않은 만큼 정부는 국무조정실 주도로 태스크포스를 구성했다. 세금 회피를 위해 벨기에 등에 페이퍼컴퍼니를 세운 뒤 외환은행을 인수한 론스타는 소송 주체도 페이퍼컴퍼니를 내세운 것으로 알려져 정부는 이 점을 집중 공략할 것으로 전해졌다.○한국 정부의 첫 ISD 사건론스타가 ICSID에 중재를 신청한 것은 2012년 11월21일이다. 신청인(원고)은 LSF-KEB홀딩스, 스타홀딩스 등 8곳이다. 이들 법인의 근거지는 룩셈부르크 한 곳, 나머지는 벨기에다. 뚜렷한 실체가 없는 페이퍼컴퍼니 형태의 특수목적법인(SPC)을 상대로 한국 정부가 싸워야 한다는 얘기다. 사건번호는 ‘ARB/12/37’. 2012년에 제기된 37번째 중재 사건을 뜻한다. 2013년 5월10일 한국 정부와 론스타가 추천한 중재인을 포함해 모두 3명으로 중재 재판부가 구성됐다. 한덕수 전 국무총리, 전광우·김석동 전 금융위원장 등 정부와 금융계 전직 고위 인사 26명이 증인 신문에 대거 참여한다.론스타는 두 가지 이유로 한국 정부에 46억7900만달러(약 5조1000억원)를 청구했다. 첫 번째는 한국 정부가 HSBC에 외환은행을 매각하려던 론스타펀드의 계획을 고의로 지연시켜 피해를 입혔다는 것이다. 론스타는 이 때문에 HSBC와 매각 협상을 벌일 때보다 훨씬 싼 3조9157억원에 외환은행을 하나은행에 팔았다고 주장하고 있다.두 번째 쟁점은 과세 문제다. 외환은행 등의 매각 과정에서 발생한 양도차익에 세금을 부과한 국세청의 조치가 부당하다는 것이 론스타의 주장이다. 론스타는 외환은행을 인수하고 매각한 주체가 벨기에·룩셈부르크 법인으로 ‘한-벨기에·룩셈부르크 투자협정’에 이중과세 금지 조항이 있는 만큼 한국에 세금을 낼 필요가 없다고 강조하고 있다.이에 대해 정부는 론스타 자회사들이 실체 없는 ‘유령회사’로 투자협정으로 보호할 대상이 아니라고 맞서고 있다.○투자자보다 정부 승률 조금 높아최대 관심사는 중재 결과다. 과세 부문에 대한 심리는 오는 6월29일 열릴 예정이어서 일러야 내년 상반기에 결론이 나온다. 국제연합무역개발회의(UNCTAD)에 따르면 1987~2007년 판정이 내려진 119건 가운데 40건은 투자자가, 42건은 정부가 승소했다. 37건은 쌍방 합의로 끝났다. 정부 측 승률이 35%에 불과하다.하지만 2010~2012년 ICSID에 회부된 중재 신청의 결과는 정부 쪽에 유리하게 나오고 있다. ISID에 올라온 90건 중 결론이 난 사례는 22건이다. 이 가운데 정부가 승소한 건이 12개고, 투자자 승소는 2건에 불과하다. 결론이 난 사례들만 놓고 보면 승소율이 54%로 올라간다. 임병덕 법무법인 한별 고문은 “영미법상 국제중재재판에 에스토펠(禁反言·말바꾸기 금지)이라는 실체법상의 원칙이 광범위하게 적용된다는 점이 한국 정부가 넘어야 할 가장 큰 장애물일 것”이라고 말했다. 박동휘 기자 donghuip@hankyung.com ◆투자자-국가소송(ISD)\n","doc2: 마이크로소프트의 창업자 빌 게이츠가 주목할 기업이라고 평가한 모뉴엘이 법정관리를 신청하자 은행들과 한국무역보험공사 간 책임공방이 일고 있다. 모뉴엘에 대출해준 은행들과 대출을 보증한 무역보험공사가 수천억원에 달하는 대출금을 회수하지 못할 것으로 보이자 책임을 떠넘기고 있는 것.모뉴엘이 시중은행들로부터 대출받은 금액은 6700억원 정도로 추정된다. 이 중 무보가 보증을 해준 금액은 약 3300억원 정도로 추산된다. 무보 관계자는 22일 “보증금액은 공식적으로 아직 밝힐 수 없다”고 말했다.모뉴엘이 무보가 발급한 보증을 담보 삼아 은행들에서 대출받은 금액을 갚지 못하면 무보가 대출금에 이자까지 더해 전액 물어줘야 한다. 무보는 이후 모뉴엘의 제품을 사간 수입 업자를 찾아 구상권을 행사하는 절차를 밟는다. 무보 측은 하지만 “이는 정상적인 상황에서 진행하는 절차이고, 지금과 같은 상황에서는 먼저 책임 소재를 가려야 한다”는 입장이다.은행들은 수출거래 내역을 제대로 파악하지 않고 보증을 해준 무보의 책임이 크다고 주장하고 있다. 모뉴엘 채권은행 관계자들은 “은행은 수출 관련 심사를 서류상으로 확인하는 것일 뿐 현장에선 하기 힘들다”며 “그건 보증을 해준 무보의 역할”이라고 했다.반면 무보 관계자는 “무보는 은행들로부터 받은 수출실적 증명서와 수출 대금이 오간 은행들의 통장을 받아 보증 심사를 한다”고 맞받았다. “수출 대금이 실제로 오간 통장 내역은 은행들이 알고 있다”며 “은행들은 이 기업에 신용대출 등 다른 거래도 하고 있어 은행들이 현장을 확인해야 할 사항”이라고 주장했다.모뉴엘이 지난 20일 법정관리를 신청했지만 이날까지도 은행들이 무보에 사고통지를 하지 않은 것을 두고도 주장이 엇갈렸다. 무보 측은 “은행들로부터 사고통지를 받아야 조사에 정식으로 착수하는데, 아직까지 은행들이 공식적인 사고통지를 하지 않고 있다”며 “이는 은행들이 담보 등을 확보하며 손실을 최대한 줄인 뒤 무보에 알리겠다는 것으로밖에 생각되지 않는다”고 비난했다. 하지만 은행들은 “사고통지는 한 달 이내에만 하면 되는 것”이라고 주장했다.\n","doc3: 마르코스 아르투로, 카를로스, 알프레도, 그리고 엑토르 벨트란 레이바의 4형제가 조직한 마약 범죄 카르텔이다. 2004 ~ 2005년, 아르투로 벨트란은 멕시코 북동부 지역의 마약 운송로를 차지하기 위한 다툼에서 시날로아를 위해 운영한 강력한 암살조직을 이끌었다.\n","이 조직은 뇌물과 협박으로 멕시코 정치, 법조계와 경찰기구에 침투하여 마약 진압작전에 대한 중요 정보를 빼내었고 심지어는 멕시코의 인터폴에도 잠입하였다.\n","2009년 13월에 카르텔의 리더인 아르투로가 멕시코 해병대에게 살해당하자 카르텔 내부에서는 엑토르 벨트란 레이바와 아르투로의 최고 수하였던 에드가르 발데스 비야레알과의 세력 다툼이 발생한다. 동시에 카르텔은 내부 분열로 사우스 퍼시픽 카르텔, 라 마노 콘 오호스, 아카플코 독립 카르텔, 그리고 라 바레도라 등의 소규모 집단으로 나뉘게 되고 후자 2개 조직은 다시 상위 카르텔 내부 분열에 개입하게 되었다.\n","멕시코 연방 경찰은 이 카르텔이 완전히 와해된 것으로 여기고 있으며 마지막 지도자였던 엑토르는 더 이상 활동 징후가 없는 도망자 신세에 있는 것으로 알려지고 있다. 미국은 엑토르에게 5백만 불의 현상금을 걸었으며 멕시코 정부는 210만 불의 현상금을 건 상태이다.\n","doc4: 메탈클래드는 멕시코가 연방 및 주 차원에서 인가된 매립장 건설 및 영업 행위에 대해 과달카사르 군 정부가 인가를 지연 및 반려하는 것을 방기하였으며, 이에 따라 북미자유무역협정 11장의 1110조 에 규정된 수용(expropriation)에 상당한 조치가 성립되었다고 주장하였다. 이러한 수용에 상당한 조치임이 인정되면 멕시코 정부는 그에 상응하는 공정 시장가격에 따른 대가를 지불해야 한다.\n","\n","이에 대해 국제투자분쟁해결센터 중재판정부는 앞서 1105조 위반 사유와 같이 멕시코의 행정절차에 있어 투명성이 보장되지 않았으므로 수용에 상당한 조치가 이뤄졌다고 결론을 내렸다. 아울러 유독성 폐기물 매립장 건설 인가권은 연방 정부의 고유 권한이며, 군 정부는 시설의 물리적 결함 등의 사유로만 제한적으로 인가를 거부할 수 있음에도 권한을 남용하여 건설 인가를 반려했다고 지적하였다.\n","\n","반면 캐나다 브리티시컬럼비아주 대법원은 역시 앞서 1105조 위반이 분쟁 범위를 넘어서고 있으므로, 이를 근거로 수용에 상당한 조치가 이뤄졌다는 결론을 내려서는 안된다고 판결하였다.\n","doc5: 메탈클래드 대 멕시코 연방 정부 사건은 국제투자분쟁해결센터(ICSID) 및 캐나다 브리티시컬럼비아주 대법원에서 열린 투자자-국가 소송 사건이다. 미국의 폐기물 관리 업체인 메탈클래드(Metalclad) 사가 멕시코 연방 정부 및 산루이스포토시주 정부로부터 얻은 폐기물 매립장 허가가 관할 과달카사르 군에서 거부되자, 이로 인해 발생한 손해에 대해 멕시코 연방 정부를 상대로 북미자유무역협정 11장에 규정된 투자자 국가 분쟁 해결 제도에 따라 손해배상 청구 소송을 제기하면서 시작되었다. 이에 대해 중재를 맡은 국제투자분쟁해결센터는 멕시코 연방 정부가 북미자유무역협정 1105조에 규정된 상대국 투자자에 대한 공정하고 동등한 처분 원칙의 위반 및 1110조에 규정된 수용에 상당한 조치를 했다는 결론을 내리고, 멕시코 연방 정부가 메탈클래드에게 1,668만 5천 달러를 배상하도록 결정하였다. 멕시코 연방 정부는 역시 북미자유무역협정에 보장된 권리에 따라, 캐나다 브리티시컬럼비아주 대법원에 이 처분의 법적 정당성에 대한 심의를 요청하였다. 이에 대해 캐나다 브리티시컬럼비아주 대법원은 북미자유무역협정 1105조 위반에 대해서는 국제투자분쟁해결센터 중재판정부의 결정 권한 밖임을 지적하였으나, 1110조의 수용 상당 조치에 대해서는 일부 책임을 인정하여 110만 달러가 감액된 1,560만 달러의 배상 판결을 확정했다.\n","\n","위의 docs 중에서만 정보를 근거로 하여, 질문에 답변해 주세요.\n","답변에서 인용한 문서의 내용에는 반드시 [[doc1]], [[doc2]], ... 형식으로 인용 표시를 해주세요.\n","추론이나 지어내는 답변은 삼가주시고, docs에 명시적으로 나타난 내용만 인용해 주세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","멕시코 연방 정부는 메탈클래드에게 1,668만 5천 달러를 배상하도록 결정하였으며, 이후 캐나다 브리티시컬럼비아주 대법원에서 이 금액이 110만 달러 감액되어 최종적으로 1,560만 달러의 배상 판결이 확정되었습니다[[doc5]].<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","\n"]}],"source":["# Llama-3, Llama-VARCO 등 최신 챗봇 계열 모델에서 여러 메시지(system/user/assistant 등)를 채팅 프롬프트 형식의 한 줄 텍스트로 자동 변환\n","# 첫 번째 학습 데이터의 messages 리스트(여러 역할별 대화)를 입력값으로 사용\n","# tokenize=False: 변환 결과를 숫자 토큰이 아니라 **사람이 읽을 수 있는 텍스트(문자열)**로 반환.\n","# add_generation_prompt=False: 모델에게 \"여기서부터 답변을 생성하라\"는 추가 프롬프트를 붙이지 않습니다.\n","text = tokenizer.apply_chat_template(\n","    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",")\n","print(text)"]},{"cell_type":"markdown","metadata":{"id":"7Uog3EGSVrRl"},"source":["## LoRA Finetuning\n","\n","**LoRA란?**\n","\n","* LoRA는 \\*\\*\"Low-Rank Adapter(저랭크 어댑터)\"\\*\\*의 줄임말입니다.\n","* 거대한 대형언어모델(LLM)의 **전체 파라미터를 일일이 미세조정(파인튜닝)하지 않고**,\n","  **딱 필요한 핵심 부분만 저렴하게 빠르게 학습**하는 최신 파인튜닝 방법입니다.\n","\n","**왜 LoRA가 등장했을까?**\n","\n","* GPT, Llama, DeepSeek 같은 대형언어모델은 **파라미터(매개변수) 수가 수십억\\~수조 개**나 됩니다.\n","* 이런 모델을 파인튜닝하려면 **막대한 GPU 메모리와 시간, 저장 공간**이 필요합니다.\n","* 하지만, 실제로 특정 태스크에 맞게 모델을 조정할 때 **전체를 다 바꿀 필요가 없습니다.**\n","* 대부분의 정보는 기존 모델에 이미 들어있고,\n","  **특정 입력(질문)과 특정 출력(답변)의 관계만 살짝 조정**해주면 충분합니다.\n","\n","**LoRA의 원리**\n","\n","* 기존 대형 모델의 핵심 연산(주로 \"곱셈\" 부분)에\n","  \\*\\*작고 얇은 \"보조 네트워크(어댑터 레이어)\"\\*\\*를 덧붙입니다.\n","* 전체 모델은 거의 건드리지 않고,\n","  **이 어댑터 레이어의 파라미터만 새로 추가해서 학습**합니다.\n","* 학습이 끝나면,\n","\n","  * 원본 모델은 그대로\n","  * 어댑터(작은 추가 파라미터)만 별도로 저장하면 끝!\n","* 추론할 땐 **원본 모델 + LoRA 어댑터**를 합쳐서 쓸 수 있습니다.\n","\n","**LoRA의 장점**\n","\n","* **파인튜닝 비용(시간, 메모리, 저장 용량)이 압도적으로 절약**됩니다.\n","* 7B, 13B, 70B 등 대형 모델도\n","  **일반 GPU(24GB/48GB)로도 쉽게 파인튜닝**이 가능합니다.\n","* **동일한 원본 모델에 다양한 LoRA 어댑터만 바꿔 끼우며\n","  다양한 분야별 파인튜닝 결과를 쉽게 쓸 수 있습니다.**\n","\n","**LoRA와 기존 방식의 비교**\n","\n","* **기존 파인튜닝:**\n","  전체 파라미터(수십\\~수백 GB)를 새로 저장/관리/학습 → 비효율적\n","* **LoRA:**\n","  원본은 그대로 두고,\n","  변화가 필요한 부분(수 MB\\~수십 MB)만 별도로 학습/저장\n","\n","\n","\n","**실전에서의 활용 예시**\n","\n","* 번역 LoRA, 요약 LoRA, 감정분석 LoRA 등\n","  **하나의 원본 모델에 여러 용도별 어댑터를 저장/관리**할 수 있습니다.\n","* **A100 80GB, 3090, T4 등 다양한 GPU 환경에서도\n","  고성능 LLM 튜닝이 매우 쉽게 가능합니다.**\n","\n","**정리:**\n","LoRA는\n","“LLM의 힘은 그대로,\n","비용/시간/메모리/유지보수는 최소로”\n","파인튜닝을 할 수 있게 해주는\n","AI 실무에서 가장 중요한 기법 중 하나입니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRIieDzQkcTs"},"outputs":[],"source":["# PEFT 라이브러리에서 LoRA 어댑터의 구조와 동작 방식을 설정하는 클래스를 가져옵\n","from peft import LoraConfig\n","\n","# LoRA 어댑터를 어떻게 적용할지 상세하게 지정하는 설정 객체\n","peft_config = LoraConfig(\n","        lora_alpha=32,\n","        lora_dropout=0.1,\n","        r=8,\n","        bias=\"none\",\n","        target_modules=[\"q_proj\", \"v_proj\"],\n","        task_type=\"CAUSAL_LM\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"BR53QuzkVrRn"},"source":["### LoRAConfig 주요 매개변수\n","\n","| 매개변수                | 의미/역할                                         | 주요 옵션·예시                                                                                                             |\n","| ------------------- | --------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |\n","| **lora\\_alpha**     | LoRA 어댑터의 학습 스케일 팩터(변화 강도). 값이 크면 학습 변화가 완만해짐 | 16, 32, 64 등                                                                                                         |\n","| **lora\\_dropout**   | 어댑터에만 적용되는 드롭아웃 확률. 과적합 방지                    | 0.05, 0.1 등                                                                                                          |\n","| **r**               | LoRA 어댑터의 랭크(정보량/두께). 값이 크면 더 많은 정보, 메모리 사용↑  | 8, 16, 32 등                                                                                                          |\n","| **bias**            | 기존 모델의 bias 파라미터도 LoRA로 튜닝할지 여부               | \"none\"(권장), \"all\"                                                                                                    |\n","| **target\\_modules** | LoRA를 어떤 레이어(부분)에 적용할지 지정                     | \"q\\_proj\", \"v\\_proj\", \"o\\_proj\", \"up\\_proj\" 등<br>(모델 구조에 따라 다름)                                                      |\n","| **task\\_type**      | LoRA가 적용될 문제 유형(파인튜닝 목적)                      | \"CAUSAL\\_LM\"(생성), \"SEQ\\_CLS\"(분류),<br>\"SEQ\\_2\\_SEQ\\_LM\"(번역/요약),<br>\"TOKEN\\_CLS\"(토큰분류),<br>\"QUESTION\\_ANSWERING\"(질의응답) |\n","\n","\n","#### target\\_modules 주요 옵션 예시\n","\n","* `\"q_proj\"`: Attention의 Query projection, 효율/성능 균형, 기본 선택\n","* `\"v_proj\"`: Attention의 Value projection, 기본 선택\n","* `\"k_proj\"`, `\"o_proj\"`: 필요 시 성능 추가용\n","* `\"up_proj\"`, `\"down_proj\"`, `\"gate_proj\"`: FFN 계열, 모델 구조/실험 목적 따라 추가\n","\n","#### task\\_type 주요 옵션 예시\n","\n","* `\"CAUSAL_LM\"`: 텍스트 생성(챗봇, GPT류)\n","* `\"SEQ_CLS\"`: 문장/문단 분류(감정, 카테고리)\n","* `\"SEQ_2_SEQ_LM\"`: 입력→출력 변환(번역, 요약 등)\n","* `\"TOKEN_CLS\"`: 단어 단위 분류(개체명 인식 등)\n","* `\"QUESTION_ANSWERING\"`: 질문-정답 위치 예측(문서 내 Q\\&A)\n","\n","\n","**정리:**\n","이 매개변수들은\n","**LoRA 어댑터의 구조, 적용 범위, 정보량, 목적을 한 번에 결정하는\n","핵심 설정값**입니다.\n","(실제 모델/태스크/실험 목표에 맞게 조합해서 사용하세요.)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5s1MkN_kdBH"},"outputs":[],"source":["# TRL(Transformers Reinforcement Learning) 라이브러리에서 SFT(지도 미세조정) 실험의 각종 설정을 담을 수 있는 클래스\n","from trl import SFTConfig\n","\n","max_seq_length=8192  # 한 입력(프롬프트+정답)에 허용되는 최대 토큰 길이\n","\n","# 아래 옵션들을 한 번에 묶어 학습 전체 설정을 관리\n","args = SFTConfig(\n","    output_dir=\"llama3-8b-rag-ko\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=2,\n","    gradient_checkpointing=True,\n","    optim=\"adamw_torch_fused\",\n","    logging_steps=10,\n","    save_strategy=\"steps\",\n","    save_steps=50,\n","    bf16=True,\n","    learning_rate=1e-4,\n","    max_grad_norm=0.3,\n","    warmup_ratio=0.03,\n","    lr_scheduler_type=\"constant\",\n","    push_to_hub=False,\n","    remove_unused_columns=False,\n","    dataset_kwargs={\"skip_prepare_dataset\": True},\n","    report_to=[],\n","    max_seq_length=max_seq_length,\n","    label_names=[\"labels\"],\n",")"]},{"cell_type":"markdown","metadata":{"id":"pccoZxMkVrRo"},"source":["### SFTConfig 주요 매개변수\n","\n","| 매개변수                                | 설명                                                  | 주요 예시 값/옵션                              |\n","| ----------------------------------- | --------------------------------------------------- | --------------------------------------- |\n","| **output\\_dir**                     | 결과 모델/로그를 저장할 경로 또는 저장소 ID                          | `\"./results\"`                           |\n","| **num\\_train\\_epochs**              | 전체 데이터를 몇 번 반복 학습할지(에포크 수)                          | `3`, `5`                                |\n","| **per\\_device\\_train\\_batch\\_size** | 각 GPU(디바이스)에서 한 번에 입력할 데이터 수(배치 크기)                 | `2`, `4`, `8`                           |\n","| **gradient\\_accumulation\\_steps**   | 여러 미니배치를 모아 한 번에 업데이트(실질 배치 크기 키우기)                 | `1`, `2`, `4`                           |\n","| **gradient\\_checkpointing**         | 메모리 절약 기능(필요할 때만 중간 계산값 저장)                         | `True`, `False`                         |\n","| **optim**                           | 최적화 알고리즘(학습 방법)                                     | `\"adamw_torch_fused\"`                   |\n","| **logging\\_steps**                  | 몇 step마다 로그를 출력할지                                   | `10`, `50`, `100`                       |\n","| **save\\_strategy**                  | 모델 저장 방식(주기)                                        | `\"steps\"`, `\"epoch\"`                    |\n","| **save\\_steps**                     | 몇 step마다 모델을 저장할지                                   | `50`, `100`                             |\n","| **bf16**                            | bfloat16 연산 사용(GPU 메모리 절약)                          | `True`, `False`                         |\n","| **learning\\_rate**                  | 파라미터 업데이트 속도(학습률)                                   | `1e-4`, `5e-5`                          |\n","| **max\\_grad\\_norm**                 | 그래디언트 클리핑 임계값(학습 안정화)                               | `0.3`, `1.0`                            |\n","| **warmup\\_ratio**                   | 워밍업 단계 비율(초기 학습률 천천히 증가)                            | `0.03`, `0.1`                           |\n","| **lr\\_scheduler\\_type**             | 학습률 조정 방식                                           | `\"constant\"`, `\"linear\"`                |\n","| **push\\_to\\_hub**                   | 학습 결과를 Hugging Face Hub로 업로드할지 여부                   | `True`, `False`                         |\n","| **hub\\_model\\_id**                  | 업로드할 Hugging Face Hub 저장소 ID                        | `\"nugunaai/llama3-8b-news-analyzer-ko\"` |\n","| **hub\\_token**                      | Hugging Face Hub 인증 토큰 사용 여부                        | `True`                                  |\n","| **remove\\_unused\\_columns**         | 학습에 안 쓰는 데이터 컬럼 자동 제거 여부                            | `True`, `False`                         |\n","| **dataset\\_kwargs**                 | 데이터셋 추가 옵션(딕셔너리 형태)                                 | `{\"skip_prepare_dataset\": True}`        |\n","| **report\\_to**                      | 학습 로그를 기록할 대상(예: wandb, tensorboard, 빈 리스트면 기록 안 함) | `[]`, `[\"wandb\"]`                       |\n","| **max\\_seq\\_length**                | 한 입력에 허용되는 최대 토큰(단어) 수                              | `2048`, `4096`, `8192`                  |\n","| **label\\_names**                    | Trainer가 label로 인식할 컬럼명                             | `[\"labels\"]`                            |\n"]},{"cell_type":"markdown","metadata":{"id":"rWXffnOoVrRo"},"source":["### data\\_collator 함수\n","\n","* 미니배치(batch) 데이터를 모델이 바로 학습할 수 있는 형태(토큰·마스크·정답)로 변환합니다.\n","* 특히 아래와 같은 LLaMA-3 채팅 포맷을 쓸 때,\n","  “어디까지가 질문/어디서부터가 답변(assistant)인지”를 정확히 구분해서\n","  모델이 정답(답변 부분)만 학습하도록 레이블을 지정합니다.\n","\n","#### LLaMA-3 채팅 포맷\n","\n","* LLaMA-3 채팅 포맷은\n","  **LLaMA-3 계열 챗봇 모델이 대화 내용을 이해하고 답변할 수 있도록 만들어진 입력 데이터 구조**입니다.\n","* 여러 역할(시스템, 유저, 어시스턴트)의 메시지를\n","  **특별한 토큰과 구조**로 묶어서 하나의 프롬프트로 합치는 방식입니다.\n","\n","\n","##### 구조 예시\n","\n","아래와 같이 대화 흐름을 명확히 구분하는 토큰들이 사용됩니다:\n","\n","```\n","<|begin_of_text|>\n","<|start_header_id|>system<|end_header_id|>\n","[시스템 역할 지침]<|eot_id|>\n","<|start_header_id|>user<|end_header_id|>\n","[유저 질문]<|eot_id|>\n","<|start_header_id|>assistant<|end_header_id|>\n","[모델의 답변]<|eot_id|>\n","```\n","\n","* `<|begin_of_text|>` : 전체 프롬프트의 시작을 알리는 토큰\n","* `<|start_header_id|>role<|end_header_id|>` : 각 메시지의 역할 구분(시스템, 유저, 어시스턴트 등)\n","* 각 메시지 끝에 `<|eot_id|>` : 하나의 메시지 블록이 끝났음을 알림\n","\n","#### 왜 이 포맷이 필요할까?\n","\n","* 모델이 **“어디까지가 시스템 안내, 어디서부터가 유저 질문, 어디서부터가 답변인지”**\n","  정확하게 파악할 수 있습니다.\n","* 여러 턴(turn)의 대화가 이어질 때도\n","  메시지 경계를 명확히 구분해 혼동 없이 맥락을 유지할 수 있습니다.\n","* LLaMA-3 계열 모델은 이런 포맷으로 학습되어 있기 때문에\n","  **실전 파인튜닝/추론 시에도 반드시 이 구조로 입력해야**\n","  기대하는 챗봇 성능을 발휘할 수 있습니다.\n","\n","#### 실제 실습/프로그래밍에서\n","\n","* 데이터 전처리에서 여러 메시지를\n","  위와 같은 포맷으로 하나의 텍스트로 합친 후,\n","  토크나이즈해서 모델에 입력합니다.\n","* assistant(정답) 부분만 레이블로 지정하면\n","  모델이 질문-답변 형식 대화에 최적화됩니다.\n","\n","**정리:**\n","LLaMA-3 채팅 포맷은\n","“모델이 다양한 역할의 대화 맥락을 혼동 없이 이해하고\n","최적의 답변을 내놓을 수 있도록\n","특별한 토큰 구조로 입력을 포맷팅하는 방식”입니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzZXZRRZkgUk"},"outputs":[],"source":["def data_collator(batch):\n","\n","    # 미니배치(batch) 데이터를 저장할 딕셔너리\n","    # \"input_ids\": 각 샘플의 숫자 토큰 시퀀스(모델 입력)\n","    # \"attention_mask\":\tinput_ids에서 실제 데이터는 1, 패딩은 0으로 구분(모델이 무시해야 할 부분 표시)\n","    # \"labels\":\t학습 정답(정답이 아닌 위치는 -100, assistant(답변) 구간만 정답 토큰 값으로 채움)\n","    new_batch = {\n","        \"input_ids\": [],\n","        \"attention_mask\": [],\n","        \"labels\": []\n","    }\n","\n","    # 미니배치(batch) 안에 있는 각 예시 데이터를 하나씩 처리\n","    for example in batch:\n","        messages = example[\"messages\"]                   # 예시 데이터에서 메시지 리스트(system, user, assistant 등)를 꺼냄.\n","\n","        # LLaMA 3 채팅 템플릿 적용 (시작 토큰 포함)\n","        prompt = \"<|begin_of_text|>\"                                                        # 전체 프롬프트 텍스트의 시작에 특별 토큰을 넣음. (LLaMA-3 채팅 포맷에서 전체 대화의 시작을 알림)\n","        for msg in messages:                                                                # 각 메시지(시스템, 유저, 어시스턴트 등)에 대해 아래 작업을 반복\n","            role = msg[\"role\"]                                                              # 메시지의 역할(예: \"system\", \"user\", \"assistant\")을 꺼냄.\n","            content = msg[\"content\"].strip()                                                # 메시지 본문(내용)을 꺼내고, 앞뒤 공백을 제거.\n","            prompt += f\"<|start_header_id|>{role}<|end_header_id|>\\n{content}<|eot_id|>\"    # 각 메시지를 LLaMA-3 채팅 포맷에 맞춰 특별 토큰으로 감싸 하나의 프롬프트 문자열로 이어붙임.\n","                                                                                            # <|start_header_id|>역할<|end_header_id|> 각 메시지의 역할(시스템/유저/어시스턴트) 표시\n","                                                                                            # \\n내용<|eot_id|> 실제 메시지 내용과 그 끝을 나타내는 토큰\n","\n","        text = prompt.strip()   # 완성된 전체 프롬프트 텍스트의 앞뒤 공백을 제거해서 최종적으로 저장\n","\n","\n","        # 프롬프트 전체 텍스트를 모델이 이해할 수 있는 숫자 토큰(input_ids)으로 변환.\n","        tokenized = tokenizer(\n","            text,\n","            truncation=True,             # 입력이 max_seq_length(최대 길이)를 넘으면 자동으로 잘라냄.\n","            max_length=max_seq_length,   # 입력 토큰의 최대 개수를 제한.\n","            padding=False,               # 이 단계에서는 패딩(길이 맞추기)을 하지 않습니다.(나중에 배치에서 패딩함)\n","            return_tensors=None,         # 결과를 일반 파이썬 리스트 형태로 반환.\n","        )\n","\n","        input_ids = tokenized[\"input_ids\"]             # 텍스트가 숫자 토큰 리스트로 바뀐 결과. 예: [128, 5551, 29871, ...] (각 숫자는 단어나 특수토큰에 해당)\n","        attention_mask = tokenized[\"attention_mask\"]   # input_ids에서 실제 데이터(=1), 패딩(=0)을 구분하는 마스크.여기선 모두 1로만 채워짐(아직 패딩이 없으므로).\n","        labels = [-100] * len(input_ids)               # input_ids와 동일한 길이의 리스트를 -100으로 채움. -100은 PyTorch에서 \"이 위치는 손실 계산(학습)에서 무시하라\"는 의미임.\n","                                                       # 이후 assistant(답변) 구간에서만 실제 정답 토큰값으로 바뀜.\n","\n","\n","        # 답변(assistant) 부분이 시작되는 지점의 특수 토큰 문자열. 이 토큰 뒤부터 모델의 정답(레이블)로 사용할 구간이 시작\n","        # assistant_header 문자열을 숫자 토큰 시퀀스로 변환. 나중에 input_ids 안에서 이 시퀀스가 어디에 있는지 찾아 \"정답 시작 위치\"로 사용.\n","        assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n","        assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n","\n","        # 한 메시지(assistant 답변)가 끝났음을 표시하는 특수 토큰 문자열\n","        # eot_token도 숫자 토큰 시퀀스로 변환. 나중에 답변의 끝 위치를 정확히 찾는 데 사용.\n","        eot_token = \"<|eot_id|>\"\n","        eot_tokens = tokenizer.encode(eot_token, add_special_tokens=False)\n","\n","\n","        # input_ids(토큰 시퀀스)에서 assistant(정답) 구간만 정확하게 찾아서 labels에 복사해 모델이 질문-답변 데이터에서“정답(답변 부분)만 학습”하도록 레이블을 세팅.\n","        i = 0\n","        while i <= len(input_ids) - len(assistant_tokens):                 # input_ids 리스트에서 assistant_tokens(=assistant 시작 토큰 시퀀스)가 어디에 있는지 찾기\n","            if input_ids[i:i + len(assistant_tokens)] == assistant_tokens: # 현재 위치(i)부터 assistant 시작 토큰 시퀀스와 정확히 일치하는 부분을 찾으면 아래 실행\n","                start = i + len(assistant_tokens)                          # assistant 답변의 \"실제 내용\"이 시작되는 토큰 위치를 저장.\n","                end = start                                                # start부터 eot_tokens(=답변 끝 토큰 시퀀스)가 처음 나올 때까지 end를 증가시켜 답변의 끝 위치를 찾음.\n","                while end <= len(input_ids) - len(eot_tokens):\n","                    if input_ids[end:end + len(eot_tokens)] == eot_tokens:\n","                        break\n","                    end += 1\n","                for j in range(start, end):                                # 답변(assistant)의 본문 구간을 labels에 복사해서 정답으로 사용. 이 구간만 손실 계산(모델 학습)에 실제로 반영.\n","                    labels[j] = input_ids[j]\n","                for j in range(end, end + len(eot_tokens)):                # eot_tokens(=답변 종료 특수토큰)도 정답에 포함. 모델이 어디서 답변을 끝내야 하는지도 학습\n","                    labels[j] = input_ids[j]\n","                break                                                      # 첫 번째 assistant 구간만 처리하고, 그 뒤는 무시(중복 적용 방지).\n","            i += 1                                                         # 다음 위치로 이동하며 assistant 시작 시퀀스를 계속 탐색.\n","\n","\n","        # 이 샘플의 input_ids, attention_mask, labels를 배치(new_batch)에 저장.\n","        new_batch[\"input_ids\"].append(input_ids)\n","        new_batch[\"attention_mask\"].append(attention_mask)\n","        new_batch[\"labels\"].append(labels)\n","\n","    # 패딩 처리\n","    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])              # 미니배치 안에서 가장 긴 input_ids의 길이를 구함. 배치 내 모든 입력이 이 길이에 맞게 통일될 예정.\n","    for i in range(len(new_batch[\"input_ids\"])):                              # 배치의 각 샘플에 대해 아래 과정을 반복\n","        pad_len = max_length - len(new_batch[\"input_ids\"][i])                 # 현재 샘플의 길이가 max_length보다 짧으면, 부족한 만큼 pad_len을 계산..\n","        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * pad_len)  # input_ids의 끝에 pad_token_id(패딩 토큰)를 pad_len만큼 추가해서 길이를 맞춤.\n","        new_batch[\"attention_mask\"][i].extend([0] * pad_len)                  # attention_mask의 끝에도 0을 pad_len만큼 추가.(패딩된 부분은 0, 실제 데이터는 1).\n","        new_batch[\"labels\"][i].extend([-100] * pad_len)                       # labels의 끝에는 -100을 pad_len만큼 추가.(패딩된 부분은 학습에서 무시).\n","\n","    for k in new_batch:\n","        new_batch[k] = torch.tensor(new_batch[k])                             # 각 리스트를 PyTorch 텐서로 변환해서 모델에 바로 입력할 수 있게 만듦.\n","\n","    return new_batch                                                          # 완성된 미니배치(모든 샘플의 길이가 같고, 텐서 형태로 변환된 딕셔너리)를 반환"]},{"cell_type":"markdown","metadata":{"id":"DoF8_C3zVrRp"},"source":["\n","* **new\\_batch 딕셔너리 준비**\n","\n","  * `input_ids`, `attention_mask`, `labels` 리스트를 만듭니다.\n","\n","* **for example in batch:**\n","\n","  * 미니배치 안에 있는 각 예제를 하나씩 처리합니다.\n","\n","* **프롬프트 합치기 (LLaMA-3 채팅 템플릿)**\n","\n","  * 각 메시지(system/user/assistant)를\n","    `<|start_header_id|>역할<|end_header_id|>\\n내용<|eot_id|>`\n","    형식으로 이어붙입니다.\n","  * 전체 프롬프트는 `<|begin_of_text|>`로 시작합니다.\n","\n","* **토큰화**\n","\n","  * 프롬프트 전체를 tokenizer로 숫자 토큰(=input\\_ids)과 attention\\_mask로 변환합니다.\n","  * labels는 일단 모두 -100으로 채웁니다(학습 무시 구간).\n","\n","* **assistant(정답) 구간 찾기**\n","\n","  * `assistant_header` 토큰 시퀀스를 input\\_ids에서 찾아 시작 위치를 구합니다.\n","  * 시작 이후 처음 나오는 `<|eot_id|>`까지가 답변 구간입니다.\n","  * 그 구간은 labels에 실제 input\\_ids 값을 복사(즉, 정답으로 처리)\n","  * `<|eot_id|>` 토큰도 레이블에 포함시켜 모델이 종료 신호까지 예측하도록 합니다.\n","\n","* **배치 패딩**\n","\n","  * 한 배치 안에서 가장 긴 input\\_ids 길이에 맞춰\n","    input\\_ids: pad\\_token\\_id,\n","    attention\\_mask: 0,\n","    labels: -100\n","    으로 뒷부분을 채웁니다.\n","\n","* **텐서 변환**\n","\n","  * PyTorch 텐서로 바꿔서 모델에 입력할 수 있는 형태로 만듭니다.\n","\n","#### 요약\n","\n","* 이 함수는 “배치 단위 데이터 → LLM 입력용 토큰/마스크/정답”을 한 번에 만들어줍니다.\n","* labels를 assistant(정답) 구간에만 활성화해,\n","  **모델이 질문과 답변이 섞인 긴 텍스트에서\n","  정답 부분만 똑똑하게 학습하도록** 도와줍니다.\n","\n","**정리:**\n","data\\_collator는 LLM 튜닝에서\n","“질문-답변 구조의 데이터를 모델이 잘 이해하고,\n","딱 필요한 부분만 정답으로 맞추게 해주는”\n","필수 전처리 함수입니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a992A7xYknO6","outputId":"db14f46d-31c3-45af-f3fa-41d23a76eb29"},"outputs":[{"name":"stdout","output_type":"stream","text":["batch:\n","input_ids 크기: torch.Size([1, 3388])\n","attention_mask 크기: torch.Size([1, 3388])\n","labels 크기: torch.Size([1, 3388])\n"]}],"source":["example = train_dataset[0]          # 학습 데이터셋에서 첫 번째 예시(example)를 꺼냠.\n","batch = data_collator([example])    # 하나의 예시만 넣어서(리스트로) data_collator 함수를 호출. 결과는 모델이 입력으로 사용할 수 있는 \"미니배치(딕셔너리 형태)\"로 변환\n","\n","print(\"batch:\")\n","print(\"input_ids 크기:\", batch[\"input_ids\"].shape)              # input_ids(숫자 토큰 시퀀스)의 크기를 출력. (배치 크기 1, 시퀀스 길이 3388)\n","print(\"attention_mask 크기:\", batch[\"attention_mask\"].shape)    # attention_mask의 크기를 출력.input_ids와 동일한 shape\n","print(\"labels 크기:\", batch[\"labels\"].shape)                    # labels(정답 토큰 시퀀스)의 크기를 출력, 역시 input_ids와 동일한 shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5i6S015skuJS","outputId":"a6309738-4123-4640-ac3a-bd0a70773b59"},"outputs":[{"name":"stdout","output_type":"stream","text":["input_ids 인코딩 결과:\n","[128000, 128006, 9125, 128007, 198, 65895, 83628, 34804, 56773, 125441, 110714, 117294, 90519, 124338, 82818, 120378, 43139, 11, 41820, 110257, 109760, 19954, 82273, 113760, 127923, 102893, 11, 107536, 117294, 67236, 57575, 73653, 61139, 18918, 106589, 93292, 17835, 55000, 58126, 111964, 44005, 15592, 75086, 27796, 80052, 627, 54059, 54542, 67890, 108308, 18359, 64857, 30446, 30426, 67890, 115061, 92769, 1473, 12, 111964, 34804, 64857, 30446, 30426, 27437, 57575, 107364, 34804, 109842, 19954, 62398, 97237, 73653, 114839, 34983, 92769, 13, 27437, 19954, 108838, 109842, 34804, 58935, 103778, 16582, 109745, 67890, 32179, 96318, 22035, 96677, 51402, 627, 12, 111964, 57575, 59777, 27797, 44005, 115155, 13094, 91786, 33390, 11, 64857, 30446, 30426, 95713, 117294, 21028, 85721, 48424, 7, 103415, 25, 4416, 5349, 16, 21128, 4416, 5349, 17, 30716, 17835, 106589, 93292, 18918, 125538, 34983, 56773, 51402, 627, 12, 27437, 21028, 106248, 27796, 81673, 85721, 48424, 16969, 115489, 61938, 13, 27437, 57575, 59777, 27797, 88525, 115932, 61139, 16969, 111964, 19954, 110097, 88525, 96677, 51402, 627, 12, 111964, 21028, 106589, 93292, 20565, 116654, 117294, 85721, 48424, 18918, 48918, 112469, 88525, 101264, 35495, 11, 107744, 57002, 59777, 27797, 106891, 49706, 28218, 5349, 16, 21128, 4416, 5349, 17, 21128, 48014, 18918, 110097, 34983, 56773, 51402, 627, 12, 107036, 111964, 34804, 109584, 167, 107569, 104508, 18359, 41820, 92245, 13, 128009, 128006, 882, 128007, 198, 103194, 52688, 25, 49208, 243, 30426, 102525, 78453, 101482, 118951, 16969, 52491, 110517, 108661, 54542, 30446, 102244, 118176, 18918, 64432, 57002, 16582, 105771, 55000, 101574, 61415, 1980, 14452, 512, 5349, 16, 25, 101228, 102937, 20565, 106446, 73653, 106943, 220, 20, 93917, 55421, 19954, 104685, 44005, 71685, 254, 111635, 101563, 222, 30446, 20565, 104008, 118951, 18918, 59134, 106687, 63171, 21121, 24486, 107896, 26799, 26799, 12, 100654, 20565, 44690, 102937, 38196, 35, 112574, 111915, 106213, 107308, 220, 868, 33177, 7, 102335, 22035, 108076, 8, 107449, 118861, 113890, 95252, 115878, 105292, 26799, 80816, 108955, 34983, 89881, 110816, 7, 19645, 926, 8, 57575, 105069, 102423, 13447, 13, 3507, 35, 16969, 220, 3753, 22, 100392, 24814, 58260, 242, 234, 109666, 120026, 28617, 67361, 13094, 101266, 29102, 102581, 101436, 118951, 18918, 59134, 106687, 38295, 116, 107387, 94821, 43139, 56773, 17835, 101585, 86351, 100654, 107896, 26799, 26799, 102823, 111426, 49085, 57002, 100654, 118951, 18918, 63171, 44690, 34983, 114881, 13, 112699, 118951, 20565, 3507, 35, 19954, 10997, 115594, 104508, 102423, 110005, 117717, 13094, 114489, 101568, 13, 99901, 20565, 101412, 119866, 56069, 114784, 63207, 73653, 60798, 115932, 63207, 118009, 118951, 16969, 102160, 100981, 93917, 30381, 101272, 56773, 116353, 106891, 115777, 101796, 120155, 114702, 101528, 13, 101852, 101136, 99105, 102477, 18918, 106958, 48765, 101, 109509, 78102, 19954, 10997, 70483, 106413, 124158, 106413, 84136, 18918, 101852, 94772, 107333, 103807, 66338, 34804, 101066, 18359, 59777, 24140, 24486, 71685, 254, 111635, 16969, 101228, 102937, 56773, 50643, 49085, 10997, 70483, 106413, 124158, 106413, 84136, 18918, 67236, 42529, 94772, 111590, 116023, 84377, 118951, 16969, 23955, 106313, 18359, 104441, 101711, 100994, 112469, 48936, 111590, 57519, 34983, 111340, 13, 109298, 112699, 118951, 21028, 111915, 3507, 35, 117906, 103778, 111635, 20565, 358, 6546, 926, 19954, 72043, 58232, 18918, 111529, 24486, 110005, 220, 679, 17, 100392, 220, 806, 100551, 1691, 33177, 101568, 13, 111529, 32428, 7, 55421, 35495, 116504, 445, 29767, 12, 3472, 33, 124800, 113081, 25941, 11, 80307, 45780, 225, 222, 124800, 113081, 25941, 78102, 220, 23, 109017, 101568, 13, 23955, 65950, 48765, 67361, 21028, 106589, 93292, 107054, 124761, 102, 71279, 230, 64189, 100968, 82233, 62398, 111003, 11, 74618, 103843, 107054, 48765, 101, 109509, 13447, 13, 5251, 248, 250, 22289, 115, 24486, 102326, 50643, 20565, 108838, 10997, 70483, 106413, 124158, 106413, 84136, 106612, 87472, 21028, 103966, 24140, 88708, 82068, 28617, 67361, 3844, 4977, 122369, 59134, 106687, 104008, 118951, 20565, 120014, 103430, 90759, 62398, 105453, 123715, 21121, 13447, 13, 117906, 73148, 16969, 3451, 50164, 14, 717, 14, 1806, 24535, 220, 679, 17, 116899, 63171, 21121, 53400, 220, 1806, 125991, 72043, 58232, 117906, 18359, 118183, 52976, 13, 220, 679, 18, 100392, 220, 20, 100551, 605, 33177, 104008, 118951, 81673, 71685, 254, 111635, 20565, 109336, 24486, 72043, 58232, 123486, 110097, 34983, 109580, 220, 18, 80732, 43139, 72043, 58232, 102888, 103079, 64189, 20565, 114702, 33943, 112039, 13, 62398, 109814, 24140, 57519, 102160, 100981, 110998, 29102, 11, 57519, 104176, 41381, 14260, 108922, 102080, 58189, 57519, 104193, 123061, 110903, 41953, 78102, 118951, 81673, 104193, 123061, 101015, 57519, 102436, 101254, 82001, 59777, 56154, 220, 1627, 80732, 13094, 107034, 32428, 101327, 52688, 19954, 62060, 93292, 116768, 52976, 13, 103778, 111635, 16969, 103405, 109521, 111436, 17835, 104008, 118951, 19954, 220, 2790, 108609, 22876, 15, 73653, 104684, 61394, 7, 103168, 220, 20, 93917, 1041, 15, 108609, 55421, 124338, 105519, 89359, 101528, 13, 111915, 122042, 16969, 104008, 118951, 20565, 34514, 5002, 19954, 103807, 66338, 34804, 101066, 18359, 102293, 101930, 122910, 101954, 71685, 254, 111635, 101563, 222, 30446, 21028, 119623, 18359, 101254, 21028, 17835, 67890, 101347, 30426, 115061, 125461, 18918, 39250, 35859, 104828, 16969, 108137, 13, 71685, 254, 111635, 16969, 23955, 109644, 34514, 5002, 81673, 102293, 101930, 114080, 114542, 112734, 33177, 54718, 107988, 116231, 101, 103897, 105, 30027, 120, 220, 18, 93917, 22387, 22, 108609, 55421, 19954, 103807, 66338, 34804, 101066, 18359, 105454, 34804, 101066, 19954, 113470, 103211, 35495, 56773, 41953, 101360, 91786, 13, 103097, 122042, 3396, 108244, 101838, 34804, 104219, 42529, 107651, 13447, 13, 103807, 66338, 34804, 101066, 122733, 102293, 101930, 125156, 57575, 113610, 24486, 104870, 49085, 101532, 108964, 19954, 101852, 101136, 18359, 86503, 54780, 24486, 102160, 42529, 102039, 21028, 66610, 60798, 20565, 86503, 65895, 16582, 105453, 105512, 71685, 254, 111635, 21028, 56773, 41953, 101568, 13, 71685, 254, 111635, 16969, 103807, 66338, 34804, 101066, 18359, 59777, 24140, 101360, 102293, 101930, 24486, 56773, 50643, 20565, 48765, 101, 109509, 14260, 53987, 102, 71279, 230, 64189, 100968, 82233, 48765, 67361, 43139, 3451, 24486, 12, 103542, 109509, 14260, 53987, 102, 71279, 230, 64189, 100968, 82233, 107896, 26799, 109567, 30381, 529, 19954, 23955, 101711, 54780, 42529, 104193, 22035, 66610, 103866, 13094, 65621, 63207, 118009, 104008, 19954, 101852, 101136, 18359, 38295, 120, 108289, 20565, 47782, 105954, 102258, 93917, 101360, 91786, 13, 120451, 112107, 118951, 16969, 71685, 254, 111635, 65677, 127702, 102823, 102326, 50643, 108838, 3451, 101314, 104366, 127702, 529, 17835, 107896, 26799, 109567, 30381, 43139, 126110, 48936, 116464, 13094, 104231, 105771, 107625, 27796, 35495, 91786, 13, 109298, 105292, 26799, 26799, 107988, 118951, 107746, 111652, 119646, 108499, 54059, 104156, 67945, 125718, 117396, 72043, 58232, 99901, 13447, 13, 104219, 42529, 86503, 52688, 19954, 102597, 106213, 104374, 74177, 16969, 220, 21, 100551, 1682, 33177, 105069, 109883, 96717, 122436, 108503, 84656, 61394, 90759, 67236, 100392, 59134, 101738, 109509, 83719, 103778, 13094, 74618, 102837, 13447, 13, 115878, 101347, 100660, 100981, 101577, 120342, 114294, 7, 74736, 1846, 121055, 103386, 100968, 33390, 220, 3753, 22, 93, 1049, 22, 100392, 106478, 122436, 118952, 86351, 220, 9079, 101868, 36609, 127230, 220, 1272, 101868, 34804, 107896, 26799, 108181, 11, 220, 2983, 101868, 34804, 118951, 20565, 107746, 44690, 101528, 13, 220, 1806, 101868, 34804, 3396, 234, 235, 101482, 106977, 21028, 17835, 110154, 118436, 13, 118951, 118408, 107746, 111652, 13094, 220, 1758, 4, 19954, 102786, 54780, 108907, 13, 118768, 220, 679, 15, 93, 679, 17, 100392, 358, 6546, 926, 19954, 99105, 64189, 53400, 72043, 58232, 111529, 21028, 99901, 16969, 118951, 113866, 19954, 101003, 29102, 102893, 116951, 35495, 91786, 13, 3507, 926, 19954, 121250, 102837, 220, 1954, 101868, 72043, 83719, 103778, 13094, 109250, 33229, 109065, 16969, 220, 1313, 101868, 101568, 13, 23955, 36609, 127230, 118951, 20565, 107746, 44690, 24486, 103521, 13094, 220, 717, 60861, 35495, 11, 107896, 26799, 26799, 107746, 44690, 16969, 220, 17, 101868, 19954, 102786, 54780, 108907, 13, 83719, 103778, 13094, 109250, 33229, 109065, 65950, 73653, 116332, 35495, 126515, 107746, 44690, 106304, 13094, 220, 4370, 4, 17835, 121250, 63375, 13447, 13, 105813, 105297, 109814, 107849, 100981, 28617, 67361, 62398, 102517, 101254, 52688, 34804, 1054, 101090, 57139, 101661, 57002, 115878, 101711, 58232, 58232, 103079, 19954, 91586, 25941, 100757, 58260, 236, 254, 7, 108493, 95543, 78244, 14260, 104508, 101974, 116407, 21121, 104193, 22035, 8, 113781, 102326, 50643, 101661, 119504, 102467, 117035, 13094, 104919, 108860, 82001, 102893, 115839, 53400, 105453, 106313, 13094, 104008, 118951, 20565, 112633, 32179, 90759, 96102, 107120, 110670, 102027, 105851, 101438, 33177, 72208, 863, 110917, 108537, 13, 104293, 58189, 169, 115594, 126887, 73836, 17156, 575, 31, 71, 92916, 2234, 916, 104358, 105292, 26799, 26799, 12, 100654, 20565, 44690, 102937, 38196, 35, 340, 5349, 17, 25, 96677, 120758, 17835, 44690, 115837, 21028, 107478, 101096, 26799, 119734, 100027, 13094, 104554, 20565, 56773, 88708, 48936, 119864, 110917, 116090, 24486, 55170, 109687, 13879, 63718, 107849, 30381, 101106, 106064, 111529, 124295, 106673, 101066, 115467, 104008, 100981, 101577, 111070, 79225, 56154, 105131, 110080, 94801, 79225, 101482, 13094, 84656, 35495, 91786, 13, 55170, 109687, 114597, 19954, 62060, 71023, 34983, 102611, 106673, 101066, 115467, 62060, 71023, 18359, 64432, 102249, 24486, 101480, 101577, 111070, 79225, 114333, 29833, 101584, 108609, 55421, 19954, 104685, 44005, 62060, 71023, 101136, 18359, 99105, 24140, 88525, 104352, 48936, 111590, 64432, 126344, 110080, 94801, 18359, 111519, 76242, 246, 21121, 35495, 65621, 72208, 13, 101555, 109687, 13879, 63718, 45618, 101711, 34804, 101066, 65950, 123103, 62060, 71023, 107094, 34804, 114656, 34804, 220, 21218, 15, 108609, 55421, 111297, 17835, 58935, 30381, 112931, 13, 23955, 72043, 101480, 42771, 20565, 64432, 102249, 18359, 61816, 102611, 114656, 34804, 106943, 220, 10568, 15, 108609, 55421, 111297, 17835, 58935, 86157, 112931, 13, 101480, 42771, 116680, 112953, 220, 1313, 33177, 1054, 110882, 106446, 34804, 118574, 104182, 117686, 116283, 101443, 238, 29833, 118318, 863, 35495, 108537, 13, 101555, 109687, 13879, 63718, 101480, 42771, 20565, 97096, 102662, 24486, 64432, 102249, 18359, 110038, 42771, 107455, 54059, 106673, 101066, 65950, 57575, 62060, 71023, 107094, 34804, 114656, 18359, 17196, 248, 22035, 104352, 108302, 101480, 42771, 20565, 62060, 71023, 101136, 19954, 23955, 26799, 102704, 102519, 34983, 57519, 106446, 103738, 32179, 59269, 246, 90759, 108239, 13, 101480, 42771, 16969, 111323, 55170, 109687, 114597, 21028, 112785, 18359, 33229, 63375, 29833, 44966, 107022, 113798, 115115, 59877, 57002, 103131, 18359, 104652, 56154, 44005, 110217, 101532, 18918, 22817, 253, 121969, 13, 101480, 42771, 118408, 34804, 110473, 1054, 109516, 37155, 57002, 103684, 116492, 57575, 111809, 44005, 110217, 101532, 109816, 11, 109296, 54780, 105718, 116492, 107031, 125921, 110080, 94801, 101228, 58232, 18918, 36609, 101103, 90759, 108239, 863, 16969, 39250, 41953, 101568, 13, 34804, 101066, 104210, 29833, 71023, 105247, 67236, 101577, 18359, 63171, 106687, 56069, 106356, 88525, 115768, 64432, 102249, 18359, 61816, 102611, 101480, 42771, 21028, 110080, 94801, 13094, 105411, 105954, 56773, 41953, 101360, 91786, 13, 55170, 109687, 114597, 104965, 103131, 34804, 101066, 116680, 26799, 104210, 1054, 34804, 101066, 34804, 29833, 71023, 106434, 106213, 111636, 90960, 99029, 57002, 43139, 74959, 44005, 72208, 33177, 120429, 103055, 41953, 19954, 101151, 55000, 21121, 110671, 65950, 13447, 863, 101203, 1054, 49706, 101868, 64432, 102249, 18359, 61816, 102611, 101480, 42771, 21028, 103135, 48936, 863, 110917, 109012, 13, 101738, 33390, 101480, 42771, 116680, 112953, 1054, 100981, 42771, 16969, 106673, 101066, 65950, 123103, 84696, 34804, 29833, 71023, 101272, 82068, 107034, 80732, 27796, 81673, 29833, 71023, 62060, 101136, 13094, 74177, 63375, 106673, 101066, 106001, 102681, 115096, 118502, 64432, 102249, 106213, 111636, 108239, 863, 35495, 107625, 107094, 103211, 13, 1054, 24140, 71023, 62060, 101136, 13094, 122499, 17835, 74177, 63375, 102681, 41953, 67236, 101577, 34804, 106673, 101066, 102823, 115877, 91786, 863, 101203, 1054, 34804, 101066, 104210, 23955, 119864, 19954, 101327, 27797, 67945, 71023, 78102, 105642, 106792, 49085, 107973, 112795, 106673, 101066, 102823, 103055, 115096, 74959, 110513, 96102, 126840, 863, 110917, 56773, 41953, 101528, 13, 101555, 109687, 13879, 63718, 121066, 220, 508, 33177, 107849, 30381, 101106, 106064, 111529, 102621, 102077, 23955, 106223, 102704, 49085, 106673, 101066, 102823, 101480, 42771, 19954, 33229, 35495, 102233, 111010, 117193, 115932, 107387, 103405, 35495, 49085, 56773, 114784, 25493, 229, 111282, 108584, 13, 101480, 42771, 118408, 34804, 1054, 34804, 101066, 65950, 123103, 33229, 35495, 102233, 111010, 118502, 90759, 127614, 19954, 37155, 77437, 43139, 122787, 24140, 127369, 11, 117686, 102704, 106673, 101066, 102823, 118574, 103684, 33229, 35495, 102233, 111010, 117193, 115768, 91786, 863, 101203, 1054, 109516, 106673, 101066, 102823, 110038, 42771, 120908, 103686, 42771, 108859, 104423, 101272, 18359, 82273, 113760, 109720, 32428, 107333, 101480, 42771, 19954, 102066, 29102, 103373, 105453, 111590, 39277, 122226, 103894, 119222, 110661, 13447, 863, 35495, 75086, 103777, 101528, 13, 110473, 106673, 101066, 104210, 1054, 56154, 35495, 102233, 107054, 62398, 104685, 23955, 96318, 19954, 73653, 127272, 116654, 72208, 863, 110917, 56773, 41953, 101528, 627, 5349, 18, 25, 96677, 100968, 102525, 25941, 49508, 100968, 105292, 17835, 11, 103236, 18918, 17835, 25941, 11, 102066, 101204, 101164, 49085, 11, 107536, 25493, 239, 101665, 100968, 48765, 101, 29726, 103272, 123899, 101974, 21028, 220, 19, 102193, 38187, 20565, 66610, 102436, 24486, 96677, 103168, 115483, 122108, 103236, 100968, 106674, 101568, 13, 220, 1049, 19, 4056, 220, 1049, 20, 100392, 11, 49508, 100968, 105292, 17835, 48765, 101, 29726, 103272, 34804, 49208, 243, 30426, 102525, 108772, 58189, 64189, 109299, 21028, 96677, 103168, 103678, 102937, 17835, 18918, 103213, 22035, 67525, 107472, 35243, 45780, 230, 120, 57575, 45618, 106223, 17835, 54059, 18918, 106958, 107065, 24486, 102258, 29854, 24486, 117452, 108281, 93917, 102436, 18359, 23955, 104381, 234, 100904, 627, 13094, 66610, 102436, 34804, 5251, 127802, 101438, 54780, 114080, 104706, 43139, 49208, 243, 30426, 102525, 117593, 11, 107849, 93917, 101015, 81673, 127972, 21121, 89359, 19954, 112662, 105292, 83290, 96677, 103168, 102464, 116859, 68611, 123194, 102597, 115489, 61139, 18918, 102558, 120, 96318, 101461, 35495, 106213, 22035, 32179, 16969, 49208, 243, 30426, 102525, 21028, 112110, 123773, 109018, 109562, 44966, 105654, 627, 1049, 24, 100392, 220, 1032, 100551, 19954, 103236, 100968, 106674, 21028, 58083, 102913, 32428, 49508, 100968, 105292, 17835, 20565, 49208, 243, 30426, 102525, 61816, 105297, 67945, 102244, 104657, 34983, 65895, 124295, 103236, 100968, 106674, 67236, 64189, 107031, 25493, 239, 101665, 100968, 48765, 101, 29726, 103272, 123899, 101974, 81673, 49508, 100968, 105292, 17835, 21028, 106287, 29833, 111320, 101954, 91586, 30446, 20565, 100968, 97096, 100933, 25941, 75086, 90759, 101164, 111012, 125701, 101852, 29854, 35243, 45780, 230, 120, 13094, 113610, 52976, 13, 101604, 118472, 103236, 100968, 106674, 34804, 67236, 64189, 101968, 55055, 17835, 33229, 114061, 117887, 30426, 113198, 103236, 100968, 106674, 11, 103959, 96677, 101687, 118003, 74177, 48424, 25941, 11, 49508, 101436, 104236, 102525, 107712, 102365, 103236, 100968, 106674, 11, 107536, 103959, 82818, 101164, 49085, 51440, 122733, 101228, 105633, 101555, 104441, 101353, 43139, 74618, 167, 231, 246, 58901, 98243, 35495, 95415, 26799, 220, 17, 60861, 66610, 102436, 34804, 106327, 59134, 82001, 103236, 100968, 106674, 67236, 64189, 101968, 55055, 19954, 74623, 44966, 102893, 116027, 627, 28313, 243, 30426, 102525, 78453, 101482, 127972, 34804, 23955, 103236, 100968, 106674, 13094, 107123, 120459, 110863, 34983, 53400, 111590, 116864, 35495, 117097, 121385, 126237, 26799, 101574, 101954, 25493, 239, 101665, 113562, 102519, 106751, 114291, 49011, 243, 102252, 20565, 108838, 101703, 105115, 26799, 101327, 42529, 19954, 65621, 111590, 116023, 119073, 91786, 13, 107449, 34804, 25493, 239, 101665, 100968, 102244, 220, 20, 106113, 73653, 102786, 21028, 103055, 57002, 101136, 18359, 105701, 101461, 104429, 49208, 243, 30426, 102525, 118951, 16969, 220, 8848, 73653, 102786, 21028, 103055, 57002, 101136, 18359, 103521, 117132, 101568, 627, 5349, 19, 25, 52491, 110517, 108661, 54542, 118162, 49208, 243, 30426, 102525, 20565, 78453, 101482, 101824, 56773, 103213, 55421, 57575, 59777, 20565, 53400, 102293, 102365, 41953, 103521, 102546, 101824, 101603, 101096, 104652, 82001, 19954, 112107, 104219, 104684, 101436, 56154, 100968, 108362, 118951, 20565, 59777, 123740, 67890, 101347, 101824, 64857, 101103, 44005, 107387, 75908, 21121, 111320, 104429, 11, 23955, 19954, 106725, 108772, 57139, 26799, 101314, 100981, 101577, 109567, 30381, 220, 806, 41953, 21028, 220, 5037, 15, 93917, 91586, 111850, 30381, 53400, 29833, 27797, 5580, 71621, 367, 121055, 59134, 65895, 24486, 66610, 60798, 20565, 102132, 102365, 107994, 35495, 56773, 41953, 105654, 13, 122016, 29833, 27797, 19954, 59134, 65895, 24486, 66610, 60798, 94801, 13094, 127507, 65219, 33390, 49208, 243, 30426, 102525, 118951, 16969, 55925, 19954, 59134, 110685, 44005, 100994, 30381, 119262, 115153, 19954, 122453, 62060, 123740, 67890, 106138, 110513, 108239, 382, 120451, 112107, 115878, 105292, 26799, 80816, 108955, 34983, 89881, 110816, 72043, 58232, 103079, 124767, 16969, 107758, 27796, 220, 5120, 20, 93917, 46810, 101738, 33229, 101314, 81673, 109583, 49208, 243, 30426, 102525, 21028, 121087, 104834, 101532, 19954, 112795, 107896, 80732, 115602, 64432, 41953, 119222, 116154, 34609, 117622, 29833, 27797, 19954, 59134, 65895, 24486, 66610, 60798, 20565, 23955, 167, 97, 226, 106872, 105954, 83719, 103778, 18359, 67236, 108584, 13, 49508, 102275, 61394, 101003, 104841, 33931, 118562, 21121, 101438, 102293, 102365, 41953, 103521, 102546, 59777, 20565, 103131, 34804, 78453, 101482, 118951, 21028, 101254, 101314, 109969, 24486, 112373, 11, 108362, 118951, 16969, 45618, 102546, 21028, 103738, 29102, 82068, 83719, 79053, 122733, 33229, 101314, 17835, 73653, 63171, 24486, 104182, 59777, 123740, 101429, 64189, 48936, 29833, 127406, 109018, 109969, 24486, 18359, 102484, 27797, 83290, 103521, 102546, 59777, 123740, 64857, 22289, 45780, 244, 20541, 35495, 67890, 82068, 105654, 382, 101738, 33390, 111031, 61415, 13447, 108100, 29102, 102199, 30426, 121735, 104221, 126751, 55430, 62060, 101661, 55421, 34804, 124012, 107758, 27796, 220, 5120, 20, 93917, 46810, 39277, 63718, 101968, 108955, 115483, 121255, 112633, 108503, 35495, 36439, 34609, 117622, 11, 117012, 106589, 93292, 17835, 29833, 27797, 19954, 59134, 65895, 24486, 66610, 60798, 20565, 23955, 167, 97, 226, 106872, 105453, 83719, 103778, 18359, 118952, 118135, 96270, 53400, 105954, 106478, 89881, 105654, 627, 5349, 20, 25, 52491, 110517, 108661, 54542, 30446, 62060, 49208, 243, 30426, 102525, 78453, 101482, 118951, 117906, 34804, 115878, 105292, 26799, 80816, 108955, 34983, 89881, 110816, 7, 19645, 926, 8, 101824, 111031, 61415, 13447, 108100, 29102, 102199, 30426, 121735, 104221, 126751, 55430, 62060, 101661, 55421, 57575, 105069, 102423, 107896, 26799, 26799, 12, 100654, 20565, 101228, 102937, 117906, 101568, 13, 107449, 21028, 118562, 21121, 101438, 104019, 107022, 50643, 32428, 52491, 110517, 108661, 54542, 30446, 3269, 22029, 566, 329, 8, 33229, 20565, 49208, 243, 30426, 102525, 78453, 101482, 118951, 101824, 105178, 102268, 107128, 101796, 101665, 30426, 55430, 118951, 123103, 119215, 34804, 118562, 21121, 101438, 102293, 102365, 41953, 108785, 20565, 20565, 93851, 48936, 104219, 104684, 101436, 56154, 100968, 108362, 57575, 101429, 64189, 65219, 26799, 11, 23955, 17835, 59777, 34983, 113610, 24486, 104423, 34983, 19954, 112107, 49208, 243, 30426, 102525, 78453, 101482, 118951, 18918, 59134, 106687, 108772, 57139, 26799, 101314, 100981, 101577, 109567, 30381, 220, 806, 41953, 19954, 111850, 30381, 53400, 107896, 26799, 26799, 109916, 101968, 108955, 125347, 63171, 49085, 19954, 106725, 104423, 34983, 103588, 57002, 105519, 89359, 101228, 102937, 18359, 63171, 21121, 116429, 94821, 107994, 13, 23955, 19954, 112107, 72043, 58232, 18918, 127088, 34804, 115878, 105292, 26799, 80816, 108955, 34983, 89881, 110816, 16969, 49208, 243, 30426, 102525, 78453, 101482, 118951, 20565, 108772, 57139, 26799, 101314, 100981, 101577, 109567, 30381, 220, 5120, 20, 93917, 19954, 111850, 30381, 53400, 126761, 100654, 107896, 26799, 26799, 19954, 102597, 100994, 30381, 101360, 101604, 102278, 24486, 72747, 80816, 102467, 117035, 21028, 46810, 101738, 101824, 220, 5037, 15, 93917, 19954, 111850, 30381, 53400, 29833, 27797, 19954, 59134, 65895, 24486, 66610, 125155, 109012, 16969, 83719, 103778, 18359, 67236, 103889, 11, 49208, 243, 30426, 102525, 78453, 101482, 118951, 20565, 52491, 110517, 108661, 54542, 30446, 102244, 220, 16, 11, 24427, 73653, 220, 20, 101584, 104685, 61394, 18918, 74769, 57002, 16582, 108438, 121469, 105654, 13, 49208, 243, 30426, 102525, 78453, 101482, 118951, 16969, 124012, 108772, 57139, 26799, 101314, 100981, 101577, 109567, 30381, 19954, 64432, 41953, 53400, 109969, 111833, 106725, 11, 111031, 61415, 13447, 108100, 29102, 102199, 30426, 121735, 104221, 126751, 55430, 62060, 101661, 55421, 19954, 23955, 72747, 80816, 21028, 107849, 82068, 37155, 65895, 33931, 19954, 102597, 106213, 21028, 18918, 127296, 105654, 13, 23955, 19954, 112107, 111031, 61415, 13447, 108100, 29102, 102199, 30426, 121735, 104221, 126751, 55430, 62060, 101661, 55421, 34804, 108772, 57139, 26799, 101314, 100981, 101577, 109567, 30381, 220, 5120, 20, 93917, 46810, 101738, 19954, 122115, 16969, 115878, 105292, 26799, 80816, 108955, 34983, 89881, 110816, 72043, 58232, 103079, 124767, 21028, 121469, 109969, 24486, 22817, 244, 94801, 18359, 67890, 82068, 111320, 112804, 11, 220, 5037, 15, 93917, 21028, 29833, 27797, 59134, 65895, 66610, 60798, 19954, 122115, 16969, 116686, 110080, 94801, 18359, 127507, 83290, 220, 5120, 73653, 104685, 61394, 20565, 103185, 106446, 53400, 220, 16, 11, 17698, 73653, 104685, 61394, 21028, 74769, 57002, 106478, 89881, 18359, 103686, 30381, 101528, 382, 82001, 21028, 27437, 72043, 57575, 73653, 61139, 18918, 106589, 93292, 17835, 55000, 58126, 11, 109760, 19954, 111964, 34983, 56773, 51402, 627, 109659, 104449, 57575, 59777, 27797, 24486, 117294, 21028, 109842, 102772, 64857, 30446, 30426, 4416, 5349, 16, 21128, 4416, 5349, 17, 21128, 2564, 106612, 77437, 43139, 59777, 27797, 125538, 18918, 61816, 92769, 627, 103948, 103778, 106593, 67890, 32179, 96318, 16969, 111964, 34804, 107455, 20565, 113161, 35495, 11, 27437, 19954, 104167, 30426, 104182, 111269, 103777, 109842, 73653, 59777, 27797, 34983, 56773, 51402, 13, 128009, 128006, 78191, 128007, 198, 28313, 243, 30426, 102525, 78453, 101482, 118951, 16969, 52491, 110517, 108661, 54542, 30446, 102244, 220, 16, 11, 24427, 73653, 220, 20, 101584, 104685, 61394, 18918, 74769, 57002, 16582, 108438, 121469, 111320, 104429, 11, 111323, 111031, 61415, 13447, 108100, 29102, 102199, 30426, 121735, 104221, 126751, 55430, 62060, 101661, 55421, 57575, 23955, 114656, 13094, 220, 5120, 73653, 104685, 61394, 103185, 106446, 106910, 82273, 102757, 104182, 220, 16, 11, 17698, 73653, 104685, 61394, 21028, 74769, 57002, 106478, 89881, 13094, 103686, 30381, 65219, 13879, 90463, 15873, 5349, 20, 22877, 128009]\n"]}],"source":["# 배치 데이터(batch)에서 첫 번째 예시의 input_ids(숫자 토큰 시퀀스)를 리스트 형태로 출력\n","# 이 리스트는 LLM(언어모델) 입력용으로 변환된 **모든 토큰의 ID(숫자)**로 이루어져 있음.\n","# 각 숫자는 프롬프트 내 단어, 구두점, 특수토큰 등 하나하나에 해당\n","print('input_ids 인코딩 결과:')\n","print(batch[\"input_ids\"][0].tolist())"]},{"cell_type":"markdown","metadata":{"id":"X8CmMwIKVrRq"},"source":["### 토크나이즈된 입력을 다시 텍스트로 디코딩\n","\n","\n","* `tokenizer.decode(...)`\n","\n","  * 숫자로 변환된 input\\_ids(토큰 시퀀스)를 다시 원래의 자연어 텍스트로 변환합니다.\n","  * 즉, \"숫자 토큰 리스트 → 텍스트\"로 복원합니다.\n","\n","* `batch[\"input_ids\"][0].tolist()`\n","\n","  * 방금 만든 배치에서 첫 번째 샘플의 input\\_ids를 리스트 형태로 꺼냅니다.\n","\n","* `skip_special_tokens=False`\n","\n","  * 특별 토큰(예: `<|begin_of_text|>`, `<|start_header_id|>` 등)을 그대로 보이게 합니다.\n","  * 이 옵션이 True면 이런 토큰들이 빠집니다.\n","\n","* `clean_up_tokenization_spaces=False`\n","\n","  * 디코딩 후 공백 등 자동 정리를 하지 않습니다.\n","\n","\n","아래 코드는\n","**모델에 입력되는 숫자 토큰이\n","정확히 어떤 텍스트로 변환되는지 직접 눈으로 확인**할 수 있게 해줍니다.\n","(토큰화·디토큰화가 원하는 대로 동작하는지 실습할 때 매우 유용한 단계입니다.)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TiPjoM1_ku_I","outputId":"02abe18a-0db5-4f45-92c8-93405d45a8ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","input_ids 텍스트 디코딩:\n","<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, 그리고 문서 내에서만 정보를 근거로 하여 답변하는 AI 비서입니다.\n","아래 지침을 반드시 지켜주세요:\n","\n","- 답변은 반드시 docs에서 찾은 내용에 한해서만 작성해주세요. docs에 없는 내용은 추론하거나 지어내지 마세요.\n","- 답변에서 인용하는 부분이 있다면, 반드시 해당 문서의 번호(예: [[doc1]], [[doc2]])로 근거를 표시해 주세요.\n","- docs의 순서와 번호는 중요합니다. docs에서 인용하지 않은 정보는 답변에 포함하지 마세요.\n","- 답변의 근거가 되는 문서 번호를 생략하지 말고, 항상 인용 태그([[doc1]], [[doc2]], ...)를 포함해 주세요.\n","- 모든 답변은 존댓말을 사용하세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n","질문: 멕시코 연방 정부는 메탈클래드에게 얼마를 보상하라고 하였나?\n","\n","docs:\n","doc1: 소송가액만 약 5조원에 달하는 론스타펀드가 한국 정부를 상대로 제기한 투자자-국가소송(ISD)의 첫 심리가 15일(현지시간) 미국 워싱턴 국제투자분쟁해결센터(ICSID)에서 열린다. ISD는 1987년 애플 홍콩법인이 스리랑카 정부를 상대로 낸 것을 시작으로 주로 선진국 투자자들이 개발도상국 정부를 제소해왔다.한국 정부가 ISD에 휘말린 것은 이번이 처음이다. 결과가 미칠 파장이 만만치 않은 만큼 정부는 국무조정실 주도로 태스크포스를 구성했다. 세금 회피를 위해 벨기에 등에 페이퍼컴퍼니를 세운 뒤 외환은행을 인수한 론스타는 소송 주체도 페이퍼컴퍼니를 내세운 것으로 알려져 정부는 이 점을 집중 공략할 것으로 전해졌다.○한국 정부의 첫 ISD 사건론스타가 ICSID에 중재를 신청한 것은 2012년 11월21일이다. 신청인(원고)은 LSF-KEB홀딩스, 스타홀딩스 등 8곳이다. 이들 법인의 근거지는 룩셈부르크 한 곳, 나머지는 벨기에다. 뚜렷한 실체가 없는 페이퍼컴퍼니 형태의 특수목적법인(SPC)을 상대로 한국 정부가 싸워야 한다는 얘기다. 사건번호는 ‘ARB/12/37’. 2012년에 제기된 37번째 중재 사건을 뜻한다. 2013년 5월10일 한국 정부와 론스타가 추천한 중재인을 포함해 모두 3명으로 중재 재판부가 구성됐다. 한덕수 전 국무총리, 전광우·김석동 전 금융위원장 등 정부와 금융계 전직 고위 인사 26명이 증인 신문에 대거 참여한다.론스타는 두 가지 이유로 한국 정부에 46억7900만달러(약 5조1000억원)를 청구했다. 첫 번째는 한국 정부가 HSBC에 외환은행을 매각하려던 론스타펀드의 계획을 고의로 지연시켜 피해를 입혔다는 것이다. 론스타는 이 때문에 HSBC와 매각 협상을 벌일 때보다 훨씬 싼 3조9157억원에 외환은행을 하나은행에 팔았다고 주장하고 있다.두 번째 쟁점은 과세 문제다. 외환은행 등의 매각 과정에서 발생한 양도차익에 세금을 부과한 국세청의 조치가 부당하다는 것이 론스타의 주장이다. 론스타는 외환은행을 인수하고 매각한 주체가 벨기에·룩셈부르크 법인으로 ‘한-벨기에·룩셈부르크 투자협정’에 이중과세 금지 조항이 있는 만큼 한국에 세금을 낼 필요가 없다고 강조하고 있다.이에 대해 정부는 론스타 자회사들이 실체 없는 ‘유령회사’로 투자협정으로 보호할 대상이 아니라고 맞서고 있다.○투자자보다 정부 승률 조금 높아최대 관심사는 중재 결과다. 과세 부문에 대한 심리는 오는 6월29일 열릴 예정이어서 일러야 내년 상반기에 결론이 나온다. 국제연합무역개발회의(UNCTAD)에 따르면 1987~2007년 판정이 내려진 119건 가운데 40건은 투자자가, 42건은 정부가 승소했다. 37건은 쌍방 합의로 끝났다. 정부 측 승률이 35%에 불과하다.하지만 2010~2012년 ICSID에 회부된 중재 신청의 결과는 정부 쪽에 유리하게 나오고 있다. ISID에 올라온 90건 중 결론이 난 사례는 22건이다. 이 가운데 정부가 승소한 건이 12개고, 투자자 승소는 2건에 불과하다. 결론이 난 사례들만 놓고 보면 승소율이 54%로 올라간다. 임병덕 법무법인 한별 고문은 “영미법상 국제중재재판에 에스토펠(禁反言·말바꾸기 금지)이라는 실체법상의 원칙이 광범위하게 적용된다는 점이 한국 정부가 넘어야 할 가장 큰 장애물일 것”이라고 말했다. 박동휘 기자 donghuip@hankyung.com ◆투자자-국가소송(ISD)\n","doc2: 마이크로소프트의 창업자 빌 게이츠가 주목할 기업이라고 평가한 모뉴엘이 법정관리를 신청하자 은행들과 한국무역보험공사 간 책임공방이 일고 있다. 모뉴엘에 대출해준 은행들과 대출을 보증한 무역보험공사가 수천억원에 달하는 대출금을 회수하지 못할 것으로 보이자 책임을 떠넘기고 있는 것.모뉴엘이 시중은행들로부터 대출받은 금액은 6700억원 정도로 추정된다. 이 중 무보가 보증을 해준 금액은 약 3300억원 정도로 추산된다. 무보 관계자는 22일 “보증금액은 공식적으로 아직 밝힐 수 없다”고 말했다.모뉴엘이 무보가 발급한 보증을 담보 삼아 은행들에서 대출받은 금액을 갚지 못하면 무보가 대출금에 이자까지 더해 전액 물어줘야 한다. 무보는 이후 모뉴엘의 제품을 사간 수입 업자를 찾아 구상권을 행사하는 절차를 밟는다. 무보 측은 하지만 “이는 정상적인 상황에서 진행하는 절차이고, 지금과 같은 상황에서는 먼저 책임 소재를 가려야 한다”는 입장이다.은행들은 수출거래 내역을 제대로 파악하지 않고 보증을 해준 무보의 책임이 크다고 주장하고 있다. 모뉴엘 채권은행 관계자들은 “은행은 수출 관련 심사를 서류상으로 확인하는 것일 뿐 현장에선 하기 힘들다”며 “그건 보증을 해준 무보의 역할”이라고 했다.반면 무보 관계자는 “무보는 은행들로부터 받은 수출실적 증명서와 수출 대금이 오간 은행들의 통장을 받아 보증 심사를 한다”고 맞받았다. “수출 대금이 실제로 오간 통장 내역은 은행들이 알고 있다”며 “은행들은 이 기업에 신용대출 등 다른 거래도 하고 있어 은행들이 현장을 확인해야 할 사항”이라고 주장했다.모뉴엘이 지난 20일 법정관리를 신청했지만 이날까지도 은행들이 무보에 사고통지를 하지 않은 것을 두고도 주장이 엇갈렸다. 무보 측은 “은행들로부터 사고통지를 받아야 조사에 정식으로 착수하는데, 아직까지 은행들이 공식적인 사고통지를 하지 않고 있다”며 “이는 은행들이 담보 등을 확보하며 손실을 최대한 줄인 뒤 무보에 알리겠다는 것으로밖에 생각되지 않는다”고 비난했다. 하지만 은행들은 “사고통지는 한 달 이내에만 하면 되는 것”이라고 주장했다.\n","doc3: 마르코스 아르투로, 카를로스, 알프레도, 그리고 엑토르 벨트란 레이바의 4형제가 조직한 마약 범죄 카르텔이다. 2004 ~ 2005년, 아르투로 벨트란은 멕시코 북동부 지역의 마약 운송로를 차지하기 위한 다툼에서 시날로아를 위해 운영한 강력한 암살조직을 이끌었다.\n","이 조직은 뇌물과 협박으로 멕시코 정치, 법조계와 경찰기구에 침투하여 마약 진압작전에 대한 중요 정보를 빼내었고 심지어는 멕시코의 인터폴에도 잠입하였다.\n","2009년 13월에 카르텔의 리더인 아르투로가 멕시코 해병대에게 살해당하자 카르텔 내부에서는 엑토르 벨트란 레이바와 아르투로의 최고 수하였던 에드가르 발데스 비야레알과의 세력 다툼이 발생한다. 동시에 카르텔은 내부 분열로 사우스 퍼시픽 카르텔, 라 마노 콘 오호스, 아카플코 독립 카르텔, 그리고 라 바레도라 등의 소규모 집단으로 나뉘게 되고 후자 2개 조직은 다시 상위 카르텔 내부 분열에 개입하게 되었다.\n","멕시코 연방 경찰은 이 카르텔이 완전히 와해된 것으로 여기고 있으며 마지막 지도자였던 엑토르는 더 이상 활동 징후가 없는 도망자 신세에 있는 것으로 알려지고 있다. 미국은 엑토르에게 5백만 불의 현상금을 걸었으며 멕시코 정부는 210만 불의 현상금을 건 상태이다.\n","doc4: 메탈클래드는 멕시코가 연방 및 주 차원에서 인가된 매립장 건설 및 영업 행위에 대해 과달카사르 군 정부가 인가를 지연 및 반려하는 것을 방기하였으며, 이에 따라 북미자유무역협정 11장의 1110조 에 규정된 수용(expropriation)에 상당한 조치가 성립되었다고 주장하였다. 이러한 수용에 상당한 조치임이 인정되면 멕시코 정부는 그에 상응하는 공정 시장가격에 따른 대가를 지불해야 한다.\n","\n","이에 대해 국제투자분쟁해결센터 중재판정부는 앞서 1105조 위반 사유와 같이 멕시코의 행정절차에 있어 투명성이 보장되지 않았으므로 수용에 상당한 조치가 이뤄졌다고 결론을 내렸다. 아울러 유독성 폐기물 매립장 건설 인가권은 연방 정부의 고유 권한이며, 군 정부는 시설의 물리적 결함 등의 사유로만 제한적으로 인가를 거부할 수 있음에도 권한을 남용하여 건설 인가를 반려했다고 지적하였다.\n","\n","반면 캐나다 브리티시컬럼비아주 대법원은 역시 앞서 1105조 위반이 분쟁 범위를 넘어서고 있으므로, 이를 근거로 수용에 상당한 조치가 이뤄졌다는 결론을 내려서는 안된다고 판결하였다.\n","doc5: 메탈클래드 대 멕시코 연방 정부 사건은 국제투자분쟁해결센터(ICSID) 및 캐나다 브리티시컬럼비아주 대법원에서 열린 투자자-국가 소송 사건이다. 미국의 폐기물 관리 업체인 메탈클래드(Metalclad) 사가 멕시코 연방 정부 및 산루이스포토시주 정부로부터 얻은 폐기물 매립장 허가가 관할 과달카사르 군에서 거부되자, 이로 인해 발생한 손해에 대해 멕시코 연방 정부를 상대로 북미자유무역협정 11장에 규정된 투자자 국가 분쟁 해결 제도에 따라 손해배상 청구 소송을 제기하면서 시작되었다. 이에 대해 중재를 맡은 국제투자분쟁해결센터는 멕시코 연방 정부가 북미자유무역협정 1105조에 규정된 상대국 투자자에 대한 공정하고 동등한 처분 원칙의 위반 및 1110조에 규정된 수용에 상당한 조치를 했다는 결론을 내리고, 멕시코 연방 정부가 메탈클래드에게 1,668만 5천 달러를 배상하도록 결정하였다. 멕시코 연방 정부는 역시 북미자유무역협정에 보장된 권리에 따라, 캐나다 브리티시컬럼비아주 대법원에 이 처분의 법적 정당성에 대한 심의를 요청하였다. 이에 대해 캐나다 브리티시컬럼비아주 대법원은 북미자유무역협정 1105조 위반에 대해서는 국제투자분쟁해결센터 중재판정부의 결정 권한 밖임을 지적하였으나, 1110조의 수용 상당 조치에 대해서는 일부 책임을 인정하여 110만 달러가 감액된 1,560만 달러의 배상 판결을 확정했다.\n","\n","위의 docs 중에서만 정보를 근거로 하여, 질문에 답변해 주세요.\n","답변에서 인용한 문서의 내용에는 반드시 [[doc1]], [[doc2]], ... 형식으로 인용 표시를 해주세요.\n","추론이나 지어내는 답변은 삼가주시고, docs에 명시적으로 나타난 내용만 인용해 주세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","멕시코 연방 정부는 메탈클래드에게 1,668만 5천 달러를 배상하도록 결정하였으며, 이후 캐나다 브리티시컬럼비아주 대법원에서 이 금액이 110만 달러 감액되어 최종적으로 1,560만 달러의 배상 판결이 확정되었습니다[[doc5]].<|eot_id|>\n"]}],"source":["text = tokenizer.decode(\n","    batch[\"input_ids\"][0].tolist(),\n","    skip_special_tokens=False,\n","    clean_up_tokenization_spaces=False\n",")\n","\n","print(\"\\ninput_ids 텍스트 디코딩:\")\n","print(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pD1WvWmkkxPl","outputId":"dc23e487-073e-408c-b6f4-64366c775db4"},"outputs":[{"name":"stdout","output_type":"stream","text":["레이블 인코딩 결과:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 28313, 243, 30426, 102525, 78453, 101482, 118951, 16969, 52491, 110517, 108661, 54542, 30446, 102244, 220, 16, 11, 24427, 73653, 220, 20, 101584, 104685, 61394, 18918, 74769, 57002, 16582, 108438, 121469, 111320, 104429, 11, 111323, 111031, 61415, 13447, 108100, 29102, 102199, 30426, 121735, 104221, 126751, 55430, 62060, 101661, 55421, 57575, 23955, 114656, 13094, 220, 5120, 73653, 104685, 61394, 103185, 106446, 106910, 82273, 102757, 104182, 220, 16, 11, 17698, 73653, 104685, 61394, 21028, 74769, 57002, 106478, 89881, 13094, 103686, 30381, 65219, 13879, 90463, 15873, 5349, 20, 22877, 128009]\n"]}],"source":["# 배치 데이터(batch)에서 첫 번째 예시의 labels(정답 토큰 시퀀스)를 리스트 형태로 출력\n","# 이 리스트는 input_ids와 같은 길이이며, 모델이 실제로 정답을 맞춰야 하는 토큰 위치에만 input_ids 값이 들어있고,\n","# 그 외 구간(질문/프롬프트 등)은 모두 -100으로 채워져 있음\n","print('레이블 인코딩 결과:')\n","print(batch[\"labels\"][0].tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6YMbUx3DXUlo","outputId":"fa53c04f-a1fd-4c3f-ecb3-d75992195a27"},"outputs":[{"data":{"text/plain":["128009"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# <|eot_id|>라는 특별 토큰을 토크나이저로 숫자 토큰 ID로 변환.\n","# add_special_tokens=False 옵션을 통해 추가적인 시작/끝 토큰 없이 딱 해당 특수 토큰만 ID로 인코딩함.\n","# 결과는 input_ids 리스트(길이 1). [0]을 붙이면 실제 <|eot_id|>에 해당하는 숫자 토큰 값을 가져옴.\n","eos_token = tokenizer(\"<|eot_id|>\",add_special_tokens=False)[\"input_ids\"][0]\n","eos_token  # 이 변수는 모델에서 \"한 메시지(assistant 답변)가 끝났다\"는 것을 표시하는 특수 토큰 ID를 담음."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B3WZZZX-kzdf","outputId":"3f4de742-4dc9-4d58-82f7-2a1847f0e4b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","labels 디코딩 결과:\n","멕시코 연방 정부는 메탈클래드에게 1,668만 5천 달러를 배상하도록 결정하였으며, 이후 캐나다 브리티시컬럼비아주 대법원에서 이 금액이 110만 달러 감액되어 최종적으로 1,560만 달러의 배상 판결이 확정되었습니다[[doc5]].<|eot_id|>\n"]}],"source":["# labels(정답 토큰 시퀀스)에서 -100이 아닌 값만 추려서 새로운 리스트를 만듦.\n","# -100은 \"이 위치는 학습/평가에서 무시\"하라는 의미이므로, 실제 답변 구간만 남게 됨.\n","label_ids = [token_id for token_id in batch[\"labels\"][0].tolist() if token_id != -100]\n","\n","# label_ids(정답 토큰 리스트)를 다시 사람이 읽을 수 있는 **텍스트(문자열)**로 변환.\n","decoded_labels = tokenizer.decode(\n","    label_ids,\n","    skip_special_tokens=False,            # 특별 토큰도 그대로 보이게 함.\n","    clean_up_tokenization_spaces=False    # 디코딩 후 공백 등 자동 정리를 하지 않음.\n",")\n","\n","print(\"\\nlabels 디코딩 결과:\")\n","print(decoded_labels)  # 레이블(정답) 시퀀스에서 학습·평가에 사용되는 답변 구간만 텍스트로 복원해 정확하게 원하는 정답만 추출되었는지 검증"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6v5pIeg9k2NC","outputId":"03710ac8-494a-4d67-cb6c-cbaf1166434a"},"outputs":[{"name":"stdout","output_type":"stream","text":["0번 데이터 길이: 3388\n","1번 데이터 길이: 3823\n","\n","배치 처리 후:\n","입력 ID 형태: torch.Size([2, 3823])\n","어텐션 마스크 형태: torch.Size([2, 3823])\n","0번 샘플 어텐션 마스크 합계: 3388\n","1번 샘플 어텐션 마스크 합계: 3823\n","\n","0번과 1번 샘플의 어텐션 마스크가 다른가요? True\n","\n","0번 샘플 어텐션 마스크: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","1번 샘플 어텐션 마스크: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","배치 내 최대 길이: 3823\n","0번 샘플 어텐션 마스크 합계 (실제 토큰 수): 3388\n","1번 샘플 어텐션 마스크 합계 (실제 토큰 수): 3823\n","0번 샘플 어텐션 마스크 1의 개수: 3388\n","0번 샘플 어텐션 마스크 0의 개수: 435\n","1번 샘플 어텐션 마스크 1의 개수: 3823\n","1번 샘플 어텐션 마스크 0의 개수: 0\n"]}],"source":["# 데이터의 최대 길이 제한\n","max_seq_length = 8192\n","\n","# 0번과 1번 데이터의 길이 확인\n","example0 = train_dataset[0]\n","example1 = train_dataset[1]\n","\n","# 개별 길이 확인 (토큰화 후)\n","tokenized0 = tokenizer(\n","    # 전체 처리 과정과 동일하게 전체 대화를 토큰화\n","    \"<|begin_of_text|>\" + \"\".join([f\"<|start_header_id|>{msg['role']}<|end_header_id|>\\n{msg['content'].strip()}<|eot_id|>\" for msg in example0[\"messages\"]]),\n","    truncation=True,\n","    max_length=max_seq_length,\n","    padding=False,\n","    return_tensors=None,\n",")\n","tokenized1 = tokenizer(\n","    # 전체 처리 과정과 동일하게 전체 대화를 토큰화\n","    \"<|begin_of_text|>\" + \"\".join([f\"<|start_header_id|>{msg['role']}<|end_header_id|>\\n{msg['content'].strip()}<|eot_id|>\" for msg in example1[\"messages\"]]),\n","    truncation=True,\n","    max_length=max_seq_length,\n","    padding=False,\n","    return_tensors=None,\n",")\n","\n","print(f\"0번 데이터 길이: {len(tokenized0['input_ids'])}\")\n","print(f\"1번 데이터 길이: {len(tokenized1['input_ids'])}\")\n","\n","# 배치로 처리하여 어텐션 마스크 비교\n","batch = data_collator([example0, example1])\n","print(\"\\n배치 처리 후:\")\n","print(f\"입력 ID 형태: {batch['input_ids'].shape}\")\n","print(f\"어텐션 마스크 형태: {batch['attention_mask'].shape}\")\n","\n","# 각 샘플의 어텐션 마스크 합계 (실제 토큰 수 확인)\n","print(f\"0번 샘플 어텐션 마스크 합계: {batch['attention_mask'][0].sum().item()}\")\n","print(f\"1번 샘플 어텐션 마스크 합계: {batch['attention_mask'][1].sum().item()}\")\n","\n","# 0번 샘플과 1번 샘플의 어텐션 마스크가 다른지 확인\n","masks_different = not torch.equal(batch['attention_mask'][0], batch['attention_mask'][1])\n","print(f\"\\n0번과 1번 샘플의 어텐션 마스크가 다른가요? {masks_different}\")\n","\n","# 어텐션 마스크 패턴 시각화 (처음 20개와 마지막 20개 토큰)\n","print(\"\\n0번 샘플 어텐션 마스크:\", batch['attention_mask'][0].tolist())\n","print(\"1번 샘플 어텐션 마스크:\", batch['attention_mask'][1].tolist())\n","\n","# 배치 내에서 가장 긴 시퀀스 길이 구하기\n","max_length_in_batch = max(len(tokenized0['input_ids']), len(tokenized1['input_ids']))\n","print(f\"\\n배치 내 최대 길이: {max_length_in_batch}\")\n","print(f\"0번 샘플 어텐션 마스크 합계 (실제 토큰 수): {batch['attention_mask'][0].sum().item()}\")\n","print(f\"1번 샘플 어텐션 마스크 합계 (실제 토큰 수): {batch['attention_mask'][1].sum().item()}\")\n","print(f\"0번 샘플 어텐션 마스크 1의 개수: {batch['attention_mask'][0].sum().item()}\")\n","print(f\"0번 샘플 어텐션 마스크 0의 개수: {(batch['attention_mask'][0] == 0).sum().item()}\")\n","print(f\"1번 샘플 어텐션 마스크 1의 개수: {batch['attention_mask'][1].sum().item()}\")\n","print(f\"1번 샘플 어텐션 마스크 0의 개수: {(batch['attention_mask'][1] == 0).sum().item()}\")\n","# 결과 검증: 긴 샘플은 모든 어텐션 마스크가 1이고, 짧은 샘플은 일부만 1이어야 함"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8lWLODFk5U6"},"outputs":[],"source":["# TRL(Transformers Reinforcement Learning) 라이브러리에서 SFT(지도 미세조정)를 위한 Trainer 클래스를 불러옴.\n","# 이 Trainer는 LLM 파인튜닝을 매우 간단하게 관리할 수 있도록 설계된 도구\n","from trl import SFTTrainer\n","\n","# Hugging Face Hub에 모델을 업로드할 때 필요한 로그인 함수\n","from huggingface_hub import login\n","\n","# Hugging Face 계정의 접근 토큰을 사용해 인증을 진행.\n","# 이 과정을 거쳐야 push_to_hub=True로 설정한 경우 모델 업로드가 가능\n","login(token='')\n","\n","# SFTTrainer 객체\n","trainer = SFTTrainer(\n","    model=model,                      # 파인튜닝할 사전학습 LLM\n","    args=args,                        # SFTConfig로 정리한 학습 세팅(하이퍼파라미터, 저장, 허브 옵션 등)\n","    train_dataset=train_dataset,      # 학습 데이터셋\n","    data_collator=data_collator,      # 위에서 만든 data_collator 함수(토큰/마스크/레이블 생성)\n","    peft_config=peft_config           # LoRA 어댑터 설정(파라미터 효율적 파인튜닝 적용)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"DNvpYSSwlPfH","outputId":"0768d67f-3617-4e1b-d447-be516126280c"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [846/846 1:23:32, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>0.770200</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.602100</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.536700</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.472200</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.523300</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.548000</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.479400</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.487300</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.537200</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.475700</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.387800</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.498300</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.483700</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.462900</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.528400</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.518400</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.452300</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.494700</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.450200</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.466000</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.445900</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.498000</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.484400</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.441300</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.460100</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.461400</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.512400</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.511200</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.421200</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.413000</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.433400</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.404100</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.355800</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.392300</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.424500</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.405700</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.438500</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.451100</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.428800</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.370600</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>0.411600</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>0.452500</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>0.447000</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>0.424300</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.386800</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>0.388700</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>0.415100</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>0.398800</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>0.429300</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.448200</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>0.391100</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>0.367200</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>0.413000</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>0.408700</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>0.390100</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>0.344500</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>0.330600</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>0.384000</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>0.328600</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.322800</td>\n","    </tr>\n","    <tr>\n","      <td>610</td>\n","      <td>0.334900</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>0.306800</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>0.292900</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>0.346500</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>0.319400</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>0.337400</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>0.333400</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>0.354200</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>0.297000</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.324700</td>\n","    </tr>\n","    <tr>\n","      <td>710</td>\n","      <td>0.327100</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>0.370400</td>\n","    </tr>\n","    <tr>\n","      <td>730</td>\n","      <td>0.338500</td>\n","    </tr>\n","    <tr>\n","      <td>740</td>\n","      <td>0.381100</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.331000</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.335600</td>\n","    </tr>\n","    <tr>\n","      <td>770</td>\n","      <td>0.373900</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>0.325400</td>\n","    </tr>\n","    <tr>\n","      <td>790</td>\n","      <td>0.367500</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.364000</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>0.371100</td>\n","    </tr>\n","    <tr>\n","      <td>820</td>\n","      <td>0.312000</td>\n","    </tr>\n","    <tr>\n","      <td>830</td>\n","      <td>0.344200</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>0.356400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=846, training_loss=0.41533792004799447, metrics={'train_runtime': 5019.7791, 'train_samples_per_second': 0.673, 'train_steps_per_second': 0.169, 'total_flos': 6.007696178425037e+17, 'train_loss': 0.41533792004799447})"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["# 앞서 준비한 SFTTrainer(지도 미세조정 Trainer) 객체에서 실제 파인튜닝 학습을 시작하는 함수.\n","# args에 설정한 하이퍼파라미터, 데이터셋, data_collator, LoRA 설정 등이 모두 반영되어 전체 학습 과정이 자동으로 진행.\n","# 전체 데이터셋을 주어진 에포크 수만큼 반복 학습\n","# 일정 step마다 로깅 및 체크포인트 저장\n","# validation, early stopping 등 추가 옵션도 내부적으로 지원\n","# 학습이 끝나면 output_dir(또는 hub)에 최종 모델 저장\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFeISu2Wyyeb"},"outputs":[],"source":["# 현재까지 학습된 모델(파라미터, 토크나이저, LoRA 어댑터 등)을 output_dir에 저장.\n","# 이 명령을 실행하면 학습을 중간에 멈추더라도 나중에 이어서 학습하거나, 바로 추론·평가에 활용할 수 있음.\n","# 체크포인트 저장과는 별도로 마지막 모델 상태를 명확히 파일로 남길 때 사용\n","# 저장 위치는 SFTConfig에서 지정한 output_dir\n","# 이 함수 호출 후 **trainer.push_to_hub()**로 Hub 업로드를 연계할 수 있음\n","# 학습을 마친 LLM(및 어댑터 등)을 디스크 또는 클라우드 저장소에 최종적으로 안전하게 저장\n","\n","trainer.save_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SElmediDmQJZ","outputId":"edb1be86-e9b3-4b10-d133-1c6e8f171b7c"},"outputs":[{"name":"stderr","output_type":"stream","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/nugunaai/llama3-8b-rag-ko/commit/04dcba27953beca1ef5168b872bf9556008aa791', commit_message='Upload LoRA adapter', commit_description='', oid='04dcba27953beca1ef5168b872bf9556008aa791', pr_url=None, repo_url=RepoUrl('https://huggingface.co/nugunaai/llama3-8b-rag-ko', endpoint='https://huggingface.co', repo_type='model', repo_id='nugunaai/llama3-8b-rag-ko'), pr_revision=None, pr_num=None)"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# 방금 저장한 모델(및 필요한 부속 파일들)을 **Hugging Face Hub(클라우드 저장소)**로 업로드하는 명령.\n","# commit_message는 업로드할 때 남기는 설명(버전 관리 기록용)\n","# 이 명령을 실행하면 누구나(또는 권한이 있는 사용자만) 인터넷에서 모델을 쉽게 불러오거나, 공유, 재사용할 수 있음.\n","# 업로드 대상은 SFTConfig에서 지정한 hub_model_id.\n","\n","# trainer.push_to_hub(commit_message=\"Upload LoRA adapter\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KYytly-qP5f"},"outputs":[],"source":["# 평가(테스트)용 프롬프트(입력)와 정답(라벨) 리스트를 각각 만듦.\n","prompt_lst = []\n","label_lst = []\n","\n","for messages in test_dataset[\"messages\"]:                                                            # 테스트 데이터셋의 각 샘플에서 messages(대화 내용 리스트)를 하나씩 꺼냄.\n","    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)      # messages 리스트를 LLaMA-3 채팅 포맷 텍스트로 변환\n","    input = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[0] + \\\n","        '<|start_header_id|>assistant<|end_header_id|>\\n'                                            # assistant(정답) 답변이 시작되기 전까지의 텍스트를 프롬프트로 만듦.\n","    label = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[1].split('<|eot_id|>')[0]  # assistant 구간의 시작 이후부터 <|eot_id|>(답변 끝) 이전까지의 텍스트만 추출해 정답으로 만듦.\n","    prompt_lst.append(input)                                                                         # 입력(프롬프트) 텍스트를 리스트에 저장\n","    label_lst.append(label)                                                                          # 정답(답변) 텍스트를 리스트에 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"39xlQV714Ynu","outputId":"8fbcd619-74de-41af-d3cc-5d11d1f5096f"},"outputs":[{"name":"stdout","output_type":"stream","text":["<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","\n","당신은 주어진 여러 문서(docs)를 바탕으로, 사용자의 질문에 최대한 정확하게, 그리고 문서 내에서만 정보를 근거로 하여 답변하는 AI 비서입니다.\n","아래 지침을 반드시 지켜주세요:\n","\n","- 답변은 반드시 docs에서 찾은 내용에 한해서만 작성해주세요. docs에 없는 내용은 추론하거나 지어내지 마세요.\n","- 답변에서 인용하는 부분이 있다면, 반드시 해당 문서의 번호(예: [[doc1]], [[doc2]])로 근거를 표시해 주세요.\n","- docs의 순서와 번호는 중요합니다. docs에서 인용하지 않은 정보는 답변에 포함하지 마세요.\n","- 답변의 근거가 되는 문서 번호를 생략하지 말고, 항상 인용 태그([[doc1]], [[doc2]], ...)를 포함해 주세요.\n","- 모든 답변은 존댓말을 사용하세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","질문: 현대미술에서 일본의 모노하 운동과 한국의 추상 표현주의가 어떤 방식으로 서로 영향을 주고 있으며, 이 두 장르의 작가들이 사용하는 재료와 주제는 어떻게 다르게 나타나는지 비교해 주실 수 있습니까?\n","\n","docs:\n","doc1: 제2차 세계대전에 패한 직후 일본 화단에는 형태를 극도로 단순하게 표현하는 서구의 ‘미니멀 아트’를 동양적으로 재해석한 전위미술 장르 ‘모노하(物派)’가 등장했다. 미니멀 아트가 감정을 철저히 배제한 채 작업하는 ‘결과의 예술’이라면, 모노하는 사물을 있는 그대로 놓아두는 것을 통해 사물과 공간, 위치, 상황, 관계 등에 접근하는 예술이다.구체적으로는 돌, 철, 나무 등 재료의 성격을 그대로 드러낸다든가 그림의 원재료인 점, 선, 면에 주목하는 식이다. 일본에서 활동한 고(故) 곽인식 화백(1919~1988)은 이 같은 모노하 운동을 주도하며 1960~1970년대 한국과 일본의 현대미술에 큰 영향을 미쳤다.일본 모노하의 선구적 작가인 곽 화백의 예술세계를 재조명하는 ‘물질과 빛의 파노라마’ 전이 오는 30일까지 서울 사간동 갤러리 현대 본관에서 열린다. 1919년 대구에서 태어난 곽 화백은 10세 때 일본으로 건너가 일본미술학교를 졸업한 뒤 ‘물질과 현상’에 대한 독특한 해석을 담은 독자적인 작품세계를 구축했다. 그는 유리를 의도적으로 깨 표면의 망을 형성하는 등 1960년대까지 전위예술을 비롯한 다양한 장르의 현대미술을 선보였다. 나무와 쇠, 유리 등의 물성에 대한 관심을 화면과 형상에 반영하는 작업은 이우환 등 당대 젊은 작가들에게 영향을 줬다.이번 전시에는 1970년대 중반부터 1980년대에 걸쳐 제작된 회화 20여점이 나왔다. 작가의 말년에 제작돼 완숙미가 돋보이는 작품들이다. 종이에 일정한 크기의 쌀알 모양 색점 또는 묵점을 마치 낙엽이 쌓이듯 끊임없이 쌓아올리면서 한지의 물성에 대한 탐구를 한 것이 특징. 먹을 사용한 검은색 또는 보라, 노랑, 푸른빛의 단색 터치로 찍어간 붓 자국이 아롱져 독특한 공간과 빛의 세계를 연출한다. 규모는 크지 않아도 미술사적 업적에 비해 제대로 평가받지 못한 작가의 존재를 일깨우는 뜻깊은 자리다.조정열 갤러리현대 대표는 “일본 현대미술의 큰 줄기를 이룬 모노하 운동에서 선구적 존재로 손꼽힌 곽 화백은 상업적이지 않은 작가라는 이유로 거의 잊힌 존재가 됐다”며 “앞으로도 다양한 전시를 통해 그의 작업에 대한 관심을 높이고 예술적 위상도 재조명할 계획”이라고 말했다.바로 옆 갤러리현대 신관과 두가헌 갤러리에서 열리는 한국 단색화(모노크롬)의 대표주자 정상화 화백의 개인전과 함께 감상하면 최근 국내외에서 불고 있는 ‘모노크롬 열풍’을 이해하는 데 도움이 될 만하다. (02)2287-3591\n","doc2: 고희(古稀)를 맞아서도 날마다 화구 앞에서 붓질하는 최홍순 화백에게 그림 작업은 ‘분노도 불안도 공포도 모두 색과 선율로 되돌려 보내기 위한 행위’다. 한국적 추상표현주의 화가로 알려진 그가 5~11일 서울 인사동 라메르갤러리에서 고희전을 연다. 서울대 미대를 졸업하고 휘문고와 성심여고, 선화예술학교에서 제자들을 길러온 최 화백은 캔버스에 빛과 생명, 우주를 색의 물결로 묘사하는 화가로 잘 알려져 있다. 경기 구리 덕소의 장욱진 화백 집에 드나들며 한국의 미의식을 익혔다. ‘생명의 율동’을 주제로 한 이번 전시에는 1970~1980년대 풍경 구상 작품부터 2000년 이후의 추상 표현주의 작품까지 30년간 그린 작품을 연대별로 나눠 100여점을 건다. 모든 사물에는 다양한 색깔의 영혼이 있다고 생각한 그는 작품 소재를 젊은 시절 체험한 민화적 이미지에서 건져 올린다. 달과 해, 연날리기, 지신밟기 같은 것이 언제나 화면 한가운데에 핵심 모티브로 등장한다. “민화는 형식이 자유롭고 표현이 활달해 그동안 꾸준히 벤치마킹해 온 장르입니다. 활활 타오르는 생명력도 매력적이고요. 꽉 짜인 구도 속에 기교적이며 감각적 표현으로 가득한 일본 그림에 비해 해학적이고 풍부한 상상력이 넘쳐나잖아요.”그의 작품에서는 음악적인 소재도 아주 특별한 메타포로 다가온다. 작품에 ‘생(生)·률(律)’의 제목을 붙인 것도 음악에 대한 추억 때문이다. 그래서 그의 작품 속에는 자유분방한 음악의 특성이 녹아 있다.“생과 율의 주제는 이른바 미술과 음악의 융합을 꾀한 겁니다. 클래식 음악은 제 그림에 하모니를 제공하는 수단이죠. 작업실에서 듣는 드보르자크, 베토벤, 모차르트 등의 화음은 늘 제 조형성을 깨워주거든요. 음악을 통해 대상을 단순화하고 생략합니다.”최 화백은 “회화를 음악이 지닌 우아함의 수준으로 끌어올리고 싶다”며 “2000년 이후 풍경과 인물 등 대상의 형태를 깨뜨려 색면 속에 추상적으로 담아냈다”고 설명했다. 그의 작품에서 자유분방한 붓질은 두텁고 둔탁한 질감으로, 화면은 현란한 색과 선율로 자리를 바꿨다. (02)730-5454\n","doc3: 전남 나주시 다시면에 조성된 천연염색공방이 천연 염색업체의 입주가 늘면서 ‘천연염색 1번지’로 떠오르고 있다.27일 나주시와 천연염색문화재단에 따르면 지난해 11월 완공된 천연염색공방에 최근까지 12개 공방이 입주해 다양한 천연 염색제품을 제작해 전시 판매하고 있다. 천연염색공방은 총사업비 15억6000만원을 들여 909㎡ 규모에 개별공방 14실과 공동작업장, 다목적회의실 등을 갖추고 있다. 이곳에 전국에서 공모를 통해 모집한 10개 업체가 입주해 천연염색 의류를 비롯 가죽제품, 완구류, 침구류 등을 만들고 있다.최근엔 다문화 가족이 운영하는 ‘에틱’, 장애인 가족이 운영하는 ‘꽃물담쟁이’가 추가로 둥지를 틀었다. 에틱을 운영하는 우즈베키스탄 출신 결혼이주여성 고가이 에밀리야는 3년 전부터 동신대 산학협력사업을 통해 축적한 천연염색 기법으로 각국의 전통의상을 만들고 있다. 꽃물담쟁이는 나주시 삼영동에 있는 복지시설인 성산원 가족들이 함께 모여 운영하는 공방으로 재활치료 프로그램의 일환으로 운영된다.나주시와 천연염색문화재단은 천연염색공방 육성을 위해 입주업체를 대상으로 작품개발, 디자인 교육, 컨설팅, 시장개척 등을 지원하기로 했다. 윤여정 나주시 전략산업과장은 “천연염색공방은 새롭게 리모델링하는 천연염색문화관과 조만간 완공될 친환경염색산업센터 및 쪽공원 등과 함께 나주를 대표할 천연염색산업 클러스터의 주요 시설이 될 것”이라며 “내달 ‘천연염색과 음악’이라는 주제로 천연염색의 아름다움을 알리는 행사도 가질 예정”이라고 말했다. 나주=최성국 기자\n","doc4: 네팔의 히말라야 자락 산등성을 가로질러 도착한 해발 2874ｍ의 고라파니. 머리가 아프거나 어지럽고 계단을 오르내리면 쉽게 숨이 차는 증상들이 자주 찾아온다. ‘왜 여기까지 와서 고생을 사서 하나’라는 생각이 절로 든다. 하지만 안나푸르나의 설봉과 밤하늘을 장식하는 별자리들과 마주치면 이내 불평이 가라앉는다. 동양화와 서양화，구상과 추상의 경계를 넘나들며 ‘조형산수’(반추상 산수화)라는 새 장르를 개척한 ‘퓨전 한국화가’ 전래식 화백(71)이 히말라야 설봉들을 마주한 감회는 남달랐다. 최근 3년간 히말라야 설봉을 그리기 위해 수차례 네팔 트레킹을 다녀온 그는 요즘 매일 파주 작업실에서 히말라야 사생 작품을 정리하며 전시회를 준비하느라 여념이 없다. 오는 20~28일 서울 서초동 예술의전당 한가람미술관에서 열리는 개인전 주제는 ‘세계의 산을 품다’. 히말라야의 여러 봉우리를 배경으로 그린 독특한 조형산수 소품부터 400호 대작까지 모두 40여점을 출품하는 대규모 전시회다. 서라벌예대(현 중앙대)를 졸업한 전 화백은 1982년 문예진흥원이 주최한 제1회 대한민국 미술대전에서 대상을 받았다. 칠순을 넘은 그는 요즘도 그때와 같은 혈기로 히말라야 작업을 즐기고 있다. ‘70대 산사나이’를 자처하는 그는 파노라마처럼 펼쳐진 30여개의 설산을 배경으로 한 풍경을 마음속에 담았다가 화면에 토해냈다. “2009년 봄 나야폴에서 아침을 맞으며 안나푸르나의 모습을 보고 남은 삶은 히말라야의 다양한 얼굴을 그리겠다고 다짐했지요. 눈에 보이는 대로 정밀하게 묘사하는 대신 웅장한 산세에 사람을 더해 나만의 시각으로 히말라야의 다양한 얼굴을 담아내는 게 정말 즐거웠거든요.”전 화백의 히말라야 풍경에는 인물이 등장하기도 한다. 그의 그림 속 인물은 구상화에 대한 향수이자 생명력을 표현하는 기호다. 그는 “그림 속에 사람을 살짝 넣었더니 산들이 살아 움직이는 것 같았다. 화면이 더 풍성해지고 현대적 미감도 살아나더라”고 설명했다. 당나라 말기 주경현이 ‘당조명화록(唐朝名畵錄)’에서 논한 일품화풍(逸品畵風)과 사품론(四品論)을 기반으로 작업했다는 그는 “히말라야 그림은 그리는 것이 아니고 탄생된다”고 강조했다. “일품산수는 실물 그대로 묘사하는 전통 산수화와 달리 작가의 천재적 재주를 바탕으로 한 사의적(寫意的) 화풍인데, 어린 시절부터 공자를 비롯해 맹자 사마천 등의 책과 문학，철학서를 탐독한 것이 히말라야 그림의 자양분이 되고 있다고 생각해요. 그림은 항상 새롭고(新)，묘한 신비로움이 있으며(妙)，능숙하고(能)，뛰어나야(逸) 하거든요.” (02)580-1300\n","doc5: 쇠 깎는 소리가 들리는 서울 영등포구 문래동은 수십년 된 가정집이 공장으로 탈바꿈한 곳이다. 1960년대풍의 낡은 공장이 밀집한 문래동에는 금속가공 장인 1300여명이 일하고 있다.하지만 이곳은 예술인이 몰려있는 예술창작촌이기도 하다. 서울예술재단 소속 문래예술공장의 이현아 총괄매니저는 “문래동에는 회화 설치미술 연극 음악 행위예술 등 다양한 분야에서 약 250명의 작가가 활동하고 있다”고 말했다.예술인이 문래동에 모여들기 시작한 것은 2000년대 초반이다. 이곳에 있던 공장들이 떠나면서 빈 공간이 하나둘 생기기 시작했다. 예술인이 많은 홍익대 앞과 가깝고, 지하철 1·2호선이 지나가 교통이 편리하며, 임차료도 쌌기 때문이다.문래동에서 금속가공업을 하는 대영기업의 박동주 사장은 “문래예술공장이 문래동 1가에 들어선 2010년 이후 예술가들이 몰려들고 있다”며 “그 덕분에 칙칙한 분위기가 밝아지고 젊은이들이 몰려 생기가 돌고 있다”고 말했다.외국 예술인들도 문래동의 독특한 풍경에 끌려 이 지역을 많이 찾고 있다. 지난해 10월에는 영국의 사이먼 휘트햄과 호주의 제러미 나이덱이 공연 준비와 강연을 위해 문래동에 오기도 했다.\n","\n","위의 docs 중에서만 정보를 근거로 하여, 질문에 답변해 주세요.\n","답변에서 인용한 문서의 내용에는 반드시 [[doc1]], [[doc2]], ... 형식으로 인용 표시를 해주세요.\n","추론이나 지어내는 답변은 삼가주시고, docs에 명시적으로 나타난 내용만 인용해 주세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n"]}],"source":["print(prompt_lst[200])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KTB0fu2I4biz","outputId":"d68aae8f-cd2f-4ae6-8238-93cf16aaa9c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","현대미술에서 일본의 모노하 운동과 한국의 추상 표현주의는 서로 다른 접근 방식과 주제를 가지고 있지만, 두 장르 모두 물질과 형태에 대한 탐구를 중심으로 하고 있습니다.\n","\n","모노하 운동은 형태를 극도로 단순하게 표현하며, 사물을 있는 그대로 놓아두는 방식으로 사물과 공간, 위치, 상황, 관계 등을 탐구합니다. 이 운동은 돌, 철, 나무와 같은 재료의 성격을 그대로 드러내는 작업을 특징으로 하며, 일본의 고(故) 곽인식 화백이 이 운동을 주도했습니다[[doc1]].\n","\n","반면, 한국의 추상 표현주의는 감정과 색, 선율을 통해 내면의 감정을 표현하는 데 중점을 둡니다. 최홍순 화백은 그의 작품에서 색과 선율을 통해 분노, 불안, 공포 등을 표현하며, 민화적 이미지와 음악적 요소를 결합하여 독특한 주제를 다룹니다[[doc2]].\n","\n","재료와 주제의 차이점으로는, 모노하 운동이 물질의 본질을 탐구하는 데 중점을 두는 반면, 한국의 추상 표현주의는 감정의 표현과 음악적 요소를 강조합니다. 모노하의 작가들은 주로 자연 재료의 물성을 탐구하는 반면, 한국의 추상 표현주의 작가들은 색과 형태를 통해 감정과 이야기를 전달하는 데 집중합니다[[doc1]], [[doc2]]. \n","\n","이러한 차이점은 두 장르가 서로 다른 문화적 배경과 미적 가치관을 반영하고 있음을 보여줍니다.\n"]}],"source":["print(label_lst[200])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YIefBl854fCE","colab":{"referenced_widgets":["0bb6b5b6a393454280a42968330b4c24"]},"outputId":"f7828976-25a8-4d10-a315-df00ac94724e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0bb6b5b6a393454280a42968330b4c24","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Device set to use cuda:0\n"]}],"source":["# PEFT 라이브러리에서 파인튜닝(LORA 등 PEFT)이 적용된 생성형 언어모델을 자동으로 불러오는 클래스를 가져옴.\n","from peft import AutoPeftModelForCausalLM\n","\n","# Hugging Face에서 토크나이저와 추론 파이프라인 클래스를 가져옴.\n","from transformers import AutoTokenizer, pipeline\n","\n","# LoRA 등으로 파인튜닝된 모델(또는 체크포인트)의 저장 경로. 실습 예시에서는 특정 체크포인트를 불러와 추론에 사용.\n","peft_model_id = \"llama3-8b-rag-ko/checkpoint-846\"\n","\n","# LoRA 등 PEFT가 적용된 사전학습 LLM을 지정한 경로에서 불러옴.\n","# device_map=\"auto\": GPU/CPU 환경에 따라 자동으로 할당\n","# torch_dtype=torch.float16:\n","# bfloat16(16비트 부동소수점) 형식으로 메모리/속도를 최적화\n","fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n","\n","# fine_tuned_model과 같은 토크나이저를 연결해서 \"텍스트 생성(챗봇, 요약 등)\" 추론 파이프라인을 만듭니다.\n","# 이 파이프라인 객체로 프롬프트 입력→모델 응답 생성까지 한 번에 처리\n","pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UR2N-nM2Rp9i","outputId":"41d2698f-cd8d-46ff-c32d-412ff955ff9f"},"outputs":[{"data":{"text/plain":["128009"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["eos_token = tokenizer(\"<|eot_id|>\",add_special_tokens=False)[\"input_ids\"][0]\n","eos_token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-UtKAiyRw4C"},"outputs":[],"source":["def test_inference(pipe, prompt):\n","    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n","    return outputs[0]['generated_text'][len(prompt):].strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YuskKzxAR26m","outputId":"a8da27b5-fdeb-4039-ce7f-a9fff28f1714"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"name":"stdout","output_type":"stream","text":["    response:\n","현대미술에서 일본의 모노하 운동과 한국의 추상 표현주의는 서로 다른 방식으로 발전하고 있으며, 이 두 장르의 작가들이 사용하는 재료와 주제는 다음과 같이 비교할 수 있습니다.\n","\n","모노하 운동은 일본에서 1960~1970년대에 등장한 전위미술 장르로, 사물을 그대로 놓아두는 방식으로 사물과 공간, 위치, 상황, 관계에 접근합니다. 이 운동의 선구적 작가인 곽인식 화백은 나무, 쇠, 유리 등의 물성을 화면과 형상에 반영하는 작품을 선보였습니다. 그의 작품은 단색과 선, 면에 주목하며, 특유의 공간과 빛의 세계를 연출하는 것이 특징입니다[[doc1]].\n","\n","반면, 한국의 추상 표현주의는 최홍순 화백을 중심으로 발전하였습니다. 최 화백은 캔버스에 빛과 생명, 우주를 색의 물결로 묘사하는 화가로 알려져 있으며, '생명의 율동'을 주제로 한 작품을 선보입니다. 그의 작품에서는 다양한 색깔의 영혼이 있는 사물을 소재로 삼고, 음악적인 소재를 통해 자유분방한 표현을 추구합니다[[doc2]].\n","\n","재료의 사용에 있어서, 모노하 운동은 주로 돌, 철, 나무와 같은 자연 재료를 사용하는 반면, 추상 표현주의는 캔버스와 같은 기존의 미술 매체를 활용합니다. 또한, 모노하 운동은 단색과 선, 면에 중점을 두는 반면, 추상 표현주의는 다양한 색채와 선을 통해 생동감 있는 표현을 시도합니다.\n","\n","주제의 경우, 모노하 운동은 사물의 본질을 드러내는 데 중점을 두는 반면, 추상 표현주의는 생명력과 율동을 주제로 삼아 음악적 요소를 강조합니다. 이러한 차이는 두 장르가 서로 다른 시각과 접근 방식을 보여주는 것입니다.\n","    label:\n","\n","현대미술에서 일본의 모노하 운동과 한국의 추상 표현주의는 서로 다른 접근 방식과 주제를 가지고 있지만, 두 장르 모두 물질과 형태에 대한 탐구를 중심으로 하고 있습니다.\n","\n","모노하 운동은 형태를 극도로 단순하게 표현하며, 사물을 있는 그대로 놓아두는 방식으로 사물과 공간, 위치, 상황, 관계 등을 탐구합니다. 이 운동은 돌, 철, 나무와 같은 재료의 성격을 그대로 드러내는 작업을 특징으로 하며, 일본의 고(故) 곽인식 화백이 이 운동을 주도했습니다[[doc1]].\n","\n","반면, 한국의 추상 표현주의는 감정과 색, 선율을 통해 내면의 감정을 표현하는 데 중점을 둡니다. 최홍순 화백은 그의 작품에서 색과 선율을 통해 분노, 불안, 공포 등을 표현하며, 민화적 이미지와 음악적 요소를 결합하여 독특한 주제를 다룹니다[[doc2]].\n","\n","재료와 주제의 차이점으로는, 모노하 운동이 물질의 본질을 탐구하는 데 중점을 두는 반면, 한국의 추상 표현주의는 감정의 표현과 음악적 요소를 강조합니다. 모노하의 작가들은 주로 자연 재료의 물성을 탐구하는 반면, 한국의 추상 표현주의 작가들은 색과 형태를 통해 감정과 이야기를 전달하는 데 집중합니다[[doc1]], [[doc2]]. \n","\n","이러한 차이점은 두 장르가 서로 다른 문화적 배경과 미적 가치관을 반영하고 있음을 보여줍니다.\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"name":"stdout","output_type":"stream","text":["    response:\n","교황 첼레스티노 1세와 교황 식스토 3세의 재위 기간 동안 교회에서 다룬 주요 이슈와 그에 따른 결정은 다음과 같습니다.\n","\n","첼레스티노 1세는 네스토리우스의 교리와 관련된 논쟁이 주요 이슈였습니다. 그는 네스토리우스가 펠라기우스주의에 빠진 율리아누스와 가깝다는 사실을 알게 되자, 알렉산드리아의 치릴로 주교에게 네스토리우스에 대한 조사를 지시하였습니다. 이 조사를 바탕으로 430년 로마 시노드에서 첼레스티노 1세는 네스토리우스를 단죄하였고, 네스토리우스는 자신의 오류를 인정하여 철회할 것을 지시받았습니다. 이후, 에페소 공의회에서 네스토리우스의 해임이 확인되었습니다[[doc1]].\n","\n","식스토 3세는 성모 마리아의 신앙적 의미와 관련된 논쟁이 주요 이슈였습니다. 그는 431년 에페소 공의회에서 성모 마리아를 '하느님의 어머니'로 공식적으로 선포하였고, 이는 그리스도의 신성과 인성의 결합에 대한 논쟁을 해결하는 데 기여하였습니다. 또한, 식스토 3세는 알렉산드리아의 치릴로 주교와 시리아 교회 사이의 평화 관계 회복을 위해 노력하였습니다[[doc3]].\n","\n","이러한 결정들은 교회의 신앙적 정체성을 강화하는 데 기여하였으며, 교황의 권위와 교회의 사목권 확장에도 영향을 미쳤습니다. 첼레스티노 1세의 네스토리우스에 대한 결정은 교회의 교리적 일관성을 유지하는 데 중요한 역할을 했고, 식스토 3세의 마리아 신앙에 대한 결정은 교회의 신앙적 가르침을 정립하는 데 기여하였습니다.\n","\n","또한, 교황 첼레스티노 1세의 재위 기간 동안 교황 식스토 3세의 재위 기간 동안 교황의 사목권이 확장되는 결정은 교회의 지역적 영향력을 강화하는 데 기여하였습니다. 식스토 3세는 교황의 사목권을 일리리아까지 뻗쳐, 교회의 지역적 영향력을 더욱 공고히 하였습니다[[doc3]].\n","\n","이러한 결정들은 교회의 신앙적, 사목적 영향뿐만 아니라, 교황의 권위와 교회의 지역적 영향력에도 긍정적인 영향을 미쳤습니다.\n","    label:\n","\n","교황 첼레스티노 1세와 교황 식스토 3세의 재위 기간 동안 교회에서 다룬 주요 이슈와 그에 따른 교황의 결정, 그리고 이러한 결정이 경제적 또는 사회적 영향에 미친 바에 대해 설명드리겠습니다.\n","\n","### 교황 첼레스티노 1세 (422-432년)\n","첼레스티노 1세는 네스토리우스의 교리 문제를 다루었습니다. 그는 처음에 네스토리우스를 콘스탄티노폴리스 주교로 착좌하는 것을 승인했으나, 네스토리우스가 펠라기우스주의에 빠진 사실을 알게 된 후 입장을 철회하였습니다. 이후 그는 알렉산드리아의 치릴로 주교에게 네스토리우스에 대한 조사를 지시하였고, 431년 에페소 공의회에서 네스토리우스를 단죄하였습니다. 이 공의회에서는 성모 마리아를 하느님의 어머니로 인정하는 결정을 내렸습니다[[doc1]].\n","\n","첼레스티노 1세는 또한 펠라기우스주의를 비판하고 정통 기독교 신앙을 수호하는 데 힘썼습니다. 그는 아일랜드에 팔라디오를 주교로 파견하여 기독교를 전파하도록 하였습니다[[doc1]].\n","\n","### 교황 식스토 3세 (432-440년)\n","식스토 3세는 에페소 공의회의 결정을 이어받아 성모 마리아를 하느님의 어머니로 부르는 것을 공식화하였고, 이를 기념하기 위해 산타 마리아 마조레 대성전을 건립하였습니다. 그는 또한 펠라기우스에 대해 동정심을 가졌으나, 결국 그를 이단으로 선언하고 단죄하였습니다[[doc3]].\n","\n","식스토 3세는 알렉산드리아의 치릴로 주교와 시리아 교회 간의 평화 관계 회복에도 관심을 가졌으며, 교황의 사목권을 일리리아까지 확장하는 데 힘썼습니다[[doc3]].\n","\n","### 경제적 및 사회적 영향\n","첼레스티노 1세와 식스토 3세의 결정은 교회의 권위와 교리의 확립에 기여하였으며, 이는 기독교 공동체의 통합과 사회적 안정에 긍정적인 영향을 미쳤습니다. 특히, 성모 마리아에 대한 교회의 공식적인 입장은 신자들에게 신앙의 확신을 주었고, 교회의 권위가 강화되는 계기가 되었습니다. 이러한 교회의 결정은 신자들의 결속을 강화하고, 교회가 사회적 문제에 대한 입장을 명확히 하도록 하였습니다[[doc1]], [[doc3]].\n","\n","결론적으로, 첼레스티노 1세와 식스토 3세의 재위 기간 동안의 주요 이슈와 결정들은 교회의 교리 확립과 사회적 안정에 기여하였으며, 이는 기독교 공동체의 경제적 및 사회적 발전에도 긍정적인 영향을 미쳤다고 할 수 있습니다.\n","--------------------------------------------------\n"]}],"source":["for prompt, label in zip(prompt_lst[200:202], label_lst[200:202]):\n","    # print(f\"    prompt:\\n{prompt}\")\n","    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n","    print(f\"    label:\\n{label}\")\n","    print(\"-\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SiEI_jdgVrRu"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"widgets":{"application/vnd.jupyter.widget-state+json":{"091f46ee8cb54730960c07438c7840ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_623eb2ffbbc945aab1b9cf20fe1eb850","IPY_MODEL_9f588df2191d40cbb146ff3586a9761a","IPY_MODEL_736cc4a9e5ae44af9c7fed01cb1ce092"],"layout":"IPY_MODEL_9799358b20ec41e3b83196d7271016fb"}},"623eb2ffbbc945aab1b9cf20fe1eb850":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd003bf41d204b2c8875a4ac79b73fe9","placeholder":"​","style":"IPY_MODEL_a71646030f184630a59165b8062536d6","value":"Loading checkpoint shards: 100%"}},"6b8ca8ede83743b4a1c28314c6a54be3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"736cc4a9e5ae44af9c7fed01cb1ce092":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b1b73e4a5634622ac66c4c5bcfe28ca","placeholder":"​","style":"IPY_MODEL_7c982aefe044420c9a17294f592ebcd8","value":" 4/4 [00:04&lt;00:00,  1.01it/s]"}},"7c982aefe044420c9a17294f592ebcd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9799358b20ec41e3b83196d7271016fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b1b73e4a5634622ac66c4c5bcfe28ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f588df2191d40cbb146ff3586a9761a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfafa78cce68498298a6ace7c694f193","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b8ca8ede83743b4a1c28314c6a54be3","value":4}},"a71646030f184630a59165b8062536d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfafa78cce68498298a6ace7c694f193":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd003bf41d204b2c8875a4ac79b73fe9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}